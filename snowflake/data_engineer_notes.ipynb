{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #1Topic 1\n",
    "A Data Engineer is investigating a query that is taking a long time to return. The Query Profile shows the following:\n",
    "\n",
    "Solution:\n",
    "- Local Disk IO:\n",
    "    - snowflake primary process data in ram. If local Disk IO > 0% mean the current warehouse is not enough ram -> increase data warehouse size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #2Topic 1\n",
    "How can the following relational data be transformed into semi-structured data using the LEAST amount of operational overhead?\n",
    "image2\n",
    "\n",
    "A. Use the TO_JSON function.\n",
    "\n",
    "B. Use the PARSE_JSON function to produce a VARIANT value.\n",
    "\n",
    "C. Use the OBJECT_CONSTRUCT function to return a Snowflake object. Most Voted\n",
    "\n",
    "D. Use the TO_VARIANT function to convert each of the relational columns to VARIANT.\n",
    "\n",
    "Solutioin:\n",
    "- correct answer is C\n",
    "    - convert from row base to semi-structured data or object data type use object_construct function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #3Topic 1\n",
    "\n",
    "A Data Engineer executes a complex query and wants to make use of Snowflake’s query results caching capabilities to reuse the results.\n",
    "Which conditions must be met? (Choose three.)\n",
    "\n",
    "A. The results must be reused within 72 hours.\n",
    "\n",
    "B. The query must be executed using the same virtual warehouse.\n",
    "\n",
    "C. The USED_CACHED_RESULT parameter must be included in the query.\n",
    "\n",
    "D. The table structure contributing to the query result cannot have changed.\n",
    "\n",
    "E. The new query must have the same syntax as the previously executed query.\n",
    "\n",
    "F. The micro-partitions cannot have changed due to changes to other data in the table.\n",
    "\n",
    "Solution:\n",
    "- correct answer is DEF\n",
    "- to reuse query result from cache, we need following condition\n",
    "    - tables used by the query have not changed\n",
    "    - the new query much have the same syntax\n",
    "    - the micro-partition have not change "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #4Topic 1\n",
    "\n",
    "A Data Engineer needs to load JSON output from some software into Snowflake using Snowpipe.\n",
    "\n",
    "Which recommendations apply to this scenario? (Choose three.)\n",
    "\n",
    "A. Load large files (1 GB or larger).\n",
    "\n",
    "B. Ensure that data files are 100-250 MB (or larger) in size, compressed.\n",
    "\n",
    "C. Load a single huge array containing multiple records into a single table row.\n",
    "\n",
    "D. Verify each value of each unique element stores a single native data type (string or number).\n",
    "\n",
    "E. Extract semi-structured data elements containing null values into relational columns before loading.\n",
    "\n",
    "F. Create data files that are less than 100 MB and stage them in cloud storage at a sequence greater than once each minute.\n",
    "\n",
    "Solution:\n",
    "- correct answer is BDE\n",
    "    - reference on file size: https://docs.snowflake.com/en/user-guide/data-load-considerations-prepare#file-sizing-best-practices-and-limitations\n",
    "    - reference on semi-structure and columnization: https://docs.snowflake.com/en/user-guide/data-load-considerations-prepare#semi-structured-data-files-and-columnarization\n",
    "    - when loading semi-structured data like json. Snowflake will try to extract json data into table format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #5Topic 1\n",
    "\n",
    "Given the table SALES which has a clustering key of column CLOSED_DATE, which table function will return the average clustering depth for the SALES_REPRESENTATIVE column for the North American region?\n",
    "\n",
    "A. select system$clustering_information('Sales', 'sales_representative', 'region = ''North America''');\n",
    "\n",
    "B. select system$clustering_depth('Sales', 'sales_representative', 'region = ''North America'''); Most Voted\n",
    "\n",
    "C. select system$clustering_depth('Sales', 'sales_representative') where region = 'North America';\n",
    "\n",
    "D. select system$clustering_information('Sales', 'sales_representative') where region = 'North America’;\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- reference link on get cluster depth: https://docs.snowflake.com/en/sql-reference/functions/system_clustering_depth\n",
    "    - SYSTEM$CLUSTERING_DEPTH( '<table_name>' , '( <col1> [ , <col2> ... ] )' [ , '<predicate>' ] )\n",
    "    - select system$clustering_depth('Sales', 'sales_representative', 'region = ''North America'''); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #6Topic 1\n",
    "\n",
    "A large table with 200 columns contains two years of historical data. When queried, the table is filtered on a single day. Below is the Query Profile:\n",
    "image3\n",
    "\n",
    "Using a size 2XL virtual warehouse, this query took over an hour to complete.\n",
    "\n",
    "What will improve the query performance the MOST?\n",
    "\n",
    "A. Increase the size of the virtual warehouse.\n",
    "\n",
    "B. Increase the number of clusters in the virtual warehouse.\n",
    "\n",
    "C. Implement the search optimization service on the table.\n",
    "\n",
    "D. Add a date column as a cluster key on the table. Most Voted\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- looking at Pruning metric (Partition scanned, partition total) -> pruning is not good.\n",
    "    - for large fact table, we should clustering on transaction date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #7Topic 1\n",
    "\n",
    "A Data Engineer is working on a Snowflake deployment in AWS eu-west-1 (Ireland). The Engineer is planning to load data from staged files into target tables using the COPY \n",
    "INTO command.\n",
    "\n",
    "Which sources are valid? (Choose three.)\n",
    "\n",
    "A. Internal stage on GCP us-central1 (Iowa)\n",
    "\n",
    "B. Internal stage on AWS eu-central-1 (Frankfurt)\n",
    "\n",
    "C. External stage on GCP us-central1 (Iowa)\n",
    "\n",
    "D. External stage in an Amazon S3 bucket on AWS eu-west-1 (Ireland)\n",
    "\n",
    "E. External stage in an Amazon S3 bucket on AWS eu-central-1 (Frankfurt)\n",
    "\n",
    "F. SSD attached to an Amazon EC2 instance on AWS eu-west-1 (Ireland)\n",
    "\n",
    "Solution:\n",
    "- correct answer is CDE\n",
    "- the question asking about internal and external stage which used to load into snowlake\n",
    "    - internal stage must be the same region\n",
    "    - external stage can be different regioin or event different cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #8Topic 1\n",
    "\n",
    "A Data Engineer wants to create a new development database (DEV) as a clone of the permanent production database (PROD). There is a requirement to disable Fail-safe for all tables.\n",
    "\n",
    "Which command will meet these requirements?\n",
    "\n",
    "A. CREATE DATABASE DEV -\n",
    "\n",
    "CLONE PROD -\n",
    "FAIL_SAFE = FALSE;\n",
    "\n",
    "B. CREATE DATABASE DEV -\n",
    "CLONE PROD;\n",
    "\n",
    "C. CREATE TRANSIENT DATABASE DEV -\n",
    "CLONE PROD;\n",
    "\n",
    "D. CREATE DATABASE DEV -\n",
    "\n",
    "CLONE PROD -\n",
    "DATA_RETENTION_TIME_IN DAYS = 0;\n",
    "\n",
    "Solution:\n",
    "- correct answer is C. use create transient database syntax.\n",
    "    - transient tables is just normal table but do not have fail safe feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #9Topic 1\n",
    "Which query will show a list of the 20 most recent executions of a specified task, MYTASK, that have been scheduled within the last hour that have ended or are still running?\n",
    "\n",
    "A. image4\n",
    "\n",
    "B. image5\n",
    "\n",
    "C. image6\n",
    "\n",
    "D. image7\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- reference on query task history: https://docs.snowflake.com/en/sql-reference/functions/task_history\n",
    "    - table function\n",
    "    - using status in this case can missing row because solution C with state in ('executing', 'succeeded', 'failed')\n",
    "        - this is missinged cancelled, skipped\n",
    "    - safer filter is where query_id is not null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #10Topic 1\n",
    "\n",
    "Which methods can be used to create a DataFrame object in Snowpark? (Choose three.)\n",
    "\n",
    "A. session.jdbc_connection()\n",
    "\n",
    "B. session.read.json()\n",
    "\n",
    "C. session.table()\n",
    "\n",
    "D. DataFrame.write()\n",
    "\n",
    "E. session.builder()\n",
    "\n",
    "F. session.sql()\n",
    "\n",
    "Solution:\n",
    "- correct answer is BCF\n",
    "- reference to use snowspark to create dataframe: https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes#constructing-a-dataframe\n",
    "    - session.read.json\n",
    "    - session.sql\n",
    "    - session.table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #11Topic 1\n",
    "\n",
    "A new CUSTOMER table is created by a data pipeline in a Snowflake schema where MANAGED ACCESS is enabled.\n",
    "Which roles can grant access to the CUSTOMER table? (Choose three.)\n",
    "\n",
    "A. The role that owns the schema\n",
    "\n",
    "B. The role that owns the database\n",
    "\n",
    "C. The role that owns the CUSTOMER table\n",
    "\n",
    "D. The SYSADMIN role\n",
    "\n",
    "E. The SECURITYADMIN role\n",
    "\n",
    "F. The USERADMIN role with the MANAGE GRANTS privilege\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, E, F\n",
    "- which role can grant access to a table\n",
    "    - role that own the schema\n",
    "    - securityadmin role\n",
    "    - role with the MANAGE GRANTS priviledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #12Topic 1\n",
    "\n",
    "What is the purpose of the BUILD_STAGE_FILE_URL function in Snowflake?\n",
    "\n",
    "A. It generates an encrypted URL for accessing a file in a stage.\n",
    "\n",
    "B. It generates a staged URL for accessing a file in a stage.\n",
    "\n",
    "C. It generates a permanent URL for accessing files in a stage.\n",
    "\n",
    "D. It generates a temporary URL for accessing a file in a stage.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- BUILD_STAGE_FILE_URL function generates a permanent URL for accessing files in a stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #13Topic 1\n",
    "The JSON below is stored in a VARIANT column named V in a table named jCustRaw:\n",
    "image8\n",
    "\n",
    "Which query will return one row per team member (stored in the teamMembers array) along with all of the attributes of each team member?\n",
    "\n",
    "A. image9\n",
    "\n",
    "B. image10 Most Voted\n",
    "\n",
    "C. image11\n",
    "\n",
    "D. image12\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- this is an example of using lateral with flatten function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #14Topic 1\n",
    "\n",
    "A company has an extensive script in Scala that transforms data by leveraging DataFrames. A Data Engineer needs to move these transformations to Snowpark.\n",
    "What characteristics of data transformations in Snowpark should be considered to meet this requirement? (Choose two.)\n",
    "\n",
    "A. It is possible to join multiple tables using DataFrames.\n",
    "\n",
    "B. Snowpark operations are executed lazily on the server.\n",
    "\n",
    "C. User-Defined Functions (UDFs) are not pushed down to Snowflake.\n",
    "\n",
    "D. Snowpark requires a separate cluster outside of Snowflake for computations.\n",
    "\n",
    "E. Columns in different DataFrames with the same name should be referred to with squared brackets.\n",
    "\n",
    "Solution:\n",
    "- correct answer is AB\n",
    "- snowspark is the same as pyspark\n",
    "    - it is possible to join multiple tables using Dataframes\n",
    "    - snowpark operations are executed lazily on the server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #15\n",
    "The following is returned from SYSTEM$CLUSTERING_INFORMATION() for a table named ORDERS with a DATE column named O_ORDERDATE:\n",
    "image13\n",
    "What does the total_constant_partition_count value indicate about this table?\n",
    "\n",
    "A. The table is clustered very well on O_ORDERDATE, as there are 493 micro-partitions that could not be significantly improved by reclustering. Most Voted\n",
    "\n",
    "B. The table is not clustered well on O_ORDERDATE, as there are 493 micro-partitions where the range of values in that column overlap with every other micro-partition in the table.\n",
    "\n",
    "C. The data in O_ORDERDATE does not change very often, as there are 493 micro-partitions containing rows where that column has not been modified since the row was created.\n",
    "\n",
    "D. The data in O_ORDERDATE has a very low cardinality, as there are 493 micro-partitions where there is only a single distinct value in that column for all rows in the micro-partition.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A.\n",
    "- reference about function SYSTEM$CLUSTERING_INFORMATION: https://docs.snowflake.com/en/sql-reference/functions/system_clustering_information\n",
    "    - total_constant_partition_count: Total number of micro-partitions for which the value of the specified columns have reached a constant state (i.e. the micro-partitions will not benefit significantly from reclustering)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #16Topic 1\n",
    "\n",
    "A company is building a dashboard for thousands of Analysts. The dashboard presents the results of a few summary queries on tables that are regularly updated. The query conditions vary by topic according to what data each Analyst needs. Responsiveness of the dashboard queries is a top priority, and the data cache should be preserved.\n",
    "How should the Data Engineer configure the compute resources to support this dashboard?\n",
    "\n",
    "A. Assign queries to a multi-cluster virtual warehouse with economy auto-scaling. Allow the system to automatically start and stop clusters according to demand.\n",
    "\n",
    "B. Assign all queries to a multi-cluster virtual warehouse set to maximized mode. Monitor to determine the smallest suitable number of clusters.\n",
    "\n",
    "C. Create a virtual warehouse for every 250 Analysts. Monitor to determine how many of these virtual warehouses are being utilized at capacity.\n",
    "\n",
    "D. Create a size XL virtual warehouse to support all the dashboard queries. Monitor query runtimes to determine whether the virtual warehouse should be resized.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "    - use maximized mode when, we have large number of concurrency query and the nuber do not fluctuate.\n",
    "    - the top priority is repsonsiness of the dashboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #17Topic 1\n",
    "\n",
    "A Data Engineer has developed a dashboard that will issue the same SQL select clause to Snowflake every 12 hours.\n",
    "How long will Snowflake use the persisted query results from the result cache, provided that the underlying data has not changed?\n",
    "\n",
    "A. 12 hours\n",
    "\n",
    "B. 24 hours\n",
    "\n",
    "C. 14 days\n",
    "\n",
    "D. 31 days\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the persisted query result cache can last upto 31 days if underlying data not change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #18Topic 1\n",
    "\n",
    "A Data Engineer ran a stored procedure containing various transactions. During the execution, the session abruptly disconnected, preventing one transaction from committing or rolling back. The transaction was left in a detached state and created a lock on resources.\n",
    "What step must the Engineer take to immediately run a new transaction?\n",
    "\n",
    "A. Call the system function SYSTEM$ABORT_TRANSACTION.\n",
    "\n",
    "B. Call the system function SYSTEM$CANCEL_TRANSACTION.\n",
    "\n",
    "C. Set the LOCK_TIMEOUT to FALSE in the stored procedure.\n",
    "\n",
    "D. Set the TRANSACTION_ABORT_ON_ERROR to TRUE in the stored procedure.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- we usually use system$abort_transaction function to abort transaction in stored procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #19Topic 1\n",
    "\n",
    "A database contains a table and a stored procedure defined as:\n",
    "image14\n",
    "The log_table is initially empty and a Data Engineer issues the following command:\n",
    "CALL insert_log(NULL::VARCHAR);\n",
    "No other operations are affecting the log_table.\n",
    "What will be the outcome of the procedure call?\n",
    "\n",
    "A. The log_table contains zero records and the stored procedure returned 1 as a return value.\n",
    "\n",
    "B. The log_table contains one record and the stored procedure returned 1 as a return value.\n",
    "\n",
    "C. The log_table contains one record and the stored procedure returned NULL as a return value.\n",
    "\n",
    "D. The log_table contains zero records and the stored procedure returned NULL as a return value.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D. log_table contains zero records and the stored procedure return NULL as a return value\n",
    "- because the clause return NULL on NUll INPUT\n",
    "    - reference on create procedure syntax and docs: https://docs.snowflake.com/en/sql-reference/sql/create-procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #20Topic 1\n",
    "\n",
    "When would a Data Engineer use TABLE with the FLATTEN function instead of the LATERAL FLATTEN combination?\n",
    "\n",
    "A. When TABLE with FLATTEN requires another source in the FROM clause to refer to.\n",
    "\n",
    "B. When TABLE with FLATTEN requires no additional source in the FROM clause to refer to.\n",
    "\n",
    "C. When the LATERAL FLATTEN combination requires no other source in the FROM clause to refer to.\n",
    "\n",
    "D. When TABLE with FLATTEN is acting like a sub-query executed for each returned row.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- flatten vs lateral flatten\n",
    "    - flatten of array of single layer of json\n",
    "    - flatten of array and then child array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #21Topic 1\n",
    "Which output is provided by both the SYSTEM$CLUSTERING_DEPTH function and the SYSTEM$CLUSTERING_INFORMATION function?\n",
    "\n",
    "A. average_depth Most Voted\n",
    "\n",
    "B. notes\n",
    "\n",
    "C. average_overlaps\n",
    "\n",
    "D. total_partition_count\n",
    "\n",
    "Solution:\n",
    "- correct answer is A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #22Topic 1\n",
    "\n",
    "A Data Engineer needs to ingest invoice data in PDF format into Snowflake so that the data can be queried and used in a forecasting solution.\n",
    "\n",
    "What is the recommended way to ingest this data?\n",
    "\n",
    "A. Use Snowpipe to ingest the files that land in an external stage into a Snowflake table.\n",
    "\n",
    "B. Use a COPY INTO command to ingest the PDF files in an external stage into a Snowflake table with a VARIANT column.\n",
    "\n",
    "C. Create an external table on the PDF files that are stored in a stage and parse the data into structured data.\n",
    "\n",
    "D. Create a Java User-Defined Function (UDF) that leverages Java-based PDF parser libraries to parse PDF data into structured data.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- first you need to parse the pdf file in ot structured data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #23Topic 1\n",
    "\n",
    "A table is loaded using Snowpipe and truncated afterwards. Later, a Data Engineer finds that the table needs to be reloaded, but the metadata of the pipe will not allow the same files to be loaded again.\n",
    "How can this issue be solved using the LEAST amount of operational overhead?\n",
    "\n",
    "A. Wait until the metadata expires and then reload the file using Snowpipe.\n",
    "\n",
    "B. Modify the file by adding a blank row to the bottom and re-stage the file.\n",
    "\n",
    "C. Set the FORCE=TRUE option in the Snowpipe COPY INTO command.\n",
    "\n",
    "D. Recreate the pipe by using the CREATE OR REPLACE PIPE command.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- just recreate new pipe will solve the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #24Topic 1\n",
    "\n",
    "A stream called TRANSACTIONS_STM is created on top of a TRANSACTIONS table in a continuous pipeline running in Snowflake. After a couple of months, the TRANSACTIONS table is renamed TRANSACTIONS_RAW to comply with new naming standards.\n",
    "What will happen to the TRANSACTIONS_STM object?\n",
    "\n",
    "A. TRANSACTIONS_STM will keep working as expected. Most Voted\n",
    "\n",
    "B. TRANSACTIONS_STM will be stale and will need to be re-created.\n",
    "\n",
    "C. TRANSACTIONS_STM will be automatically renamed TRANSACTIONS_RAW_STM.\n",
    "\n",
    "D. Reading from the TRANSACTIONS_STM stream will succeed for some time after the expected STALE_TIME.\n",
    "\n",
    "Solution:\n",
    "- change table name do not affect steam which capture that table change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #25Topic 1\n",
    "\n",
    "A Data Engineer is evaluating the performance of a query in a development environment.\n",
    "image15\n",
    "image16\n",
    "Based on the Query Profile, what are some performance tuning options the Engineer can use? (Choose two.)\n",
    "\n",
    "A. Add a LIMIT to the ORDER BY if possible Most Voted\n",
    "\n",
    "B. Use a multi-cluster virtual warehouse with the scaling policy set to standard\n",
    "\n",
    "C. Move the query to a larger virtual warehouse Most Voted\n",
    "\n",
    "D. Create indexes to ensure sorted access to data\n",
    "\n",
    "E. Increase the MAX_CLUSTER_COUNT\n",
    "\n",
    "Solution:\n",
    "- correct answer is B, C\n",
    "- we have problem with spilling, then chosing larger virtual warehouse is good\n",
    "- we should you multiple cluster warehouse if we have long run query\n",
    "- limit is not reduce data scanning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #26Topic 1\n",
    "\n",
    "Which methods will trigger an action that will evaluate a DataFrame? (Choose two.)\n",
    "\n",
    "A. DataFrame.random_split()\n",
    "\n",
    "B. DataFrame.collect() Most Voted\n",
    "\n",
    "C. DataFrame.select()\n",
    "\n",
    "D. DataFrame.col()\n",
    "\n",
    "E. DataFrame.show() Most Voted\n",
    "\n",
    "Solution:\n",
    "- correct answer is B, C\n",
    "- dataframe.show(), dataframe.collect() functions will evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #27Topic 1\n",
    "\n",
    "Which Snowflake objects does the Snowflake Kafka connector use? (Choose three.)\n",
    "\n",
    "A. Pipe Most Voted\n",
    "\n",
    "B. Serverless task\n",
    "\n",
    "C. Internal user stage\n",
    "\n",
    "D. Internal table stage Most Voted\n",
    "\n",
    "E. Internal named stage Most Voted\n",
    "\n",
    "F. Storage integration\n",
    "\n",
    "Solution:\n",
    "- correct answer is ADE\n",
    "- kafka connector will three snowflake object:\n",
    "    - pipe: to move data\n",
    "    - internal name stag: to store temp data\n",
    "    - internal table: data destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #28Topic 1\n",
    "\n",
    "A Data Engineer has created table t1 with one column c1 with datatype VARIANT: create or replace table t1 (c1 variant);\n",
    "The Engineer has loaded the following JSON data set, which has information about 4 laptop models, into the table.\n",
    "image17\n",
    "The Engineer now wants to query that data set so that results are shown as normal structured data. The result should be 4 rows and 4 columns, without the double quotes surrounding the data elements in the JSON data.\n",
    "The result should be similar to the use case where the data was selected from a normal relational table t2, where t2 has string data type columns model_id, model, manufacturer, and model_name, and is queried with the SQL clause select * from t2;\n",
    "Which select command will produce the correct results?\n",
    "\n",
    "A. image18\n",
    "\n",
    "B. image19\n",
    "\n",
    "C. image20\n",
    "\n",
    "D. image21\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- fot this senario we need to use latteral flatten. Because, we need to reference column c1:key\n",
    "- values:key::type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #29Topic 1\n",
    "\n",
    "What is a characteristic of the use of external tokenization?\n",
    "\n",
    "A. Secure data sharing can be used with external tokenization.\n",
    "\n",
    "B. External tokenization cannot be used with database replication.\n",
    "\n",
    "C. Pre-loading of unmasked data is supported with external tokenization.\n",
    "\n",
    "D. External tokenization allows the preservation of analytical values after de-identification.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- reference about external tokenization: https://docs.snowflake.com/en/user-guide/security-column-ext-token-intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #30Topic 1\n",
    "\n",
    "A Data Engineer is implementing a near real-time ingestion pipeline to load data into Snowflake using the Snowflake Kafka connector. There will be three Kafka topics created.\n",
    "Which Snowflake objects are created automatically when the Kafka connector starts? (Choose three.)\n",
    "\n",
    "A. Tables\n",
    "\n",
    "B. Tasks\n",
    "\n",
    "C. Pipes\n",
    "\n",
    "D. Internal stages\n",
    "\n",
    "E. External stages\n",
    "\n",
    "F. Materialized views\n",
    "\n",
    "Solution:\n",
    "- correct answer is ACD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #31Topic 1\n",
    "\n",
    "The following chart represents the performance of a virtual warehouse over time:\n",
    "image22\n",
    "\n",
    "A Data Engineer notices that the warehouse is queueing queries. The warehouse is size X-Small, the minimum and maximum cluster counts are set to 1, the scaling policy is set to standard, and auto-suspend is set to 10 minutes.\n",
    "How can the performance be improved?\n",
    "\n",
    "A. Change the cluster settings.\n",
    "\n",
    "B. Increase the size of the warehouse.\n",
    "\n",
    "C. Change the scaling policy to economy.\n",
    "\n",
    "D. Change auto-suspend to a longer time frame.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A. change maximum cluster count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #32Topic 1\n",
    "\n",
    "A secure function returns data coming through an inbound share.\n",
    "What will happen if a Data Engineer tries to assign USAGE privileges on this function to an outbound share?\n",
    "\n",
    "A. An error will be returned because the Engineer cannot share data that has already been shared.\n",
    "\n",
    "B. An error will be returned because only views and secure stored procedures can be shared.\n",
    "\n",
    "C. An error will be returned because only secure functions can be shared with inbound shares.\n",
    "\n",
    "D. The Engineer will be able to share the secure function with other accounts.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A.\n",
    "- we need to understand inbound vs outbout. inbound (receiver of the share) vs outbound (provider of the share)\n",
    "- an object can not be reshared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #33Topic 1\n",
    "\n",
    "Which functions will compute a 'fingerprint' over an entire table, query result, or window to quickly detect changes to table contents or query results? (Choose two.)\n",
    "\n",
    "A. HASH(*)\n",
    "\n",
    "B. HASH_AGG(*)\n",
    "\n",
    "C. HASH_AGG(<expr>, <expr>)\n",
    "\n",
    "D. HASH_AGG_COMPARE(*)\n",
    "\n",
    "E. HASH_COMPARE(*)\n",
    "\n",
    "Solution:\n",
    "- correct answer is BC\n",
    "- we need know the the different between hash vs HASH_AGG. hash is row level while hash_agg for multiple rows level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #34Topic 1\n",
    "\n",
    "Which stages support external tables?\n",
    "\n",
    "A. Internal stages only; within a single Snowflake account\n",
    "\n",
    "B. Internal stages only; from any Snowflake account in the organization\n",
    "\n",
    "C. External stages only; from any region, and any cloud provider\n",
    "\n",
    "D. External stages only; only on the same region and cloud provider as the Snowflake account\n",
    "\n",
    "Solution:\n",
    "- correct answer is C. external stage support external table only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #35Topic 1\n",
    "\n",
    "A Data Engineer wants to check the status of a pipe named my_pipe. The pipe is inside a database named test and a schema named Extract (case-sensitive).\n",
    "Which query will provide the status of the pipe?\n",
    "\n",
    "A. SELECT SYSTEM$PIPE_STATUS(\"test.'extract'.my_pipe\");\n",
    "\n",
    "B. SELECT SYSTEM$PIPE_STATUS('test.\"Extract\".my_pipe');\n",
    "\n",
    "C. SELECT * FROM SYSTEM$PIPE_STATUS('test.\"Extract\".my_pipe');\n",
    "\n",
    "D. SELECT * FROM SYSTEM$PIPE_STATUS(\"test.'extract'.my_pipe\");\n",
    "\n",
    "Solution:\n",
    "- correct answer is B.\n",
    "- reference link: https://docs.snowflake.com/en/sql-reference/functions/system_pipe_status\n",
    "    - select without *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #36Topic 1\n",
    "\n",
    "Company A and Company B both have Snowflake accounts. Company A's account is hosted on a different cloud provider and region than Company B's account. Companies A and B are not in the same Snowflake organization.\n",
    "How can Company A share data with Company B? (Choose two.)\n",
    "\n",
    "A. Create a share within Company A's account and add Company B's account as a recipient of that share.\n",
    "\n",
    "B. Create a share within Company A's account, and create a reader account that is a recipient of the share. Grant Company B access to the reader account.\n",
    "\n",
    "C. Use database replication to replicate Company A's data into Company B's account. Create a share within Company B's account and grant users within Company B's \n",
    "account access to the share.\n",
    "\n",
    "D. Create a new account within Company A's organization in the same cloud provider and region as Company B's account. Use database replication to replicate Company A's data to the new account. Create a share within the new account, and add Company B's account as a recipient of that share.\n",
    "\n",
    "E. Create a separate database within Company A's account to contain only those data sets they wish to share with Company B. Create a share within Company A's account and add all the objects within this separate database to the share. Add Company B's account as a recipient of the share.\n",
    "\n",
    "Solution:\n",
    "- correct answer is AD\n",
    "- both solution A and D are sharing data between account using share\n",
    "    - https://docs.snowflake.com/en/user-guide/data-sharing-intro\n",
    "    - A is about sharing without replica while D is about replica and then share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #37Topic 1\n",
    "\n",
    "A Data Engineer is trying to load the following rows from a CSV file into a table in Snowflake with the following structure:\n",
    "image23\n",
    "image24\n",
    "The engineer is using the following COPY INTO statement:\n",
    "image25\n",
    "However, the following error is received:\n",
    "Number of columns in file (6) does not match that of the corresponding table (3), use file format option error_on_column_count_mismatch=false to ignore this error File 'address.csv.gz', line 3, character 1 Row 1 starts at line 2, column \"STGCUSTOMER\"[6] If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option.\n",
    "Which file format option should be used to resolve the error and successfully load all the data into the table?\n",
    "\n",
    "A. ESCAPE_UNENCLOSED FIELD = '\\\\'\n",
    "\n",
    "B. ERROR_ON_COLUMN_COUNT_MISMATCH = FALSE\n",
    "\n",
    "C. FIELD_DELIMITER = ','\n",
    "\n",
    "D. FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "\n",
    "Solution:\n",
    "- correct answer is D. If we load csv and have do not match row then we can use option FIELD_OPTIONALLY_ENCLOSED_BY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #38Topic 1\n",
    "\n",
    "A Data Engineer is working on a continuous data pipeline which receives data from Amazon Kinesis Firehose and loads the data into a staging table which will later be used in the data transformation process. The average file size is 300-500 MB.\n",
    "The Engineer needs to ensure that Snowpipe is performant while minimizing costs.\n",
    "How can this be achieved?\n",
    "\n",
    "A. Increase the size of the virtual warehouse used by Snowpipe.\n",
    "\n",
    "B. Split the files before loading them and set the SIZE_LIMIT option to 250 MB.\n",
    "\n",
    "C. Change the file compression size and increase the frequency of the Snowpipe loads.\n",
    "\n",
    "D. Decrease the buffer size to trigger delivery of files sized between 100 to 250 MB in Kinesis Firehose.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #39Topic 1\n",
    "\n",
    "Within a Snowflake account. permissions have been defined with custom roles and role hierarchies.\n",
    "To set up column-level masking using a role in the hierarchy of the current user, what command would be used?\n",
    "\n",
    "A. CURRENT_ROLE\n",
    "\n",
    "B. INVOKER_ROLE\n",
    "\n",
    "C. IS_ROLE_IN_SESSION\n",
    "\n",
    "D. IS_GRANTED_TO_INVOKER_ROLE\n",
    "\n",
    "Solution:\n",
    "- correct answer is D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #40Topic 1\n",
    "\n",
    "Assuming a Data Engineer has all appropriate privileges and context, which statements would be used to assess whether the User-Defined Function (UDF), MYDATABASE.SALES.REVENUE_BY_REGION, exists and is secure? (Choose two.)\n",
    "\n",
    "A. SHOW USER FUNCTIONS LIKE 'REVENUE_BY_REGION' IN SCHEMA SALES;\n",
    "\n",
    "B. SELECT IS_SECURE FROM SNOWFLAKE.INFORMATION_SCHEMA.FUNCTIONS WHERE FUNCTION_SCHEMA = 'SALES' AND FUNCTION_NAME = 'REVENUE_BY_REGION';\n",
    "\n",
    "C. SELECT IS_SECURE FROM INFORMATION_SCHEMA.FUNCTIONS WHERE FUNCTION_SCHEMA = 'SALES' AND FUNCTION_NAME = 'REVENUE_BY_REGION';\n",
    "\n",
    "D. SHOW EXTERNAL FUNCTIONS LIKE 'REVENUE_BY_REGION' IN SCHEMA SALES;\n",
    "\n",
    "E. SHOW SECURE FUNCTIONS LIKE 'REVENUE_BY_REGION' IN SCHEMA SALES;\n",
    "\n",
    "Solution:\n",
    "- correct answer is AC\n",
    "- to check user defined funtion exist in the schema\n",
    "    - show user functions like\n",
    "- to check is_secured\n",
    "    - select is_secured from information_schema.functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #41Topic 1\n",
    "\n",
    "A Data Engineer has written a stored procedure that will run with caller's rights. The Engineer has granted ROLEA the right to use this stored procedure.\n",
    "What is a characteristic of the stored procedure being called using ROLEA?\n",
    "\n",
    "A. The stored procedure must run with caller's rights; it cannot be converted later to run with owner's rights.\n",
    "\n",
    "B. If the stored procedure accesses an object that ROLEA does not have access to, the stored procedure will fail.\n",
    "\n",
    "C. The stored procedure will run in the context (database and schema) where the owner created the stored procedure.\n",
    "\n",
    "D. ROLEA will not be able to see the source code for the stored procedure, even though the role has usage privileges on the stored procedure.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- reference with stored procedure with caller's right: https://docs.snowflake.com/en/developer-guide/stored-procedure/stored-procedures-rights#caller-s-rights-stored-procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #42Topic 1\n",
    "\n",
    "What is a characteristic of the use of binding variables in JavaScript stored procedures in Snowflake?\n",
    "\n",
    "A. All types of JavaScript variables can be bound.\n",
    "\n",
    "B. All Snowflake first-class objects can be bound.\n",
    "\n",
    "C. Only JavaScript variables of type number, string, and SfDate can be bound.\n",
    "\n",
    "D. Users are restricted from binding JavaScript variables because they create SQL injection attack vulnerabilities.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- there are some restriction in binding variable in javascript user defined function. only a certain type and be binded like number, string, sfDate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #43Topic 1\n",
    "\n",
    "Which use case would be BEST suited for the search optimization service?\n",
    "\n",
    "A. Analysts who need to perform aggregates over high-cardinality columns.\n",
    "\n",
    "B. Business users who need fast response times using highly selective filters.\n",
    "\n",
    "C. Data Scientists who seek specific JOIN statements with large volumes of data.\n",
    "\n",
    "D. Data Engineers who create clustered tables with frequent reads against clustering keys.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #44Topic 1\n",
    "\n",
    "What is a characteristic of the operations of streams in Snowflake?\n",
    "\n",
    "A. Whenever a stream is queried, the offset is automatically advanced.\n",
    "\n",
    "B. When a stream is used to update a target table, the offset is advanced to the current time.\n",
    "\n",
    "C. Querying a stream returns all change records and table rows from the current offset to the current time.\n",
    "\n",
    "D. Each committed and uncommitted transaction on the source table automatically puts a change record in the stream.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #45Topic 1\n",
    "\n",
    "At what isolation level are Snowflake streams?\n",
    "\n",
    "A. Snapshot\n",
    "\n",
    "B. Repeatable read\n",
    "\n",
    "C. Read committed\n",
    "\n",
    "D. Read uncommitted\n",
    "\n",
    "Solution:\n",
    "- correct answer is B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #46Topic 1\n",
    "\n",
    "What kind of Snowflake integration is required when defining an external function in Snowflake?\n",
    "\n",
    "A. API integration\n",
    "\n",
    "B. HTTP integration\n",
    "\n",
    "C. Notification integration\n",
    "\n",
    "D. Security integration\n",
    "\n",
    "Solution:\n",
    "- correct answer is A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #47Topic 1\n",
    "\n",
    "A Data Engineer is writing a Python script using the Snowflake Connector for Python. The Engineer will use the snowflake.connector.connect function to connect to Snowflake.\n",
    "The requirements are:\n",
    "Raise an exception if the specified database, schema, or warehouse does not exist\n",
    "\n",
    "Improve download performance -\n",
    "Which parameters of the connect function should be used? (Choose two.)\n",
    "\n",
    "A. authenticator\n",
    "\n",
    "B. arrow_number_to_decimal\n",
    "\n",
    "C. client_prefetch_threads\n",
    "\n",
    "D. client_session_keep_alive\n",
    "\n",
    "E. validate_default_parameters\n",
    "\n",
    "Solution:\n",
    "- correct answer is CE\n",
    "- reference link: https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-api\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #48Topic 1\n",
    "\n",
    "A Data Engineer wants to centralize grant management to maximize security. A user needs OWNERSHIP on a table in a new schema. However, this user should not have the ability to make grant decisions.\n",
    "What is the correct way to do this?\n",
    "\n",
    "A. Grant OWNERSHIP to the user on the table.\n",
    "\n",
    "B. Revoke grant decisions from the user on the table.\n",
    "\n",
    "C. Revoke grant decisions from the user on the schema.\n",
    "\n",
    "D. Add the WITH MANAGED ACCESS parameter on the schema.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #49Topic 1\n",
    "\n",
    "A CSV file, around 1 TB in size, is generated daily on an on-premise server. A corresponding table, internal stage, and file format have already been created in Snowflake to facilitate the data loading process.\n",
    "How can the process of bringing the CSV file into Snowflake be automated using the LEAST amount of operational overhead?\n",
    "\n",
    "A. Create a task in Snowflake that executes once a day and runs a COPY INTO statement that references the internal stage. The internal stage will read the files directly from the on-premise server and copy the newest file into the table from the on-premise server to the Snowflake table.\n",
    "\n",
    "B. On the on-premise server, schedule a SQL file to run using SnowSQL that executes a PUT to push a specific file to the internal stage. Create a task that executes once a day in Snowflake and runs a COPY INTO statement that references the internal stage. Schedule the task to start after the file lands in the internal stage.\n",
    "\n",
    "C. On the on-premise server, schedule a SQL file to run using SnowSQL that executes a PUT to push a specific file to the internal stage. Create a pipe that runs a COPY INTO statement that references the internal stage. Snowpipe auto-ingest will automatically load the file from the internal stage when the new file lands in the internal stage.\n",
    "\n",
    "D. On the on-premise server, schedule a Python file that uses the Snowpark Python library. The Python script will read the CSV data into a DataFrame and generate an INSERT INTO statement that will directly load into the table. The script will bypass the need to move a file into an internal stage.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the key information with least amount of operational overhead -> use pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #50Topic 1\n",
    "\n",
    "What are characteristics of Snowpark Python packages? (Choose three.)\n",
    "\n",
    "A. Third-party packages can be registered as a dependency to the Snowpark session using the session.import() method.\n",
    "\n",
    "B. Python packages can access any external endpoints.\n",
    "\n",
    "C. Python packages can only be loaded in a local environment.\n",
    "\n",
    "D. Third-party supported Python packages are locked down to prevent hitting.\n",
    "\n",
    "E. The SQL command DESCRIBE FUNCTION will list the imported Python packages of the Python User-Defined Function (UDF).\n",
    "\n",
    "F. Querying information_schema.packages will provide a list of supported Python packages and versions.\n",
    "\n",
    "Solution:\n",
    "- correct answer is AEF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #51Topic 1\n",
    "\n",
    "While running an external function, the following error message is received:\n",
    "Error: Function received the wrong number of rows\n",
    "What is causing this to occur?\n",
    "\n",
    "A. External functions do not support multiple rows.\n",
    "\n",
    "B. Nested arrays are not supported in the JSON response.\n",
    "\n",
    "C. The JSON returned by the remote service is not constructed correctly.\n",
    "\n",
    "D. The return message did not produce the same number of rows that it received.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D.\n",
    "- we need to know snowflake external function.\n",
    "- reference on snowflake external function:https://docs.snowflake.com/en/sql-reference/external-functions-introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #52Topic 1\n",
    "\n",
    "A Data Engineer enables a result cache at the session level with the following command:\n",
    "ALTER SESSION SET USE_CACHED_RESULT = TRUE;\n",
    "The Engineer then runs the following SELECT query twice without delay:\n",
    "SELECT *\n",
    "FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER\n",
    "SAMPLE(10) SEED (99);\n",
    "The underlying table does not change between executions.\n",
    "What are the results of both runs?\n",
    "\n",
    "A. The first and second run returned the same results, because SAMPLE is deterministic.\n",
    "\n",
    "B. The first and second run returned the same results, because the specific SEED value was provided.\n",
    "\n",
    "C. The first and second run returned different results, because the query is evaluated each time it is run.\n",
    "\n",
    "D. The first and second run returned different results, because the query uses * instead of an explicit column list.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B. Because we use seed which allow deterministic sample selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #53Topic 1\n",
    "\n",
    "A company built a sales reporting system with Python, connecting to Snowflake using the Python Connector. Based on the user's selections, the system generates the SQL queries needed to fetch the data for the report. First it gets the customers that meet the given query parameters (on average 1000 customer records for each report run), and then it loops the customer records sequentially. Inside that loop it runs the generated SQL clause for the current customer to get the detailed data for that customer number from the sales data table.\n",
    "When the Data Engineer tested the individual SQL clauses, they were fast enough (1 second to get the customers, 0.5 second to get the sales data for one customer), but the total runtime of the report is too long.\n",
    "How can this situation be improved?\n",
    "\n",
    "A. Increase the size of the virtual warehouse.\n",
    "\n",
    "B. Increase the number of maximum clusters of the virtual warehouse.\n",
    "\n",
    "C. Define a clustering key for the sales data table.\n",
    "\n",
    "D. Rewrite the report to eliminate the use of the loop construct.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #54Topic 1\n",
    "A company is using Snowpipe to bring in millions of rows every day of Change Data Capture (CDC) into a Snowflake staging table on a real-time basis. The CDC needs to get processed and combined with other data in Snowflake and land in a final table as part of the full data pipeline.\n",
    "How can a Data Engineer MOST efficiently process the incoming CDC on an ongoing basis?\n",
    "\n",
    "A. Create a stream on the staging table and schedule a task that transforms data from the stream, only when the stream has data.\n",
    "\n",
    "B. Transform the data during the data load with Snowpipe by modifying the related COPY INTO statement to include transformation steps such as CASE statements and JOINS.\n",
    "\n",
    "C. Schedule a task that dynamically retrieves the last time the task was run from information_schema.task_history and use that timestamp to process the delta of the new rows since the last time the task was run.\n",
    "\n",
    "D. Use a CREATE OR REPLACE TABLE AS statement that references the staging table and includes all the transformation SQL. Use a task to run the full CREATE OR REPLACE TABLE AS statement on a scheduled basis.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- we can config cdc from stagging table to target table with snowstream + task. we can also config task to run when only snowstream have data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #55Topic 1\n",
    "\n",
    "A Data Engineer is building a pipeline to transform a 1 TB table by joining it with supplemental tables. The Engineer is applying filters and several aggregations leveraging Common Table Expressions (CTEs) using a size Medium virtual warehouse in a single query in Snowflake.\n",
    "After checking the Query Profile, what is the recommended approach to MAXIMIZE performance of this query if the Profile shows data spillage?\n",
    "\n",
    "A. Enable clustering on the table.\n",
    "\n",
    "B. Increase the warehouse size.\n",
    "\n",
    "C. Rewrite the query to remove the CTEs.\n",
    "\n",
    "D. Switch to a multi-cluster virtual warehouse.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #56Topic 1\n",
    "\n",
    "Which system role is recommended for a custom role hierarchy to be ultimately assigned to?\n",
    "\n",
    "A. ACCOUNTADMIN\n",
    "\n",
    "B. SECURITYADMIN\n",
    "\n",
    "C. SYSADMIN\n",
    "\n",
    "D. USERADMIN\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- Snowflake recommends creating a hierarchy of custom roles, with the top-most custom role assigned to the system role SYSADMIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #57Topic 1\n",
    "\n",
    "Which callback function is required within a JavaScript User-Defined Function (UDF) for it to execute successfully?\n",
    "\n",
    "A. initialize()\n",
    "\n",
    "B. processRow()\n",
    "\n",
    "C. handler()\n",
    "\n",
    "D. finalize()\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- reference link: https://docs.snowflake.com/en/developer-guide/udf/javascript/udf-javascript-tabular-functions\n",
    "- The defined object must include a callback function named processRow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #58Topic 1\n",
    "\n",
    "Which Snowflake feature facilitates access to external API services such as geocoders, data transformation, machine learning models, and other custom code?\n",
    "\n",
    "A. Security integration\n",
    "\n",
    "B. External tables\n",
    "\n",
    "C. External functions\n",
    "\n",
    "D. Java User-Defined Functions (UDFs)\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- reference on external functions: https://docs.snowflake.com/en/sql-reference/external-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #59Topic 1\n",
    "\n",
    "A Data Engineer needs to know the details regarding the micro-partition layout for a table named Invoice using a built-in function.\n",
    "Which query will provide this information?\n",
    "\n",
    "A. SELECT SYSTEM$CLUSTERING_INFORMATION('Invoice');\n",
    "\n",
    "B. SELECT $CLUSTERING_INFORMATION('Invoice');\n",
    "\n",
    "C. CALL SYSTEM$CLUSTERING_INFORMATION('Invoice');\n",
    "\n",
    "D. CALL $CLUSTERING_INFORMATION('Invoice');\n",
    "\n",
    "Solution:\n",
    "- correct answer is A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #60Topic 1\n",
    "\n",
    "A Data Engineer would like to define a file structure for loading and unloading data.\n",
    "Where can the file structure be defined? (Choose three.)\n",
    "\n",
    "A. COPY command\n",
    "\n",
    "B. MERGE command\n",
    "\n",
    "C. FILE FORMAT object\n",
    "\n",
    "D. PIPE object\n",
    "\n",
    "E. STAGE object\n",
    "\n",
    "F. INSERT command\n",
    "\n",
    "Solution:\n",
    "- correct answer is ACE\n",
    "- we can specified files structure for load and unloading data with copy command, file format and stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
