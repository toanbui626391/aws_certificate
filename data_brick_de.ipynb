{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #1\n",
    "\n",
    "An upstream system has been configured to pass the date for a given batch of data to the Databricks Jobs API as a parameter. The notebook to be scheduled will use this parameter to load data with the following code: df = spark.read.format(\"parquet\").load(f\"/mnt/source/(date)\")\n",
    "Which code block should be used to create the date Python variable used in the above code block?\n",
    "\n",
    "A. date = spark.conf.get(\"date\")\n",
    "\n",
    "B. input_dict = input()\n",
    "date= input_dict[\"date\"]\n",
    "\n",
    "C. import sys\n",
    "date = sys.argv[1]\n",
    "\n",
    "D. date = dbutils.notebooks.getParam(\"date\")\n",
    "\n",
    "E. dbutils.widgets.text(\"date\", \"null\")\n",
    "date = dbutils.widgets.get(\"date\")\n",
    "\n",
    "Solution:\n",
    "- Correct answer is E, the question is about using dbutils widget to get configurable \n",
    "  - dbutils.widgets.text -> create a text box named date\n",
    "  - dbutils.widgets.get -> get value from text box date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #2\n",
    "The Databricks workspace administrator has configured interactive clusters for each of the data engineering groups. To control costs, clusters are set to terminate after 30 minutes of inactivity. Each user should be able to execute workloads against their assigned clusters at any time of the day.\n",
    "Assuming users have been added to a workspace but not granted any permissions, which of the following describes the minimal permissions a user would need to start and attach to an already configured cluster.\n",
    "\n",
    "A. \"Can Manage\" privileges on the required cluster\n",
    "\n",
    "B. Workspace Admin privileges, cluster creation allowed, \"Can Attach To\" privileges on the required cluster\n",
    "\n",
    "C. Cluster creation allowed, \"Can Attach To\" privileges on the required cluster\n",
    "\n",
    "D. \"Can Restart\" privileges on the required cluster Most Voted\n",
    "\n",
    "E. Cluster creation allowed, \"Can Restart\" privileges on the required cluster\n",
    "\n",
    "Solution:\n",
    "- Correct ansewr is D, the question is about permission for user to submit and run job on cluster which auto suspend or terminate\n",
    "  - correct answer is Can Attach which include can attach to and start and restart compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "When scheduling Structured Streaming jobs for production, which configuration automatically recovers from query failures and keeps costs low?\n",
    "\n",
    "A. Cluster: New Job Cluster;\n",
    "Retries: Unlimited;\n",
    "Maximum Concurrent Runs: Unlimited\n",
    "\n",
    "B. Cluster: New Job Cluster;\n",
    "Retries: None;\n",
    "Maximum Concurrent Runs: 1\n",
    "\n",
    "C. Cluster: Existing All-Purpose Cluster;\n",
    "Retries: Unlimited;\n",
    "Maximum Concurrent Runs: 1\n",
    "\n",
    "D. Cluster: New Job Cluster;\n",
    "Retries: Unlimited;\n",
    "Maximum Concurrent Runs: 1\n",
    "\n",
    "E. Cluster: Existing All-Purpose Cluster;\n",
    "Retries: None;\n",
    "Maximum Concurrent Runs: 1\n",
    "\n",
    "Solution:\n",
    "- correct answer is C, the question is about how to save computing cost while config job\n",
    "  - cluster, retries, and maximum concurrent run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "The data engineering team has configured a Databricks SQL query and alert to monitor the values in a Delta Lake table. The recent_sensor_recordings table contains an identifying sensor_id alongside the timestamp and temperature for the most recent 5 minutes of recordings.\n",
    "The below query is used to create the alert:\n",
    "image1\n",
    "The query is set to refresh each minute and always completes in less than 10 seconds. The alert is set to trigger when mean (temperature) > 120. Notifications are triggered to be sent at most every 1 minute.\n",
    "If this alert raises notifications for 3 consecutive minutes and then stops, which statement must be true?\n",
    "\n",
    "A. The total average temperature across all sensors exceeded 120 on three consecutive executions of the query\n",
    "\n",
    "B. The recent_sensor_recordings table was unresponsive for three consecutive runs of the query\n",
    "\n",
    "C. The source query failed to update properly for three consecutive minutes and then restarted\n",
    "\n",
    "D. The maximum temperature recording for at least one sensor exceeded 120 on three consecutive executions of the query\n",
    "\n",
    "E. The average temperature recordings for at least one sensor exceeded 120 on three consecutive executions of the query\n",
    "\n",
    "Solution:\n",
    "- correct answer is E, the question is about understand the query and notification set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #5\n",
    "A junior developer complains that the code in their notebook isn't producing the correct results in the development environment. A shared screenshot reveals that while they're using a notebook versioned with Databricks Repos, they're using a personal branch that contains old logic. The desired branch named dev-2.3.9 is not available from the branch selection dropdown.\n",
    "Which approach will allow this developer to review the current logic for this notebook?\n",
    "\n",
    "A. Use Repos to make a pull request use the Databricks REST API to update the current branch to dev-2.3.9\n",
    "\n",
    "B. Use Repos to pull changes from the remote Git repository and select the dev-2.3.9 branch.\n",
    "\n",
    "C. Use Repos to checkout the dev-2.3.9 branch and auto-resolve conflicts with the current branch\n",
    "\n",
    "D. Merge all changes back to the main branch in the remote Git repository and clone the repo again\n",
    "\n",
    "E. Use Repos to merge the current branch and the dev-2.3.9 branch, then make a pull request to sync with the remote repository\n",
    "\n",
    "Solution:\n",
    "- correct answer is B. the question is about how to make databrick cluster update with the lasted of git repo\n",
    "  - Use Repos to pull changes from the remote Git repository and select the dev-2.3.9 branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #6\n",
    "The security team is exploring whether or not the Databricks secrets module can be leveraged for connecting to an external database.\n",
    "After testing the code with all Python variables being defined with strings, they upload the password to the secrets module and configure the correct permissions for the currently active user. They then modify their code to the following (leaving all other variables unchanged).\n",
    "image2\n",
    "Which statement describes what will happen when the above code is executed?\n",
    "\n",
    "A. The connection to the external table will fail; the string \"REDACTED\" will be printed.\n",
    "\n",
    "B. An interactive input box will appear in the notebook; if the right password is provided, the connection will succeed and the encoded password will be saved to DBFS.\n",
    "\n",
    "C. An interactive input box will appear in the notebook; if the right password is provided, the connection will succeed and the password will be printed in plain text.\n",
    "\n",
    "D. The connection to the external table will succeed; the string value of password will be printed in plain text.\n",
    "\n",
    "E. The connection to the external table will succeed; the string \"REDACTED\" will be printed.\n",
    "\n",
    "Solution:\n",
    "- correct answer is E. the question is about using dbutils.secrets module to get secret.\n",
    "  - when you try to print secrets will be redacted\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #7\n",
    "The data science team has created and logged a production model using MLflow. The following code correctly imports and applies the production model to output the predictions as a new DataFrame named preds with the schema \"customer_id LONG, predictions DOUBLE, date DATE\".\n",
    "image3\n",
    "The data science team would like predictions saved to a Delta Lake table with the ability to compare all predictions across time. Churn predictions will be made at most once per day.\n",
    "Which code block accomplishes this task while minimizing potential compute costs?\n",
    "\n",
    "A. preds.write.mode(\"append\").saveAsTable(\"churn_preds\") Most Voted\n",
    "\n",
    "B. preds.write.format(\"delta\").save(\"/preds/churn_preds\")\n",
    "\n",
    "C. image4\n",
    "\n",
    "D. image5\n",
    "\n",
    "E. \n",
    "\n",
    "Solution:\n",
    "- correct answer is A. the question is about how to save predictions from ml models to delta lake table\n",
    "  - with delta lake table we do not need to specified format, we just need to specified mode (append, etcs) and table name and the method is saveAsTable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #8\n",
    "An upstream source writes Parquet data as hourly batches to directories named with the current date. A nightly batch job runs the following code to ingest all data from the previous day as indicated by the date variable:\n",
    "image7\n",
    "Assume that the fields customer_id and order_id serve as a composite key to uniquely identify each order.\n",
    "If the upstream system is known to occasionally produce duplicate entries for a single order hours apart, which statement is correct?\n",
    "\n",
    "A. Each write to the orders table will only contain unique records, and only those records without duplicates in the target table will be written.\n",
    "\n",
    "B. Each write to the orders table will only contain unique records, but newly written records may have duplicates already present in the target table.\n",
    "\n",
    "C. Each write to the orders table will only contain unique records; if existing records with the same key are present in the target table, these records will be overwritten.\n",
    "\n",
    "D. Each write to the orders table will only contain unique records; if existing records with the same key are present in the target table, the operation will fail.\n",
    "\n",
    "E. Each write to the orders table will run deduplication over the union of new and existing records, ensuring no duplicate records are present.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B. the question is about how to deal with duplicates\n",
    "  - the code snip will drop duplicate on source but do not have logic to handle duplicate on target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #9\n",
    "A junior member of the data engineering team is exploring the language interoperability of Databricks notebooks. The intended outcome of the below code is to register a view of all sales that occurred in countries on the continent of Africa that appear in the geo_lookup table.\n",
    "Before executing the code, running SHOW TABLES on the current database indicates the database contains only two tables: geo_lookup and sales.\n",
    "image8\n",
    "Which statement correctly describes the outcome of executing these command cells in order in an interactive notebook?\n",
    "\n",
    "A. Both commands will succeed. Executing show tables will show that countries_af and sales_af have been registered as views.\n",
    "\n",
    "B. Cmd 1 will succeed. Cmd 2 will search all accessible databases for a table or view named countries_af: if this entity exists, Cmd 2 will succeed.\n",
    "\n",
    "C. Cmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable representing a PySpark DataFrame.\n",
    "\n",
    "D. Both commands will fail. No new variables, tables, or views will be created.\n",
    "\n",
    "E. Cmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable containing a list of strings.\n",
    "\n",
    "Solution:\n",
    "- correct answer is E. the question is about how to use python variable in a sql query\n",
    "  - you can not use python variable directly in a sql query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #10\n",
    "A Delta table of weather records is partitioned by date and has the below schema: date DATE, device_id INT, temp FLOAT, latitude FLOAT, longitude FLOAT\n",
    "To find all the records from within the Arctic Circle, you execute a query with the below filter: latitude > 66.3\n",
    "Which statement describes how the Delta engine identifies which files to load?\n",
    "\n",
    "A. All records are cached to an operational database and then the filter is applied\n",
    "\n",
    "B. The Parquet file footers are scanned for min and max statistics for the latitude column\n",
    "\n",
    "C. All records are cached to attached storage and then the filter is applied\n",
    "\n",
    "D. The Delta log is scanned for min and max statistics for the latitude column\n",
    "\n",
    "E. The Hive metastore is scanned for min and max statistics for the latitude column\n",
    "\n",
    "Solution:\n",
    "- correct answer is D. the questioin is about how databrick internal do when filter on columns\n",
    "  - The Delta log is scanned for min and max statistics for the latitude column -> identifies which files to load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #11\n",
    "The data engineering team has configured a job to process customer requests to be forgotten (have their data deleted). All user data that needs to be deleted is stored in Delta Lake tables using default table settings.\n",
    "The team has decided to process all deletions from the previous week as a batch job at 1am each Sunday. The total duration of this job is less than one hour. Every Monday at 3am, a batch job executes a series of VACUUM commands on all Delta Lake tables throughout the organization.\n",
    "The compliance officer has recently learned about Delta Lake's time travel functionality. They are concerned that this might allow continued access to deleted data.\n",
    "Assuming all delete logic is correctly implemented, which statement correctly addresses this concern?\n",
    "\n",
    "A. Because the VACUUM command permanently deletes all files containing deleted records, deleted records may be accessible with time travel for around 24 hours.\n",
    "\n",
    "B. Because the default data retention threshold is 24 hours, data files containing deleted records will be retained until the VACUUM job is run the following day.\n",
    "\n",
    "C. Because Delta Lake time travel provides full access to the entire history of a table, deleted records can always be recreated by users with full admin privileges.\n",
    "\n",
    "D. Because Delta Lake's delete statements have ACID guarantees, deleted records will be permanently purged from all storage systems as soon as a delete job completes.\n",
    "\n",
    "E. Because the default data retention threshold is 7 days, data files containing deleted records will be retained until the VACUUM job is run 8 days later.\n",
    "\n",
    "Solution:\n",
    "- correct answer is E. the question is about Vacuum command and time travel feature of delta table\n",
    "  - Because the default data retention threshold is 7 days, data files containing deleted records will be retained until the VACUUM job is run 8 days later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #12\n",
    "A junior data engineer has configured a workload that posts the following JSON to the Databricks REST API endpoint 2.0/jobs/create.\n",
    "image9\n",
    "Assuming that all configurations and referenced resources are available, which statement describes the result of executing this workload three times?\n",
    "\n",
    "A. Three new jobs named \"Ingest new data\" will be defined in the workspace, and they will each run once daily.\\\n",
    "\n",
    "B. The logic defined in the referenced notebook will be executed three times on new clusters with the configurations of the provided cluster ID.\n",
    "\n",
    "C. Three new jobs named \"Ingest new data\" will be defined in the workspace, but no jobs will be executed.\n",
    "\n",
    "D. One new job named \"Ingest new data\" will be defined in the workspace, but it will not be executed.\n",
    "\n",
    "E. The logic defined in the referenced notebook will be executed three times on the referenced existing all purpose cluster.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "  - Three new jobs named \"Ingest new data\" will be defined in the workspace, but no jobs will be executed. Because request body do not have schedule config and default behavior is that we need to trigger manually with runNow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #13\n",
    "An upstream system is emitting change data capture (CDC) logs that are being written to a cloud object storage directory. Each record in the log indicates the change type (insert, update, or delete) and the values for each field after the change. The source table has a primary key identified by the field pk_id.\n",
    "For auditing purposes, the data governance team wishes to maintain a full record of all values that have ever been valid in the source system. For analytical purposes, only the most recent value for each record needs to be recorded. The Databricks job to ingest these records occurs once per hour, but each individual record may have changed multiple times over the course of an hour.\n",
    "Which solution meets these requirements?\n",
    "\n",
    "A. Create a separate history table for each pk_id resolve the current state of the table by running a union all filtering the history tables for the most recent state.\n",
    "\n",
    "B. Use MERGE INTO to insert, update, or delete the most recent entry for each pk_id into a bronze table, then propagate all changes throughout the system.\n",
    "\n",
    "C. Iterate through an ordered set of changes to the table, applying each in turn; rely on Delta Lake's versioning ability to create an audit log.\n",
    "\n",
    "D. Use Delta Lake's change data feed to automatically process CDC data from an external system, propagating all changes to all dependent tables in the Lakehouse.\n",
    "\n",
    "E. Ingest all log information into a bronze table; use MERGE INTO to insert, update, or delete the most recent entry for each pk_id into a silver table to recreate the current table state.\n",
    "\n",
    "Solution:\n",
    "- correct answer is E. The question is about how to design data pipline to meet requirement (audit and user)\n",
    "  - using bronze, silver and gold tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #14\n",
    "An hourly batch job is configured to ingest data files from a cloud object storage container where each batch represent all records produced by the source system in a given hour. The batch job to process these records into the Lakehouse is sufficiently delayed to ensure no late-arriving data is missed. The user_id field represents a unique key for the data, which has the following schema: user_id BIGINT, username STRING, user_utc STRING, user_region STRING, last_login BIGINT, auto_pay BOOLEAN, last_updated BIGINT\n",
    "New records are all ingested into a table named account_history which maintains a full record of all data in the same schema as the source. The next table in the system is named account_current and is implemented as a Type 1 table representing the most recent value for each unique user_id.\n",
    "Assuming there are millions of user accounts and tens of thousands of records processed hourly, which implementation can be used to efficiently update the described account_current table as part of each hourly batch job?\n",
    "\n",
    "A. Use Auto Loader to subscribe to new files in the account_history directory; configure a Structured Streaming trigger once job to batch update newly detected files into the account_current table.\n",
    "\n",
    "B. Overwrite the account_current table with each batch using the results of a query against the account_history table grouping by user_id and filtering for the max value of last_updated.\n",
    "\n",
    "C. Filter records in account_history using the last_updated field and the most recent hour processed, as well as the max last_iogin by user_id write a merge statement to update or insert the most recent value for each user_id.\n",
    "\n",
    "D. Use Delta Lake version history to get the difference between the latest version of account_history and one version prior, then write these records to account_current.\n",
    "\n",
    "E. Filter records in account_history using the last_updated field and the most recent hour processed, making sure to deduplicate on username; write a merge statement to update or insert the most recent value for each username.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C. The question is about how to effective update type 1 table from historical tables\n",
    "  - Filter records in account_history using the last_updated field and the most recent hour processed\n",
    "  -  as well as the max last_iogin by user_id write a merge statement to update or insert the most recent value for each user_id\n",
    "  - So basically, filter and find the most updated record and then write upsert statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #15\n",
    "A table in the Lakehouse named customer_churn_params is used in churn prediction by the machine learning team. The table contains information about customers derived from a number of upstream sources. Currently, the data engineering team populates this table nightly by overwriting the table with the current valid values derived from upstream data sources.\n",
    "The churn prediction model used by the ML team is fairly stable in production. The team is only interested in making predictions on records that have changed in the past 24 hours.\n",
    "Which approach would simplify the identification of these changed records?\n",
    "\n",
    "A. Apply the churn model to all rows in the customer_churn_params table, but implement logic to perform an upsert into the predictions table that ignores rows where predictions have not changed.\n",
    "\n",
    "B. Convert the batch job to a Structured Streaming job using the complete output mode; configure a Structured Streaming job to read from the customer_churn_params table and incrementally predict against the churn model.\n",
    "\n",
    "C. Calculate the difference between the previous model predictions and the current customer_churn_params on a key identifying unique customers before making new predictions; only make predictions on those customers not in the previous predictions.\n",
    "\n",
    "D. Modify the overwrite logic to include a field populated by calling spark.sql.functions.current_timestamp() as data are being written; use this field to identify records written on a particular date.\n",
    "\n",
    "E. Replace the current overwrite logic with a merge statement to modify only those records that have changed; write logic to make predictions on the changed records identified by the change data feed.\n",
    "\n",
    "Solution:\n",
    "- correct answer is E, the question is about how to get change record on lakehouse table\n",
    "  - using upsert and change data feed feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #16\n",
    "A table is registered with the following code:\n",
    "image10\n",
    "Both users and orders are Delta Lake tables. Which statement describes the results of querying recent_orders?\n",
    "\n",
    "A. All logic will execute at query time and return the result of joining the valid versions of the source tables at the time the query finishes.\n",
    "\n",
    "B. All logic will execute when the table is defined and store the result of joining tables to the DBFS; this stored data will be returned when the table is queried.\n",
    "\n",
    "C. Results will be computed and cached when the table is defined; these cached results will incrementally update as new records are inserted into source tables.\n",
    "\n",
    "D. All logic will execute at query time and return the result of joining the valid versions of the source tables at the time the query began.\n",
    "\n",
    "E. The versions of each source table will be stored in the table transaction log; query results will be saved to DBFS with each query.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B, the question is about what will happend when using create table query\n",
    "  - All logic will execute when the table is defined and store the result of joining tables to the DBFS; this stored data will be returned when the table is queried"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #17\n",
    "A production workload incrementally applies updates from an external Change Data Capture feed to a Delta Lake table as an always-on Structured Stream job. When data was initially migrated for this table, OPTIMIZE was executed and most data files were resized to 1 GB. Auto Optimize and Auto Compaction were both turned on for the streaming production job. Recent review of data files shows that most data files are under 64 MB, although each partition in the table contains at least 1 GB of data and the total table size is over 10 TB.\n",
    "Which of the following likely explains these smaller file sizes?\n",
    "\n",
    "A. Databricks has autotuned to a smaller target file size to reduce duration of MERGE operations\n",
    "\n",
    "B. Z-order indices calculated on the table are preventing file compaction\n",
    "\n",
    "C. Bloom filter indices calculated on the table are preventing file compaction\n",
    "\n",
    "D. Databricks has autotuned to a smaller target file size based on the overall size of data in the table\n",
    "\n",
    "E. Databricks has autotuned to a smaller target file size based on the amount of data in each partition\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, the question is about change data capture feed to delta lake table with structured stream job. The question is about why recent file (auto optimmize and auto compaction is on) is under 64 MB.\n",
    "  - Databricks has autotuned to a smaller target file size to reduce duration of MERGE operations -> better upsert operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #18\n",
    "Which statement regarding stream-static joins and static Delta tables is correct?\n",
    "\n",
    "A. Each microbatch of a stream-static join will use the most recent version of the static Delta table as of each microbatch.\n",
    "\n",
    "B. Each microbatch of a stream-static join will use the most recent version of the static Delta table as of the job's initialization.\n",
    "\n",
    "C. The checkpoint directory will be used to track state information for the unique keys present in the join.\n",
    "\n",
    "D. Stream-static joins cannot use static Delta tables because of consistency issues.\n",
    "\n",
    "E. The checkpoint directory will be used to track updates to the static Delta table.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, the question is about concept of stream-static joins, static Delta tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #19\n",
    "A junior data engineer has been asked to develop a streaming data pipeline with a grouped aggregation using DataFrame df. The pipeline needs to calculate the average humidity and average temperature for each non-overlapping five-minute interval. Events are recorded once per minute per device.\n",
    "Streaming DataFrame df has the following schema:\n",
    "\"device_id INT, event_time TIMESTAMP, temp FLOAT, humidity FLOAT\"\n",
    "Code block:\n",
    "image11\n",
    "Choose the response that correctly fills in the blank within the code block to complete this task.\n",
    "\n",
    "A. to_interval(\"event_time\", \"5 minutes\").alias(\"time\")\n",
    "\n",
    "B. window(\"event_time\", \"5 minutes\").alias(\"time\")\n",
    "\n",
    "C. \"event_time\"\n",
    "\n",
    "D. window(\"event_time\", \"10 minutes\").alias(\"time\")\n",
    "\n",
    "E. lag(\"event_time\", \"10 minutes\").alias(\"time\")\n",
    "\n",
    "Solution:\n",
    "- correct answer is B, the question is about using window function to compute average of period of time interval\n",
    "  - understand the concept of watermark: In Spark Structured Streaming, a watermark is a mechanism for handling data that arrives late. It essentially acts as a threshold that determines how long the system waits for events to arrive before considering them irrelevant.\n",
    "  - understand the concept of window: In Spark Streaming, windows serve a similar purpose to Spark SQL, but specifically for analyzing data streams. They allow you to group and analyze data over specific time intervals within the continuous data stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #20\n",
    "\n",
    "A data architect has designed a system in which two Structured Streaming jobs will concurrently write to a single bronze Delta table. Each job is subscribing to a different topic from an Apache Kafka source, but they will write data with the same schema. To keep the directory structure simple, a data engineer has decided to nest a checkpoint directory to be shared by both streams.\n",
    "The proposed directory structure is displayed below:\n",
    "image12\n",
    "Which statement describes whether this checkpoint directory structure is valid for the given scenario and why?\n",
    "\n",
    "A. No; Delta Lake manages streaming checkpoints in the transaction log.\n",
    "\n",
    "B. Yes; both of the streams can share a single checkpoint directory.\n",
    "\n",
    "C. No; only one stream can write to a Delta Lake table.\n",
    "\n",
    "D. Yes; Delta Lake supports infinite concurrent writers.\n",
    "\n",
    "E. No; each of the streams needs to have its own checkpoint directory.\n",
    "\n",
    "Solution:\n",
    "- correct answer is E, the question is about how two streaming job can write to the same table\n",
    "  - two streaming job can write to the same table with unique checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #21\n",
    "A Structured Streaming job deployed to production has been experiencing delays during peak hours of the day. At present, during normal execution, each microbatch of data is processed in less than 3 seconds. During peak hours of the day, execution time for each microbatch becomes very inconsistent, sometimes exceeding 30 seconds. The streaming write is currently configured with a trigger interval of 10 seconds.\n",
    "Holding all other variables constant and assuming records need to be processed in less than 10 seconds, which adjustment will meet the requirement?\n",
    "\n",
    "A. Decrease the trigger interval to 5 seconds; triggering batches more frequently allows idle executors to begin processing the next batch while longer running tasks from previous batches finish.\n",
    "\n",
    "B. Increase the trigger interval to 30 seconds; setting the trigger interval near the maximum execution time observed for each batch is always best practice to ensure no records are dropped.\n",
    "\n",
    "C. The trigger interval cannot be modified without modifying the checkpoint directory; to maintain the current stream state, increase the number of shuffle partitions to maximize parallelism.\n",
    "\n",
    "D. Use the trigger once option and configure a Databricks job to execute the query every 10 seconds; this ensures all backlogged records are processed with each batch.\n",
    "\n",
    "E. Decrease the trigger interval to 5 seconds; triggering batches more frequently may prevent records from backing up and large batches from causing spill.\n",
    "\n",
    "Solution:\n",
    "- correct answer is E, the question is about the affact of reduce trigger time\n",
    "  - Decrease the trigger interval to 5 seconds; triggering batches more frequently may prevent records from backing up and large batches from causing spill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #22\n",
    "Which statement describes Delta Lake Auto Compaction?\n",
    "\n",
    "A. An asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 1 GB.\n",
    "\n",
    "B. Before a Jobs cluster terminates, OPTIMIZE is executed on all tables modified during the most recent job.\n",
    "\n",
    "C. Optimized writes use logical partitions instead of directory partitions; because partition boundaries are only represented in metadata, fewer small files are written.\n",
    "\n",
    "D. Data is queued in a messaging bus instead of committing data directly to memory; all data is committed from the messaging bus in one batch once the job is complete.\n",
    "\n",
    "E. An asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 128 MB.\n",
    "\n",
    "Solution:\n",
    "- correct answer is E, the question is about what is auto compaction feature\n",
    "  - An asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 128 MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #23\n",
    "Which statement characterizes the general programming model used by Spark Structured Streaming?\n",
    "\n",
    "A. Structured Streaming leverages the parallel processing of GPUs to achieve highly parallel data throughput.\n",
    "\n",
    "B. Structured Streaming is implemented as a messaging bus and is derived from Apache Kafka.\n",
    "\n",
    "C. Structured Streaming uses specialized hardware and I/O streams to achieve sub-second latency for data transfer.\n",
    "\n",
    "D. Structured Streaming models new data arriving in a data stream as new rows appended to an unbounded table.\n",
    "\n",
    "E. Structured Streaming relies on a distributed network of nodes that hold incremental state values for cached stages.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D, how structure steaming work\n",
    "  - Structured Streaming models new data arriving in a data stream as new rows appended to an unbounded table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #24\n",
    "Which configuration parameter directly affects the size of a spark-partition upon ingestion of data into Spark?\n",
    "\n",
    "A. spark.sql.files.maxPartitionBytes\n",
    "\n",
    "B. spark.sql.autoBroadcastJoinThreshold\n",
    "\n",
    "C. spark.sql.files.openCostInBytes\n",
    "\n",
    "D. spark.sql.adaptive.coalescePartitions.minPartitionNum\n",
    "\n",
    "E. spark.sql.adaptive.advisoryPartitionSizeInBytes\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, the question is about config which to affect to size of ingest data files\n",
    "  - spark.sql.files.maxPartitionBytes -> cts as a ceiling for each partition size. Spark won't create any partition exceeding the specified value during data ingestion."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
