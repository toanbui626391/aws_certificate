{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #1\n",
    "\n",
    "A company needs to architect a hybrid DNS solution. This solution will use an Amazon Route 53 private hosted zone for the domain cloud.example.com for the resources stored within VPCs.\n",
    "The company has the following DNS resolution requirements:\n",
    "On-premises systems should be able to resolve and connect to cloud.example.com.\n",
    "All VPCs should be able to resolve cloud.example.com.\n",
    "There is already an AWS Direct Connect connection between the on-premises corporate network and AWS Transit Gateway.\n",
    "Which architecture should the company use to meet these requirements with the HIGHEST performance?\n",
    "\n",
    "A. Associate the private hosted zone to all the VPCs. Create a Route 53 inbound resolver in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver.\n",
    "\n",
    "B. Associate the private hosted zone to all the VPCs. Deploy an Amazon EC2 conditional forwarder in the shared services VPC. Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the conditional forwarder.\n",
    "\n",
    "C. Associate the private hosted zone to the shared services VPCreate a Route 53 outbound resolver in the shared services VPAttach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the outbound resolver.\n",
    "\n",
    "D. Associate the private hosted zone to the shared services VPC. Create a Route 53 inbound resolver in the shared services VPC. Attach the shared services VPC to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about centralize DNS solution\n",
    "    - Associate the private hosted zone to all the VPCs\n",
    "    - Create a Route 53 inbound resolver in the shared services VPC\n",
    "    - Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #2\n",
    "\n",
    "A company is providing weather data over a REST-based API to several customers. The API is hosted by Amazon API Gateway and is integrated with different AWS Lambda functions for each API operation. The company uses Amazon Route 53 for DNS and has created a resource record of weather.example.com. The company stores data for the API in Amazon DynamoDB tables. The company needs a solution that will give the API the ability to fail over to a different AWS Region.\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Deploy a new set of Lambda functions in a new Region. Update the API Gateway API to use an edge-optimized API endpoint with Lambda functions from both Regions as targets. Convert the DynamoDB tables to global tables.\n",
    "\n",
    "B. Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables.\n",
    "\n",
    "C. Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target health monitoring. Convert the DynamoDB tables to global tables.\n",
    "\n",
    "D. Deploy a new API Gateway API in a new Region. Change the Lambda functions to global functions. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about api gateway and lambda function need to have failover capability in multiple regions\n",
    "    - create new api gateway and lambda function in another regions -> api gateway and lambda function which in multiple regions\n",
    "    - Change the Route 53 DNS record to a failover record -> DNS record can handle in case of fail over\n",
    "    - Enable target health monitoring -> to know region is failling or not\n",
    "    - Convert the DynamoDB tables to global tables -> DynamoDb is replica in multiple regions (global tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #3\n",
    "\n",
    "A company uses AWS Organizations with a single OU named Production to manage multiple accounts. All accounts are members of the Production OU. Administrators use deny list SCPs in the root of the organization to manage access to restricted services.\n",
    "The company recently acquired a new business unit and invited the new unit’s existing AWS account to the organization. Once onboarded, the administrators of the new business unit discovered that they are not able to update existing AWS Config rules to meet the company’s policies.\n",
    "Which option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance?\n",
    "\n",
    "A. Remove the organization’s root SCPs that limit access to AWS Config. Create AWS Service Catalog products for the company’s standard AWS Config rules and deploy them throughout the organization, including the new account.\n",
    "\n",
    "B. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the new account to the Production OU when adjustments to AWS Config are complete.\n",
    "\n",
    "C. Convert the organization’s root SCPs from deny list SCPs to allow list SCPs to allow the required services only. Temporarily apply an SCP to the organization’s root that allows AWS Config actions for principals only in the new account.\n",
    "\n",
    "D. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization’s root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the question is about relation between Oranization, SCP (Service Control Policy) and how to apply to change aws config\n",
    "    - because of SPC in root -> new unit account can not chnage aws config\n",
    "    - create new OU (Oranization Unit), config SCP on Onboarding OU which allo aws config \n",
    "    - move SCP from root to Production -> so that new account can change config\n",
    "    - move new account to Production when aws config is completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #4\n",
    "\n",
    "A company is running a two-tier web-based application in an on-premises data center. The application layer consists of a single server running a stateful application. The application connects to a PostgreSQL database running on a separate server. The application’s user base is expected to grow significantly, so the company is migrating the application and database to AWS. The solution will use Amazon Aurora PostgreSQL, Amazon EC2 Auto Scaling, and Elastic Load Balancing.\n",
    "Which solution will provide a consistent user experience that will allow the application and database tiers to scale?\n",
    "\n",
    "A. Enable Aurora Auto Scaling for Aurora Replicas. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled.\n",
    "\n",
    "B. Enable Aurora Auto Scaling for Aurora writers. Use an Application Load Balancer with the round robin routing algorithm and sticky sessions enabled.\n",
    "\n",
    "C. Enable Aurora Auto Scaling for Aurora Replicas. Use an Application Load Balancer with the round robin routing and sticky sessions enabled.\n",
    "\n",
    "D. Enable Aurora Scaling for Aurora writers. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about solution which allow two tier application (application + database) to scale\n",
    "    - Enable Aurora Auto Scaling for Aurora Replicas -> allow database to scale\n",
    "    - Use an Application Load Balancer with the round robin routing and sticky sessions enabled -> better user experience (do not lost session data) when scale up or down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #5\n",
    "\n",
    "A company uses a service to collect metadata from applications that the company hosts on premises. Consumer devices such as TVs and internet radios access the applications. Many older devices do not support certain HTTP headers and exhibit errors when these headers are present in responses. The company has configured an on-premises load balancer to remove the unsupported headers from responses sent to older devices, which the company identified by the User-Agent headers.\n",
    "The company wants to migrate the service to AWS, adopt serverless technologies, and retain the ability to support the older devices. The company has already migrated the applications into a set of AWS Lambda functions.\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB to invoke the correct Lambda function for each type of request. Create a CloudFront function to remove the problematic headers based on the value of the User-Agent header.\n",
    "\n",
    "B. Create an Amazon API Gateway REST API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Modify the default gateway responses to remove the problematic headers based on the value of the User-Agent header.\n",
    "\n",
    "C. Create an Amazon API Gateway HTTP API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Create a response mapping template to remove the problematic headers based on the value of the User-Agent. Associate the response data mapping with the HTTP API.\n",
    "\n",
    "D. Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB to invoke the correct Lambda function for each type of request. Create a Lambda@Edge function that will remove the problematic headers in response to viewer requests based on the value of the User-Agent header.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- this question is aabout how to delivery response to client device base on request header, we need to support older device \n",
    "    - Create an Amazon CloudFront distribution for the metadata service. Create a CloudFront function to remove the problematic headers based on the value of the User-Agent header -> CloudFront function can help in remove un-wanted header\n",
    "    - Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB. Configure the ALB to invoke the correct Lambda function for each type of request -> CloudFront forware request to Application Load Balancer, then ALB will invoke correct lambda function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question #6\n",
    "\n",
    "A retail company needs to provide a series of data files to another company, which is its business partner. These files are saved in an Amazon S3 bucket under Account A, which belongs to the retail company. The business partner company wants one of its IAM users, User_DataProcessor, to access the files from its own AWS account (Account B).\n",
    "Which combination of steps must the companies take so that User_DataProcessor can access the S3 bucket successfully? (Choose two.)\n",
    "\n",
    "Solution:\n",
    "- correct answer is C, D\n",
    "- is is about access s3 bucket from another account\n",
    "    - in account A allow user of account B to access s3 bucket\n",
    "    - in account B allow to access bucket from account a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #7\n",
    "\n",
    "A company is running a traditional web application on Amazon EC2 instances. The company needs to refactor the application as microservices that run on containers. Separate versions of the application exist in two distinct environments: production and testing. Load for the application is variable, but the minimum load and the maximum load are known. A solutions architect needs to design the updated application with a serverless architecture that minimizes operational complexity.\n",
    "Which solution will meet these requirements MOST cost-effectively?\n",
    "\n",
    "A. Upload the container images to AWS Lambda as functions. Configure a concurrency limit for the associated Lambda functions to handle the expected peak load. Configure two separate Lambda integrations within Amazon API Gateway: one for production and one for testing.\n",
    "\n",
    "B. Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Container Service (Amazon ECS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the ECS clusters.\n",
    "\n",
    "C. Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Kubernetes Service (Amazon EKS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the EKS clusters.\n",
    "\n",
    "D. Upload the container images to AWS Elastic Beanstalk. In Elastic Beanstalk, create separate environments and deployments for production and testing. Configure two separate Application Load Balancers to direct traffic to the Elastic Beanstalk deployments.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about change from ec2 to container based application but need to minized operational complexity and cost effective\n",
    "    - solution can be B or C. We choose C because Amazon Elastic Container Service is aws full managed which is recommened by aws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #8\n",
    "\n",
    "A company has a multi-tier web application that runs on a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB and the Auto Scaling group are replicated in a backup AWS Region. The minimum value and the maximum value for the Auto Scaling group are set to zero. An Amazon RDS Multi-AZ DB instance stores the application’s data. The DB instance has a read replica in the backup Region. The application presents an endpoint to end users by using an Amazon Route 53 record.\n",
    "The company needs to reduce its RTO to less than 15 minutes by giving the application the ability to automatically fail over to the backup Region. The company does not have a large enough budget for an active-active strategy.\n",
    "What should a solutions architect recommend to meet these requirements?\n",
    "\n",
    "A. Reconfigure the application’s Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function.\n",
    "\n",
    "B. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Configure Route 53 with a health check that monitors the web application and sends an Amazon Simple Notification Service (Amazon SNS) notification to the Lambda function when the health check status is unhealthy. Update the application’s Route 53 record with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs.\n",
    "\n",
    "C. Configure the Auto Scaling group in the backup Region to have the same values as the Auto Scaling group in the primary Region. Reconfigure the application’s Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Remove the read replica. Replace the read replica with a standalone RDS DB instance. Configure Cross-Region Replication between the RDS DB instances by using snapshots and Amazon S3.\n",
    "\n",
    "D. Configure an endpoint in AWS Global Accelerator with the two ALBs as equal weighted targets. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function.\n",
    "\n",
    "Solution:\n",
    "- correct anser is B\n",
    "- the question is about desing multi-tier application which allow failover backup region\n",
    "    - Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values -> lambda function to promote read replica and change config for auto scaling group\n",
    "    - Configure Route 53 with a health check that monitors the web application and sends an Amazon Simple Notification Service (Amazon SNS) notification to the Lambda function when the health check status is unhealthy -> health check at route 53 to check for failover, send notification to SNS topics to invoke lambda function\n",
    "    - Update the application’s Route 53 record with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs. -> route traffic to alb in backup regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #9\n",
    "\n",
    "A company is hosting a critical application on a single Amazon EC2 instance. The application uses an Amazon ElastiCache for Redis single-node cluster for an in-memory data store. The application uses an Amazon RDS for MariaDB DB instance for a relational database. For the application to function, each piece of the infrastructure must be healthy and must be in an active state.\n",
    "A solutions architect needs to improve the application's architecture so that the infrastructure can automatically recover from failure with the least possible downtime.\n",
    "Which combination of steps will meet these requirements? (Choose three.)\n",
    "\n",
    "A. Use an Elastic Load Balancer to distribute traffic across multiple EC2 instances. Ensure that the EC2 instances are part of an Auto Scaling group that has a minimum capacity of two instances.\n",
    "\n",
    "B. Use an Elastic Load Balancer to distribute traffic across multiple EC2 instances. Ensure that the EC2 instances are configured in unlimited mode.\n",
    "\n",
    "C. Modify the DB instance to create a read replica in the same Availability Zone. Promote the read replica to be the primary DB instance in failure scenarios.\n",
    "\n",
    "D. Modify the DB instance to create a Multi-AZ deployment that extends across two Availability Zones.\n",
    "\n",
    "E. Create a replication group for the ElastiCache for Redis cluster. Configure the cluster to use an Auto Scaling group that has a minimum capacity of two instances.\n",
    "\n",
    "F. Create a replication group for the ElastiCache for Redis cluster. Enable Multi-AZ on the cluster.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, D, F\n",
    "- the question is about least possible downtime for application which have cache, database\n",
    "    - Use an Elastic Load Balancer to distribute traffic across multiple EC2 instances. Ensure that the EC2 instances are part of an Auto Scaling group that has a minimum capacity of two instances\n",
    "    - Modify the DB instance to create a Multi-AZ deployment that extends across two Availability Zones\n",
    "    - Create a replication group for the ElastiCache for Redis cluster. Enable Multi-AZ on the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #10\n",
    "\n",
    "A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that points to the ALB. Static content is cached. Amazon Route 53 is used to host all public zones.\n",
    "After an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP headers that are returned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs.\n",
    "While the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page to visitors.\n",
    "Which combination of steps will meet this requirement with the LEAST amount of operational overhead? (Choose two.)\n",
    "\n",
    "A. Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3.\n",
    "\n",
    "B. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server.\n",
    "\n",
    "C. Modify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS records to point to a publicly accessible webpage.\n",
    "\n",
    "D. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server.\n",
    "\n",
    "E. Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, E\n",
    "- the question is about provide custom error page instead of the standar ALB error page\n",
    "    - Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3 -> have a custome error page\n",
    "    - Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page -> config CloudFront to use error page when error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #11\n",
    "A company has many AWS accounts and uses AWS Organizations to manage all of them. A solutions architect must implement a solution that the company can use to share a common network across multiple accounts.\n",
    "The company’s infrastructure team has a dedicated infrastructure account that has a VPC. The infrastructure team must use this account to manage the network. Individual accounts cannot have the ability to manage their own networks. However, individual accounts must be able to create AWS resources within subnets.\n",
    "Which combination of actions should the solutions architect perform to meet these requirements? (Choose two.)\n",
    "\n",
    "A. Create a transit gateway in the infrastructure account.\n",
    "\n",
    "B. Enable resource sharing from the AWS Organizations management account.\n",
    "\n",
    "C. Create VPCs in each AWS account within the organization in AWS Organizations. Configure the VPCs to share the same CIDR range and subnets as the VPC in the infrastructure account. Peer the VPCs in each individual account with the VPC in the infrastructure account.\n",
    "\n",
    "D. Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each subnet to associate with the resource share.\n",
    "\n",
    "E. Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each prefix list to associate with the resource share.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B, D\n",
    "- the question is about large organization have multiple account but want to centralize networking (VPC)\n",
    "    - Enable resource sharing from the AWS Organizations management account -> Organization share resource between account\n",
    "    - Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each subnet to associate with the resource share\n",
    "    - shared VPC: he account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #12\n",
    "A company wants to use a third-party software-as-a-service (SaaS) application. The third-party SaaS application is consumed through several API calls. The third-party SaaS application also runs on AWS inside a VPC.\n",
    "The company will consume the third-party SaaS application from inside a VPC. The company has internal security policies that mandate the use of private connectivity that does not traverse the internet. No resources that run in the company VPC are allowed to be accessed from outside the company’s VPC. All permissions must conform to the principles of least privilege.\n",
    "Which solution meets these requirements?\n",
    "\n",
    "A. Create an AWS PrivateLink interface VPC endpoint. Connect this endpoint to the endpoint service that the third-party SaaS application provides. Create a security group to limit the access to the endpoint. Associate the security group with the endpoint.\n",
    "\n",
    "B. Create an AWS Site-to-Site VPN connection between the third-party SaaS application and the company VPC. Configure network ACLs to limit access across the VPN tunnels.\n",
    "\n",
    "C. Create a VPC peering connection between the third-party SaaS application and the company VPUpdate route tables by adding the needed routes for the peering connection.\n",
    "\n",
    "D. Create an AWS PrivateLink endpoint service. Ask the third-party SaaS provider to create an interface VPC endpoint for this endpoint service. Grant permissions for the endpoint service to the specific account of the third-party SaaS provider.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about allow to call api from third party application which in different VPC, and traffic can not move through internet\n",
    "    - use AWS PrivateLink interface VPC endpoint. Connect this endpoint to the endpoint service that the third-party SaaS application provides\n",
    "    - reate a security group to limit the access to the endpoint. Associate the security group with the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #13\n",
    "A company needs to implement a patching process for its servers. The on-premises servers and Amazon EC2 instances use a variety of tools to perform patching. Management requires a single report showing the patch status of all the servers and instances.\n",
    "Which set of actions should a solutions architect take to meet these requirements?\n",
    "\n",
    "A. Use AWS Systems Manager to manage patches on the on-premises servers and EC2 instances. Use Systems Manager to generate patch compliance reports.\n",
    "\n",
    "B. Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use Amazon QuickSight integration with OpsWorks to generate patch compliance reports.\n",
    "\n",
    "C. Use an Amazon EventBridge rule to apply patches by scheduling an AWS Systems Manager patch remediation job. Use Amazon Inspector to generate patch compliance reports.\n",
    "\n",
    "D. Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use AWS X-Ray to post the patch status to AWS Systems Manager OpsCenter to generate patch compliance reports.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to manage patching job on both on-premises and ec2 instance -> use System Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #14\n",
    "\n",
    "A company is running an application on several Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. The load on the application varies throughout the day, and EC2 instances are scaled in and out on a regular basis. Log files from the EC2 instances are copied to a central Amazon S3 bucket every 15 minutes. The security team discovers that log files are missing from some of the terminated EC2 instances.\n",
    "Which set of actions will ensure that log files are copied to the central S3 bucket from the terminated EC2 instances?\n",
    "\n",
    "A. Create a script to copy log files to Amazon S3, and store the script in a file on the EC2 instance. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to send ABANDON to the Auto Scaling group to prevent termination, run the script to copy the log files, and terminate the instance using the AWS SDK.\n",
    "\n",
    "B. Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send CONTINUE to the Auto Scaling group to terminate the instance.\n",
    "\n",
    "C. Change the log delivery rate to every 5 minutes. Create a script to copy log files to Amazon S3, and add the script to EC2 instance user data. Create an Amazon EventBridge rule to detect EC2 instance termination. Invoke an AWS Lambda function from the EventBridge rule that uses the AWS CLI to run the user-data script to copy the log files and terminate the instance.\n",
    "\n",
    "D. Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook that publishes a message to an Amazon Simple Notification Service (Amazon SNS) topic. From the SNS notification, call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send ABANDON to the Auto Scaling group to terminate the instance.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to ensure that log files are copied to the central S3 bucket from the terminated EC2 instances\n",
    "    - Create an AWS Systems Manager document with a script to copy log files to Amazon S3 -> to copy logs to s3\n",
    "    - Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group -> to detact termitating event\n",
    "    - Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send CONTINUE to the Auto Scaling group to terminate the instance -> lambda function to sendCommand to run document to load logs to s3 and then continue when copy completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #15\n",
    "\n",
    "A company is using multiple AWS accounts. The DNS records are stored in a private hosted zone for Amazon Route 53 in Account A. The company’s applications and databases are running in Account B.\n",
    "A solutions architect will deploy a two-tier application in a new VPC. To simplify the configuration, the db.example.com CNAME record set for the Amazon RDS endpoint was created in a private hosted zone for Amazon Route 53.\n",
    "During deployment, the application failed to start. Troubleshooting revealed that db.example.com is not resolvable on the Amazon EC2 instance. The solutions architect confirmed that the record set was created correctly in Route 53.\n",
    "Which combination of steps should the solutions architect take to resolve this issue? (Choose two.)\n",
    "\n",
    "A. Deploy the database on a separate EC2 instance in the new VPC. Create a record set for the instance’s private IP in the private hosted zone.\n",
    "\n",
    "B. Use SSH to connect to the application tier EC2 instance. Add an RDS endpoint IP address to the /etc/resolv.conf file.\n",
    "\n",
    "C. Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B.\n",
    "\n",
    "D. Create a private hosted zone for the example com domain in Account B. Configure Route 53 replication between AWS accounts.\n",
    "\n",
    "E. Associate a new VPC in Account B with a hosted zone in Account A. Delete the association authorization in Account A.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C, E\n",
    "- the question is about dns record and application in 2 different account with two different VPC\n",
    "    - Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B -> we need authorization which allow vpc in account b to associate with private hosted zone in account A\n",
    "    - Associate a new VPC in Account B with a hosted zone in Account A. Delete the association authorization in Account A -> use the authentication to connect vpc of account b to private hosted zone of account a. delete authorization in account a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #16\n",
    "\n",
    "A company used Amazon EC2 instances to deploy a web fleet to host a blog site. The EC2 instances are behind an Application Load Balancer (ALB) and are configured in an Auto Scaling group. The web application stores all blog content on an Amazon EFS volume.\n",
    "The company recently added a feature for bloggers to add video to their posts, attracting 10 times the previous user traffic. At peak times of day, users report buffering and timeout issues while attempting to reach the site or watch videos.\n",
    "Which is the MOST cost-efficient and scalable deployment that will resolve the issues for users?\n",
    "\n",
    "A. Reconfigure Amazon EFS to enable maximum I/O.\n",
    "\n",
    "B. Update the blog site to use instance store volumes for storage. Copy the site contents to the volumes at launch and to Amazon S3 at shutdown.\n",
    "\n",
    "C. Configure an Amazon CloudFront distribution. Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3.\n",
    "\n",
    "D. Set up an Amazon CloudFront distribution for all site contents, and point the distribution at the ALB.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to host video content\n",
    "    - Configure an Amazon CloudFront distribution. Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #17\n",
    "A company with global offices has a single 1 Gbps AWS Direct Connect connection to a single AWS Region. The company’s on-premises network uses the connection to communicate with the company’s resources in the AWS Cloud. The connection has a single private virtual interface that connects to a single VPC.\n",
    "A solutions architect must implement a solution that adds a redundant Direct Connect connection in the same Region. The solution also must provide connectivity to other Regions through the same pair of Direct Connect connections as the company expands into other Regions.\n",
    "Which solution meets these requirements?\n",
    "\n",
    "A. Provision a Direct Connect gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect connection. Create a new private virtual interface on each connection, and connect both private virtual interfaces to the Direct Connect gateway. Connect the Direct Connect gateway to the single VPC.\n",
    "\n",
    "B. Keep the existing private virtual interface. Create the second Direct Connect connection. Create a new private virtual interface on the new connection, and connect the new private virtual interface to the single VPC.\n",
    "\n",
    "C. Keep the existing private virtual interface. Create the second Direct Connect connection. Create a new public virtual interface on the new connection, and connect the new public virtual interface to the single VPC.\n",
    "\n",
    "D. Provision a transit gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect connection. Create a new private virtual interface on each connection, and connect both private virtual interfaces to the transit gateway. Associate the transit gateway with the single VPC.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about networking in multiple regions with cloud and on-premises\n",
    "    - transition gateway is regional resource -> we can remove it\n",
    "    - use Direct Connect gateway (each resource in each region)\n",
    "    - create private virtual interface and connect with coresspondent direct connect gateway\n",
    "    - connect direct connect gateway with sing VPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #18\n",
    "\n",
    "A company has a web application that allows users to upload short videos. The videos are stored on Amazon EBS volumes and analyzed by custom recognition software for categorization.\n",
    "The website contains static content that has variable traffic with peaks in certain months. The architecture consists of Amazon EC2 instances running in an Auto Scaling group for the web application and EC2 instances running in an Auto Scaling group to process an Amazon SQS queue. The company wants to re-architect the application to reduce operational overhead using AWS managed services where possible and remove dependencies on third-party software.\n",
    "Which solution meets these requirements?\n",
    "\n",
    "A. Use Amazon ECS containers for the web application and Spot instances for the Auto Scaling group that processes the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos.\n",
    "\n",
    "B. Store the uploaded videos in Amazon EFS and mount the file system to the EC2 instances for the web application. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.\n",
    "\n",
    "C. Host the web application in Amazon S3. Store the uploaded videos in Amazon S3. Use S3 event notification to publish events to the SQS queue. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.\n",
    "\n",
    "D. Use AWS Elastic Beanstalk to launch EC2 instances in an Auto Scaling group for the web application and launch a worker environment to process the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the question is about re-architect the application to reduce operational overhead using AWS managed services where possible and remove dependencies on third-party software\n",
    "    - Use AWS Elastic Beanstalk to launch EC2 instances in an Auto Scaling group for the web application and launch a worker environment to process the SQS queue\n",
    "    - Replace the custom software with Amazon Rekognition to categorize the videos\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #19\n",
    "A company has a serverless application comprised of Amazon CloudFront, Amazon API Gateway, and AWS Lambda functions. The current deployment process of the application code is to create a new version number of the Lambda function and run an AWS CLI script to update. If the new function version has errors, another CLI script reverts by deploying the previous working version of the function. The company would like to decrease the time to deploy new versions of the application logic provided by the Lambda functions, and also reduce the time to detect and revert when errors are identified.\n",
    "How can this be accomplished?\n",
    "\n",
    "A. Create and deploy nested AWS CloudFormation stacks with the parent stack consisting of the AWS CloudFront distribution and API Gateway, and the child stack containing the Lambda function. For changes to Lambda, create an AWS CloudFormation change set and deploy; if errors are triggered, revert the AWS CloudFormation change set to the previous version.\n",
    "\n",
    "B. Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift traffic to the new version, and use pre-traffic and post-traffic test functions to verify code. Rollback if Amazon CloudWatch alarms are triggered.\n",
    "\n",
    "C. Refactor the AWS CLI scripts into a single script that deploys the new Lambda version. When deployment is completed, the script tests execute. If errors are detected, revert to the previous Lambda version.\n",
    "\n",
    "D. Create and deploy an AWS CloudFormation stack that consists of a new API Gateway endpoint that references the new Lambda version. Change the CloudFront origin to the new API Gateway endpoint, monitor errors and if detected, change the AWS CloudFront origin to the previous API Gateway endpoint.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #20\n",
    "A company is planning to store a large number of archived documents and make the documents available to employees through the corporate intranet. Employees will access the system by connecting through a client VPN service that is attached to a VPC. The data must not be accessible to the public.\n",
    "The documents that the company is storing are copies of data that is held on physical media elsewhere. The number of requests will be low. Availability and speed of retrieval are not concerns of the company.\n",
    "Which solution will meet these requirements at the LOWEST cost?\n",
    "\n",
    "A. Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint.\n",
    "\n",
    "B. Launch an Amazon EC2 instance that runs a web server. Attach an Amazon Elastic File System (Amazon EFS) file system to store the archived data in the EFS One Zone-Infrequent Access (EFS One Zone-IA) storage class Configure the instance security groups to allow access only from private networks.\n",
    "\n",
    "C. Launch an Amazon EC2 instance that runs a web server Attach an Amazon Elastic Block Store (Amazon EBS) volume to store the archived data. Use the Cold HDD (sc1) volume type. Configure the instance security groups to allow access only from private networks.\n",
    "\n",
    "D. Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 Glacier Deep Archive storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about store document for archive purpose. The documents is access throug client-server protocal\n",
    "    - s3 can not access through client-server protocal -> remove D\n",
    "    - correct answer is A, \n",
    "    - Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class as default\n",
    "    - Configure the S3 bucket for website hosting\n",
    "    - Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #21\n",
    "A company is using an on-premises Active Directory service for user authentication. The company wants to use the same authentication service to sign in to the company’s AWS accounts, which are using AWS Organizations. AWS Site-to-Site VPN connectivity already exists between the on-premises environment and all the company’s AWS accounts.\n",
    "The company’s security policy requires conditional access to the accounts based on user groups and roles. User identities must be managed in a single location.\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Configure AWS IAM Identity Center (AWS Single Sign-On) to connect to Active Directory by using SAML 2.0. Enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protocol. Grant access to the AWS accounts by using attribute-based access controls (ABACs).\n",
    "\n",
    "B. Configure AWS IAM Identity Center (AWS Single Sign-On) by using IAM Identity Center as an identity source. Enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protocol. Grant access to the AWS accounts by using IAM Identity Center permission sets.\n",
    "\n",
    "C. In one of the company’s AWS accounts, configure AWS Identity and Access Management (IAM) to use a SAML 2.0 identity provider. Provision IAM users that are mapped to the federated users. Grant access that corresponds to appropriate groups in Active Directory. Grant access to the required AWS accounts by using cross-account IAM users.\n",
    "\n",
    "D. In one of the company’s AWS accounts, configure AWS Identity and Access Management (IAM) to use an OpenID Connect (OIDC) identity provider. Provision IAM roles that grant access to the AWS account for the federated users that correspond to appropriate groups in Active Directory. Grant access to the required AWS accounts by using cross-account IAM roles.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the questin is about using on-premises Active Directory service as identity provider\n",
    "    - Configure AWS IAM Identity Center (AWS Single Sign-On) to connect to Active Directory by using SAML 2.0\n",
    "    - Enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protocol\n",
    "    - Grant access to the AWS accounts by using attribute-based access controls (ABACs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #22\n",
    "\n",
    "A software company has deployed an application that consumes a REST API by using Amazon API Gateway, AWS Lambda functions, and an Amazon DynamoDB table. The application is showing an increase in the number of errors during PUT requests. Most of the PUT calls come from a small number of clients that are authenticated with specific API keys.\n",
    "A solutions architect has identified that a large number of the PUT requests originate from one client. The API is noncritical, and clients can tolerate retries of unsuccessful calls. However, the errors are displayed to customers and are causing damage to the API’s reputation.\n",
    "What should the solutions architect recommend to improve the customer experience?\n",
    "\n",
    "A. Implement retry logic with exponential backoff and irregular variation in the client application. Ensure that the errors are caught and handled with descriptive error messages.\n",
    "\n",
    "B. Implement API throttling through a usage plan at the API Gateway level. Ensure that the client application handles code 429 replies without error.\n",
    "\n",
    "C. Turn on API caching to enhance responsiveness for the production stage. Run 10-minute load tests. Verify that the cache capacity is appropriate for the workload.\n",
    "\n",
    "D. Implement reserved concurrency at the Lambda function level to provide the resources that are needed during sudden increases in traffic.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to deal with small percent of customer make a lot of request which cause error\n",
    "- API throttling is a technique that can be used to control the rate of requests to an API. This can be useful in situations where a small number of clients are making a large number of requests, which is causing errors. By implementing API throttling through a usage plan at the API Gateway level, the solutions architect can limit the number of requests that a client can make, which will help to reduce the number of errors.\n",
    "It's important that the client application handles the code 429 replies without error, this will help to improve the customer experience by reducing the number of errors that are displayed to customers. Additionally, it will prevent the API's reputation from being damaged by the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #23\n",
    "A company is running a data-intensive application on AWS. The application runs on a cluster of hundreds of Amazon EC2 instances. A shared file system also runs on several EC2 instances that store 200 TB of data. The application reads and modifies the data on the shared file system and generates a report. The job runs once monthly, reads a subset of the files from the shared file system, and takes about 72 hours to complete. The compute instances scale in an Auto Scaling group, but the instances that host the shared file system run continuously. The compute and storage instances are all in the same AWS Region.\n",
    "A solutions architect needs to reduce costs by replacing the shared file system instances. The file system must provide high performance access to the needed data for the duration of the 72-hour run.\n",
    "Which solution will provide the LARGEST overall cost reduction while meeting these requirements?\n",
    "\n",
    "A. Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Intelligent-Tiering storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using lazy loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.\n",
    "\n",
    "B. Migrate the data from the existing shared file system to a large Amazon Elastic Block Store (Amazon EBS) volume with Multi-Attach enabled. Attach the EBS volume to each of the instances by using a user data script in the Auto Scaling group launch template. Use the EBS volume as the shared storage for the duration of the job. Detach the EBS volume when the job is complete\n",
    "\n",
    "C. Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Standard storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using batch loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.\n",
    "\n",
    "D. Migrate the data from the existing shared file system to an Amazon S3 bucket. Before the job runs each month, use AWS Storage Gateway to create a file gateway with the data from Amazon S3. Use the file gateway as the shared storage for the job. Delete the file gateway when the job is complete.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to design solution which cost saving when the job only run month and process subset of the data in the file system\n",
    "    - Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Intelligent-Tiering storage class.\n",
    "    - Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using lazy loading -> choose active data when needed\n",
    "    - Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #24\n",
    "A company is developing a new service that will be accessed using TCP on a static port. A solutions architect must ensure that the service is highly available, has redundancy across Availability Zones, and is accessible using the DNS name my.service.com, which is publicly accessible. The service must use fixed address assignments so other companies can add the addresses to their allow lists.\n",
    "Assuming that resources are deployed in multiple Availability Zones in a single Region, which solution will meet these requirements?\n",
    "\n",
    "A. Create Amazon EC2 instances with an Elastic IP address for each instance. Create a Network Load Balancer (NLB) and expose the static TCP port. Register EC2 instances with the NLB. Create a new name server record set named my.service.com, and assign the Elastic IP addresses of the EC2 instances to the record set. Provide the Elastic IP addresses of the EC2 instances to the other companies to add to their allow lists.\n",
    "\n",
    "B. Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP addresses for the ECS cluster. Create a Network Load Balancer (NLB) and expose the TCP port. Create a target group and assign the ECS cluster name to the NLCreate a new A record set named my.service.com, and assign the public IP addresses of the ECS cluster to the record set. Provide the public IP addresses of the ECS cluster to the other companies to add to their allow lists.\n",
    "\n",
    "C. Create Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone. Create a Network Load Balancer (NLB) and expose the assigned TCP port. Assign the Elastic IP addresses to the NLB for each Availability Zone. Create a target group and register the EC2 instances with the NLB. Create a new A (alias) record set named my.service.com, and assign the NLB DNS name to the record set.\n",
    "\n",
    "D. Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP address for each host in the cluster. Create an Application Load Balancer (ALB) and expose the static TCP port. Create a target group and assign the ECS service definition name to the ALB. Create a new CNAME record set and associate the public IP addresses to the record set. Provide the Elastic IP addresses of the Amazon EC2 instances to the other companies to add to their allow lists.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about design network solution for tcp application\n",
    "    - tcp traffic application -> network load balancer\n",
    "    - multiple available zone -> Create one Elastic IP address for each Availability Zone. Assign the Elastic IP addresses to the NLB for each Availability Zone. Create a target group and register the EC2 instances with the NLB. Create a new A (alias) record set named my.service.com, and assign the NLB DNS name to the record set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #25\n",
    "\n",
    "A company uses an on-premises data analytics platform. The system is highly available in a fully redundant configuration across 12 servers in the company’s data center.\n",
    "The system runs scheduled jobs, both hourly and daily, in addition to one-time requests from users. Scheduled jobs can take between 20 minutes and 2 hours to finish running and have tight SLAs. The scheduled jobs account for 65% of the system usage. User jobs typically finish running in less than 5 minutes and have no SLA. The user jobs account for 35% of system usage. During system failures, scheduled jobs must continue to meet SLAs. However, user jobs can be delayed.\n",
    "A solutions architect needs to move the system to Amazon EC2 instances and adopt a consumption-based model to reduce costs with no long-term commitments. The solution must maintain high availability and must not affect the SLAs.\n",
    "Which solution will meet these requirements MOST cost-effectively?\n",
    "\n",
    "A. Split the 12 instances across two Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand Instances with Capacity Reservations. Run four instances in each Availability Zone as Spot Instances.\n",
    "\n",
    "B. Split the 12 instances across three Availability Zones in the chosen AWS Region. In one of the Availability Zones, run all four instances as On-Demand Instances with Capacity Reservations. Run the remaining instances as Spot Instances.\n",
    "\n",
    "C. Split the 12 instances across three Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand Instances with a Savings Plan. Run two instances in each Availability Zone as Spot Instances.\n",
    "\n",
    "D. Split the 12 instances across three Availability Zones in the chosen AWS Region. Run three instances in each Availability Zone as On-Demand Instances with Capacity Reservations. Run one instance in each Availability Zone as a Spot Instance.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the question is design solution for analytic work which required redundancy, sla (Service Level Agreement) for scheudled work\n",
    "    - Split the 12 instances across three Availability Zones in the chosen AWS Region -> redundancy\n",
    "    - Run three instances in each Availability Zone as On-Demand Instances with Capacity Reservations. Run one instance in each Availability Zone as a Spot Instance -> on-demen (pay as you go and more stable) -> handle SLA work load. Sport instance for short job and no SLA work load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #26\n",
    "\n",
    "A security engineer determined that an existing application retrieves credentials to an Amazon RDS for MySQL database from an encrypted file in Amazon S3. For the next version of the application, the security engineer wants to implement the following application design changes to improve security:\n",
    "The database must use strong, randomly generated passwords stored in a secure AWS managed service.\n",
    "The application resources must be deployed through AWS CloudFormation.\n",
    "The application must rotate credentials for the database every 90 days.\n",
    "A solutions architect will generate a CloudFormation template to deploy the application.\n",
    "Which resources specified in the CloudFormation template will meet the security engineer’s requirements with the LEAST amount of operational overhead?\n",
    "\n",
    "A. Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Specify a Secrets Manager RotationSchedule resource to rotate the database password every 90 days.\n",
    "\n",
    "B. Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Create an AWS Lambda function resource to rotate the database password. Specify a Parameter Store RotationSchedule resource to rotate the database password every 90 days.\n",
    "\n",
    "C. Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Create an Amazon EventBridge scheduled rule resource to trigger the Lambda function password rotation every 90 days.\n",
    "\n",
    "D. Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Specify an AWS AppSync DataSource resource to automatically rotate the database password every 90 days.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about manage databse secret using aws service\n",
    "    - Generate the database password as a secret resource using AWS Secrets Manager\n",
    "    - Create an AWS Lambda function resource to rotate the database password\n",
    "    - Specify a Secrets Manager RotationSchedule resource to rotate the database password every 90 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #27\n",
    "A company is storing data in several Amazon DynamoDB tables. A solutions architect must use a serverless architecture to make the data accessible publicly through a simple API over HTTPS. The solution must scale automatically in response to demand.\n",
    "Which solutions meet these requirements? (Choose two.)\n",
    "\n",
    "A. Create an Amazon API Gateway REST API. Configure this API with direct integrations to DynamoDB by using API Gateway’s AWS integration type.\n",
    "\n",
    "B. Create an Amazon API Gateway HTTP API. Configure this API with direct integrations to Dynamo DB by using API Gateway’s AWS integration type.\n",
    "\n",
    "C. Create an Amazon API Gateway HTTP API. Configure this API with integrations to AWS Lambda functions that return data from the DynamoDB tables.\n",
    "\n",
    "D. Create an accelerator in AWS Global Accelerator. Configure this accelerator with AWS Lambda@Edge function integrations that return data from the DynamoDB tables.\n",
    "\n",
    "E. Create a Network Load Balancer. Configure listener rules to forward requests to the appropriate AWS Lambda functions.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, C\n",
    "- the question is about design solution to publicly access DynamoDB tables\n",
    "    - Create an Amazon API Gateway REST API. Configure this API with direct integrations to DynamoDB by using API Gateway’s AWS integration type -> provide private access to DynamoDb\n",
    "    - Create an Amazon API Gateway HTTP API. Configure this API with integrations to AWS Lambda functions that return data from the DynamoDB tables -> provide public access through lamda function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #28\n",
    "\n",
    "A company has registered 10 new domain names. The company uses the domains for online marketing. The company needs a solution that will redirect online visitors to a specific URL for each domain. All domains and target URLs are defined in a JSON document. All DNS records are managed by Amazon Route 53.\n",
    "A solutions architect must implement a redirect service that accepts HTTP and HTTPS requests.\n",
    "Which combination of steps should the solutions architect take to meet these requirements with the LEAST amount of operational effort? (Choose three.)\n",
    "\n",
    "A. Create a dynamic webpage that runs on an Amazon EC2 instance. Configure the webpage to use the JSON document in combination with the event message to look up and respond with a redirect URL.\n",
    "\n",
    "B. Create an Application Load Balancer that includes HTTP and HTTPS listeners.\n",
    "\n",
    "C. Create an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a redirect URL.\n",
    "\n",
    "D. Use an Amazon API Gateway API with a custom domain to publish an AWS Lambda function.\n",
    "\n",
    "E. Create an Amazon CloudFront distribution. Deploy a Lambda@Edge function.\n",
    "\n",
    "F. Create an SSL certificate by using AWS Certificate Manager (ACM). Include the domains as Subject Alternative Names.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C, E, F\n",
    "- the question is about disign redirection solution which is least operational effort\n",
    "    - Create an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a redirect URL -> lambda function to find which url to direct to\n",
    "    - Create an Amazon CloudFront distribution. Deploy a Lambda@Edge function -> run lambda functio\n",
    "    - Create an SSL certificate by using AWS Certificate Manager (ACM). Include the domains as Subject Alternative Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #29\n",
    "\n",
    "A company that has multiple AWS accounts is using AWS Organizations. The company’s AWS accounts host VPCs, Amazon EC2 instances, and containers.\n",
    "The company’s compliance team has deployed a security tool in each VPC where the company has deployments. The security tools run on EC2 instances and send information to the AWS account that is dedicated for the compliance team. The company has tagged all the compliance-related resources with a key of “costCenter” and a value or “compliance”.\n",
    "The company wants to identify the cost of the security tools that are running on the EC2 instances so that the company can charge the compliance team’s AWS account. The cost calculation must be as accurate as possible.\n",
    "What should a solutions architect do to meet these requirements?\n",
    "\n",
    "A. In the management account of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Use the tag breakdown in the report to obtain the total cost for the costCenter tagged resources.\n",
    "\n",
    "B. In the member accounts of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Schedule a monthly AWS Lambda function to retrieve the reports and calculate the total cost for the costCenter tagged resources.\n",
    "\n",
    "C. In the member accounts of the organization activate the costCenter user-defined tag. From the management account, schedule a monthly AWS Cost and Usage Report. Use the tag breakdown in the report to calculate the total cost for the costCenter tagged resources.\n",
    "\n",
    "D. Create a custom report in the organization view in AWS Trusted Advisor. Configure the report to generate a monthly billing summary for the costCenter tagged resources in the compliance team’s AWS account.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to allocate cost\n",
    "    - In the management account of the organization, activate the costCenter user-defined tag\n",
    "    - Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Use the tag breakdown in the report to obtain the total cost for the costCenter tagged resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #30\n",
    "A company has 50 AWS accounts that are members of an organization in AWS Organizations. Each account contains multiple VPCs. The company wants to use AWS Transit Gateway to establish connectivity between the VPCs in each member account. Each time a new member account is created, the company wants to automate the process of creating a new VPC and a transit gateway attachment.\n",
    "Which combination of steps will meet these requirements? (Choose two.)\n",
    "\n",
    "A. From the management account, share the transit gateway with member accounts by using AWS Resource Access Manager.\n",
    "\n",
    "B. From the management account, share the transit gateway with member accounts by using an AWS Organizations SCP.\n",
    "\n",
    "C. Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a VPC transit gateway attachment in a member account. Associate the attachment with the transit gateway in the management account by using the transit gateway ID.\n",
    "\n",
    "D. Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a peering transit gateway attachment in a member account. Share the attachment with the transit gateway in the management account by using a transit gateway service-linked role.\n",
    "\n",
    "E. From the management account, share the transit gateway with member accounts by using AWS Service Catalog.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, C\n",
    "- the question is about design network with VPC and transit gateway\n",
    "    - From the management account, share the transit gateway with member accounts by using AWS Resource Access Manager\n",
    "    - Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a VPC transit gateway attachment in a member account -> we have transit gateway, VPC + VPC transit gateway attachement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #31\n",
    "\n",
    "An enterprise company wants to allow its developers to purchase third-party software through AWS Marketplace. The company uses an AWS Organizations account structure with full features enabled, and has a shared services account in each organizational unit (OU) that will be used by procurement managers. The procurement team’s policy indicates that developers should be able to obtain third-party software from an approved list only and use Private Marketplace in AWS Marketplace to achieve this requirement. The procurement team wants administration of Private Marketplace to be restricted to a role named procurement-manager-role, which could be assumed by procurement managers. Other IAM users, groups, roles, and account administrators in the company should be denied Private Marketplace administrative access.\n",
    "What is the MOST efficient way to design an architecture to meet these requirements?\n",
    "\n",
    "A. Create an IAM role named procurement-manager-role in all AWS accounts in the organization. Add the PowerUserAccess managed policy to the role. Apply an inline policy to all IAM users and roles in every AWS account to deny permissions on the AWSPrivateMarketplaceAdminFullAccess managed policy.\n",
    "\n",
    "B. Create an IAM role named procurement-manager-role in all AWS accounts in the organization. Add the AdministratorAccess managed policy to the role. Define a permissions boundary with the AWSPrivateMarketplaceAdminFullAccess managed policy and attach it to all the developer roles.\n",
    "\n",
    "C. Create an IAM role named procurement-manager-role in all the shared services accounts in the organization. Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role. Create an organization root-level SCP to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role. Create another organization root-level SCP to deny permissions to create an IAM role named procurement-manager-role to everyone in the organization.\n",
    "\n",
    "D. Create an IAM role named procurement-manager-role in all AWS accounts that will be used by developers. Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role. Create an SCP in Organizations to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role. Apply the SCP to all the shared services accounts in the organization.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about deisng Private market with a list of software\n",
    "    - Create an IAM role named procurement-manager-role in all the shared services accounts in the organization\n",
    "    - Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role\n",
    "    - Create an organization root-level SCP to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role\n",
    "    - Create another organization root-level SCP to deny permissions to create an IAM role named procurement-manager-role to everyone in the organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #32\n",
    "A company is in the process of implementing AWS Organizations to constrain its developers to use only Amazon EC2, Amazon S3, and Amazon DynamoDB. The developers account resides in a dedicated organizational unit (OU). The solutions architect has implemented the following SCP on the developers account:\n",
    "When this policy is deployed, IAM users in the developers account are still able to use AWS services that are not listed in the policy.\n",
    "What should the solutions architect do to eliminate the developers’ ability to use services outside the scope of this policy?\n",
    "\n",
    "A. Create an explicit deny statement for each AWS service that should be constrained.\n",
    "B. Remove the FullAWSAccess SCP from the developers account’s OU.\n",
    "C. Modify the FullAWSAccess SCP to explicitly deny all services.\n",
    "D. Add an explicit deny statement using a wildcard to the end of the SCP.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about SCP in a specific account\n",
    "    - remove default full access of SCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #33\n",
    "\n",
    "A company is hosting a monolithic REST-based API for a mobile app on five Amazon EC2 instances in public subnets of a VPC. Mobile clients connect to the API by using a domain name that is hosted on Amazon Route 53. The company has created a Route 53 multivalue answer routing policy with the IP addresses of all the EC2 instances. Recently, the app has been overwhelmed by large and sudden increases to traffic. The app has not been able to keep up with the traffic.\n",
    "A solutions architect needs to implement a solution so that the app can handle the new and varying load.\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Separate the API into individual AWS Lambda functions. Configure an Amazon API Gateway REST API with Lambda integration for the backend. Update the Route 53 record to point to the API Gateway API.\n",
    "\n",
    "B. Containerize the API logic. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Run the containers in the cluster by using Amazon EC2. Create a Kubernetes ingress. Update the Route 53 record to point to the Kubernetes ingress.\n",
    "\n",
    "C. Create an Auto Scaling group. Place all the EC2 instances in the Auto Scaling group. Configure the Auto Scaling group to perform scaling actions that are based on CPU utilization. Create an AWS Lambda function that reacts to Auto Scaling group changes and updates the Route 53 record.\n",
    "\n",
    "D. Create an Application Load Balancer (ALB) in front of the API. Move the EC2 instances to private subnets in the VPC. Add the EC2 instances as targets for the ALB. Update the Route 53 record to point to the ALB.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to design a solution to handle deman with least operational overhead\n",
    "    - lambda function + api gateway + update rout 53 record point to api gatewate\n",
    "    - remove option D because it is not mention Scaling Group for scaling and  ALB (Application Load Balancer) do not work directly with EC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #34\n",
    "A company has created an OU in AWS Organizations for each of its engineering teams. Each OU owns multiple AWS accounts. The organization has hundreds of AWS accounts.\n",
    "A solutions architect must design a solution so that each OU can view a breakdown of usage costs across its AWS accounts.\n",
    "Which solution meets these requirements?\n",
    "\n",
    "A. Create an AWS Cost and Usage Report (CUR) for each OU by using AWS Resource Access Manager. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.\n",
    "\n",
    "B. Create an AWS Cost and Usage Report (CUR) from the AWS Organizations management account. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.\n",
    "\n",
    "C. Create an AWS Cost and Usage Report (CUR) in each AWS Organizations member account. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.\n",
    "\n",
    "D. Create an AWS Cost and Usage Report (CUR) by using AWS Systems Manager. Allow each team to visualize the CUR through Systems Manager OpsCenter dashboards.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about manage and view cost in Organization with multiple Ou and account\n",
    "    - we need centralize management account\n",
    "    - use QuickSight dashboard to review cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #35\n",
    "A company is storing data on premises on a Windows file server. The company produces 5 GB of new data daily.\n",
    "The company migrated part of its Windows-based workload to AWS and needs the data to be available on a file system in the cloud. The company already has established an AWS Direct Connect connection between the on-premises network and AWS.\n",
    "Which data migration strategy should the company use?\n",
    "\n",
    "A. Use the file gateway option in AWS Storage Gateway to replace the existing Windows file server, and point the existing file share to the new file gateway.\n",
    "\n",
    "B. Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon FSx.\n",
    "\n",
    "C. Use AWS Data Pipeline to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS).\n",
    "\n",
    "D. Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS).\n",
    "\n",
    "Solution:\n",
    "- solution is B\n",
    "- the question is about how to sync data between Windows file server and aws file system\n",
    "    - EFS only work with Linux File System therefore we go with FSx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #36\n",
    "A company’s solutions architect is reviewing a web application that runs on AWS. The application references static assets in an Amazon S3 bucket in the us-east-1 Region. The company needs resiliency across multiple AWS Regions. The company already has created an S3 bucket in a second Region.\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Configure the application to write each object to both S3 buckets. Set up an Amazon Route 53 public hosted zone with a record set by using a weighted routing policy for each S3 bucket. Configure the application to reference the objects by using the Route 53 DNS name.\n",
    "\n",
    "B. Create an AWS Lambda function to copy objects from the S3 bucket in us-east-1 to the S3 bucket in the second Region. Invoke the Lambda function each time an object is written to the S3 bucket in us-east-1. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins.\n",
    "\n",
    "C. Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins.\n",
    "\n",
    "D. Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. If failover is required, update the application code to load S3 objects from the S3 bucket in the second Region.\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about replica static assets in s3 bucket with multiple region\n",
    "    - using replica and then use CloudFront with multiple origin for the asset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #37\n",
    "A company is hosting a three-tier web application in an on-premises environment. Due to a recent surge in traffic that resulted in downtime and a significant financial impact, company management has ordered that the application be moved to AWS. The application is written in .NET and has a dependency on a MySQL database. A solutions architect must design a scalable and highly available solution to meet the demand of 200,000 daily users.\n",
    "Which steps should the solutions architect take to design an appropriate solution?\n",
    "\n",
    "A. Use AWS Elastic Beanstalk to create a new application with a web server environment and an Amazon RDS MySQL Multi-AZ DB instance. The environment should launch a Network Load Balancer (NLB) in front of an Amazon EC2 Auto Scaling group in multiple Availability Zones. Use an Amazon Route 53 alias record to route traffic from the company’s domain to the NLB.\n",
    "\n",
    "B. Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group spanning three Availability Zones. The stack should launch a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a Retain deletion policy. Use an Amazon Route 53 alias record to route traffic from the company’s domain to the ALB.\n",
    "\n",
    "C. Use AWS Elastic Beanstalk to create an automatically scaling web server environment that spans two separate Regions with an Application Load Balancer (ALB) in each Region. Create a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a cross-Region read replica. Use Amazon Route 53 with a geoproximity routing policy to route traffic between the two Regions.\n",
    "\n",
    "D. Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon ECS cluster of Spot instances spanning three Availability Zones. The stack should launch an Amazon RDS MySQL DB instance with a Snapshot deletion policy. Use an Amazon Route 53 alias record to route traffic from the company’s domain to the ALB.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "    - use CloudFormation to define stack\n",
    "    - Amazone Route 53 -> ALB -> Auto Scaling Group + Multi-AZ Aurora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #38\n",
    "A company is using AWS Organizations to manage multiple AWS accounts. For security purposes, the company requires the creation of an Amazon Simple Notification Service (Amazon SNS) topic that enables integration with a third-party alerting system in all the Organizations member accounts.\n",
    "A solutions architect used an AWS CloudFormation template to create the SNS topic and stack sets to automate the deployment of CloudFormation stacks. Trusted access has been enabled in Organizations.\n",
    "What should the solutions architect do to deploy the CloudFormation StackSets in all AWS accounts?\n",
    "\n",
    "A. Create a stack set in the Organizations member accounts. Use service-managed permissions. Set deployment options to deploy to an organization. Use CloudFormation StackSets drift detection.\n",
    "\n",
    "B. Create stacks in the Organizations member accounts. Use self-service permissions. Set deployment options to deploy to an organization. Enable the CloudFormation StackSets automatic deployment.\n",
    "\n",
    "C. Create a stack set in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets automatic deployment.\n",
    "\n",
    "D. Create stacks in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets drift detection.\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to enable SNS (Simple Notification Service) with a third-party alerting system in all the Oranization member account\n",
    "    - we need centralized solution for notification and alerting\n",
    "    - we need stack set to automate deployment of CloudFormation stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #39\n",
    "A company wants to migrate its workloads from on premises to AWS. The workloads run on Linux and Windows. The company has a large on-premises infrastructure that consists of physical machines and VMs that host numerous applications.\n",
    "\n",
    "The company must capture details about the system configuration, system performance, running processes, and network connections of its on-premises workloads. The company also must divide the on-premises applications into groups for AWS migrations. The company needs recommendations for Amazon EC2 instance types so that the company can run its workloads on AWS in the most cost-effective manner.\n",
    "\n",
    "Which combination of steps should a solutions architect take to meet these requirements? (Choose three.)\n",
    "\n",
    "A. Assess the existing applications by installing AWS Application Discovery Agent on the physical machines and VMs.\n",
    "\n",
    "B. Assess the existing applications by installing AWS Systems Manager Agent on the physical machines and VMs.\n",
    "\n",
    "C. Group servers into applications for migration by using AWS Systems Manager Application Manager.\n",
    "\n",
    "D. Group servers into applications for migration by using AWS Migration Hub.\n",
    "\n",
    "E. Generate recommended instance types and associated costs by using AWS Migration Hub.\n",
    "\n",
    "F. Import data about server sizes into AWS Trusted Advisor. Follow the recommendations for cost optimization.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, D, E\n",
    "    - Assess the existing applications by installing AWS Application Discovery Agent on the physical machines and VMs\n",
    "    - Group servers into applications for migration by using AWS Migration Hub\n",
    "    - Generate recommended instance types and associated costs by using AWS Migration Hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #40\n",
    "A company is hosting an image-processing service on AWS in a VPC. The VPC extends across two Availability Zones. Each Availability Zone contains one public subnet and one private subnet.\n",
    "\n",
    "The service runs on Amazon EC2 instances in the private subnets. An Application Load Balancer in the public subnets is in front of the service. The service needs to communicate with the internet and does so through two NAT gateways. The service uses Amazon S3 for image storage. The EC2 instances retrieve approximately 1 ТВ of data from an S3 bucket each day.\n",
    "\n",
    "The company has promoted the service as highly secure. A solutions architect must reduce cloud expenditures as much as possible without compromising the service’s security posture or increasing the time spent on ongoing operations.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Replace the NAT gateways with NAT instances. In the VPC route table, create a route from the private subnets to the NAT instances.\n",
    "\n",
    "B. Move the EC2 instances to the public subnets. Remove the NAT gateways.\n",
    "\n",
    "C. Set up an S3 gateway VPC endpoint in the VPAttach an endpoint policy to the endpoint to allow the required actions on the S3 bucket.\n",
    "\n",
    "D. Attach an Amazon Elastic File System (Amazon EFS) volume to the EC2 instances. Host the images on the EFS volume.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- Setting up an S3 gateway VPC endpoint in the VPC and attaching an endpoint policy to the endpoint will allow the EC2 instances to securely access the S3 bucket for image storage without the need for NAT gateways, reducing costs without compromising security or increasing ongoing operations. This option reduces the costs associated with the NAT gateways and allows for faster data retrieval from the S3 bucket as traffic does not have to go through the internet gateway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #41\n",
    "A company recently deployed an application on AWS. The application uses Amazon DynamoDB. The company measured the application load and configured the RCUs and WCUs on the DynamoDB table to match the expected peak load. The peak load occurs once a week for a 4-hour period and is double the average load. The application load is close to the average load for the rest of the week. The access pattern includes many more writes to the table than reads of the table.\n",
    "\n",
    "A solutions architect needs to implement a solution to minimize the cost of the table.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Use AWS Application Auto Scaling to increase capacity during the peak period. Purchase reserved RCUs and WCUs to match the average load.\n",
    "\n",
    "B. Configure on-demand capacity mode for the table.\n",
    "\n",
    "C. Configure DynamoDB Accelerator (DAX) in front of the table. Reduce the provisioned read capacity to match the new peak load on the table.\n",
    "\n",
    "D. Configure DynamoDB Accelerator (DAX) in front of the table. Configure on-demand capacity mode for the table.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about scaling config for DynamoDB\n",
    "    - Use AWS Application Auto Scaling to increase capacity during the peak period\n",
    "    - Purchase reserved RCUs and WCUs to match the average load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #42\n",
    "A solutions architect needs to advise a company on how to migrate its on-premises data processing application to the AWS Cloud. Currently, users upload input files through a web portal. The web server then stores the uploaded files on NAS and messages the processing server over a message queue. Each media file can take up to 1 hour to process. The company has determined that the number of media files awaiting processing is significantly higher during business hours, with the number of files rapidly declining after business hours.\n",
    "\n",
    "What is the MOST cost-effective migration recommendation?\n",
    "\n",
    "A. Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in an Amazon S3 bucket.\n",
    "\n",
    "B. Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, create a new Amazon EC2 instance to pull requests from the queue and process the files. Store the processed files in Amazon EFS. Shut down the EC2 instance after the task is complete.\n",
    "\n",
    "C. Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in Amazon EFS.\n",
    "\n",
    "D. Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. Use Amazon EC2 instances in an EC2 Auto Scaling group to pull requests from the queue and process the files. Scale the EC2 instances based on the SQS queue length. Store the processed files in an Amazon S3 bucket.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the question is about migrate data-processing application to aws. \n",
    "    - create new queue for the application messages\n",
    "    - scale with scaling group\n",
    "    - store processed files with Amazon EFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #43\n",
    "A company is using Amazon OpenSearch Service to analyze data. The company loads data into an OpenSearch Service cluster with 10 data nodes from an Amazon S3 bucket that uses S3 Standard storage. The data resides in the cluster for 1 month for read-only analysis. After 1 month, the company deletes the index that contains the data from the cluster. For compliance purposes, the company must retain a copy of all input data.\n",
    "\n",
    "The company is concerned about ongoing costs and asks a solutions architect to recommend a new solution.\n",
    "\n",
    "Which solution will meet these requirements MOST cost-effectively?\n",
    "\n",
    "A. Replace all the data nodes with UltraWarm nodes to handle the expected capacity. Transition the input data from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster.\n",
    "\n",
    "B. Reduce the number of data nodes in the cluster to 2 Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Transition the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy.\n",
    "\n",
    "C. Reduce the number of data nodes in the cluster to 2. Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Add cold storage nodes to the cluster Transition the indexes from UltraWarm to cold storage. Delete the input data from the S3 bucket after 1 month by using an S3 Lifecycle policy.\n",
    "\n",
    "D. Reduce the number of data nodes in the cluster to 2. Add instance-backed data nodes to handle the expected capacity. Transition the input data from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster.\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about reduce cost when using OpenSearch\n",
    "    - Reduce the number of data nodes in the cluster to 2 Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data -> reduce cost\n",
    "    - Transition the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #44\n",
    "A company has 10 accounts that are part of an organization in AWS Organizations. AWS Config is configured in each account. All accounts belong to either the Prod OU or the NonProd OU.\n",
    "\n",
    "The company has set up an Amazon EventBridge rule in each AWS account to notify an Amazon Simple Notification Service (Amazon SNS) topic when an Amazon EC2 security group inbound rule is created with 0.0.0.0/0 as the source. The company’s security team is subscribed to the SNS topic.\n",
    "\n",
    "For all accounts in the NonProd OU, the security team needs to remove the ability to create a security group inbound rule that includes 0.0.0.0/0 as the source.\n",
    "\n",
    "Which solution will meet this requirement with the LEAST operational overhead?\n",
    "\n",
    "A. Modify the EventBridge rule to invoke an AWS Lambda function to remove the security group inbound rule and to publish to the SNS topic. Deploy the updated rule to the NonProd OU.\n",
    "\n",
    "B. Add the vpc-sg-open-only-to-authorized-ports AWS Config managed rule to the NonProd OU.\n",
    "\n",
    "C. Configure an SCP to allow the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is not 0.0.0.0/0. Apply the SCP to the NonProd OU.\n",
    "\n",
    "D. Configure an SCP to deny the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0. Apply the SCP to the NonProd OU.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- using SCP deny policy at NonProd Ou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #45\n",
    "\n",
    "A company hosts a Git repository in an on-premises data center. The company uses webhooks to invoke functionality that runs in the AWS Cloud. The company hosts the webhook logic on a set of Amazon EC2 instances in an Auto Scaling group that the company set as a target for an Application Load Balancer (ALB). The Git server calls the ALB for the configured webhooks. The company wants to move the solution to a serverless architecture.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. For each webhook, create and configure an AWS Lambda function URL. Update the Git servers to call the individual Lambda function URLs.\n",
    "\n",
    "B. Create an Amazon API Gateway HTTP API. Implement each webhook logic in a separate AWS Lambda function. Update the Git servers to call the API Gateway endpoint.\n",
    "\n",
    "C. Deploy the webhook logic to AWS App Runner. Create an ALB, and set App Runner as the target. Update the Git servers to call the ALB endpoint.\n",
    "\n",
    "D. Containerize the webhook logic. Create an Amazon Elastic Container Service (Amazon ECS) cluster, and run the webhook logic in AWS Fargate. Create an Amazon API Gateway REST API, and set Fargate as the target. Update the Git servers to call the API Gateway endpoint.\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- how to manage webhook to invoke functionality to serverless architecture\n",
    "    - webhook logic in lambda function, git server call api gateway http api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #46\n",
    "A company is planning to migrate 1,000 on-premises servers to AWS. The servers run on several VMware clusters in the company’s data center. As part of the migration plan, the company wants to gather server metrics such as CPU details, RAM usage, operating system information, and running processes. The company then wants to query and analyze the data.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Deploy and configure the AWS Agentless Discovery Connector virtual appliance on the on-premises hosts. Configure Data Exploration in AWS Migration Hub. Use AWS Glue to perform an ETL job against the data. Query the data by using Amazon S3 Select.\n",
    "\n",
    "B. Export only the VM performance information from the on-premises hosts. Directly import the required data into AWS Migration Hub. Update any missing information in Migration Hub. Query the data by using Amazon QuickSight.\n",
    "\n",
    "C. Create a script to automatically gather the server information from the on-premises hosts. Use the AWS CLI to run the put-resource-attributes command to store the detailed server data in AWS Migration Hub. Query the data directly in the Migration Hub console.\n",
    "\n",
    "D. Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the question is about gather operation information in on-premises server\n",
    "    - Deploy the AWS Application Discovery Agent to each on-premises server\n",
    "    - Configure Data Exploration in AWS Migration Hub\n",
    "    - Use Amazon Athena to run predefined queries against the data in Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #47\n",
    "A company is building a serverless application that runs on an AWS Lambda function that is attached to a VPC. The company needs to integrate the application with a new service from an external provider. The external provider supports only requests that come from public IPv4 addresses that are in an allow list.\n",
    "\n",
    "The company must provide a single public IP address to the external provider before the application can start using the new service.\n",
    "\n",
    "Which solution will give the application the ability to access the new service?\n",
    "\n",
    "A. Deploy a NAT gateway. Associate an Elastic IP address with the NAT gateway. Configure the VPC to use the NAT gateway.\n",
    "\n",
    "B. Deploy an egress-only internet gateway. Associate an Elastic IP address with the egress-only internet gateway. Configure the elastic network interface on the Lambda function to use the egress-only internet gateway.\n",
    "\n",
    "C. Deploy an internet gateway. Associate an Elastic IP address with the internet gateway. Configure the Lambda function to use the internet gateway.\n",
    "\n",
    "D. Deploy an internet gateway. Associate an Elastic IP address with the internet gateway. Configure the default route in the public VPC route table to use the internet gateway.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to config networking to consume data from external service which only allow whilelist public ip address\n",
    "    - Deploy a NAT gateway. Associate an Elastic IP address with the NAT gateway. Configure the VPC to use the NAT gateway\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #48\n",
    "A solutions architect has developed a web application that uses an Amazon API Gateway Regional endpoint and an AWS Lambda function. The consumers of the web application are all close to the AWS Region where the application will be deployed. The Lambda function only queries an Amazon Aurora MySQL database. The solutions architect has configured the database to have three read replicas.\n",
    "\n",
    "During testing, the application does not meet performance requirements. Under high load, the application opens a large number of database connections. The solutions architect must improve the application’s performance.\n",
    "\n",
    "Which actions should the solutions architect take to meet these requirements? (Choose two.)\n",
    "\n",
    "A. Use the cluster endpoint of the Aurora database.\n",
    "\n",
    "B. Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database.\n",
    "\n",
    "C. Use the Lambda Provisioned Concurrency feature.\n",
    "\n",
    "D. Move the code for opening the database connection in the Lambda function outside of the event handler.\n",
    "\n",
    "E. Change the API Gateway endpoint to an edge-optimized endpoint.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B, D\n",
    "- the question is about how to handle application which create many connection with the database\n",
    "    - Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database\n",
    "    - Move the code for opening the database connection in the Lambda function outside of the event handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #49\n",
    "A company is planning to host a web application on AWS and wants to load balance the traffic across a group of Amazon EC2 instances. One of the security requirements is to enable end-to-end encryption in transit between the client and the web server.\n",
    "\n",
    "Which solution will meet this requirement?\n",
    "\n",
    "A. Place the EC2 instances behind an Application Load Balancer (ALB). Provision an SSL certificate using AWS Certificate Manager (ACM), and associate the SSL certificate with the ALB. Export the SSL certificate and install it on each EC2 instance. Configure the ALB to listen on port 443 and to forward traffic to port 443 on the instances.\n",
    "\n",
    "B. Associate the EC2 instances with a target group. Provision an SSL certificate using AWS Certificate Manager (ACM). Create an Amazon CloudFront distribution and configure it to use the SSL certificate. Set CloudFront to use the target group as the origin server.\n",
    "\n",
    "C. Place the EC2 instances behind an Application Load Balancer (ALB) Provision an SSL certificate using AWS Certificate Manager (ACM), and associate the SSL certificate with the ALB. Provision a third-party SSL certificate and install it on each EC2 instance. Configure the ALB to listen on port 443 and to forward traffic to port 443 on the instances.\n",
    "\n",
    "D. Place the EC2 instances behind a Network Load Balancer (NLB). Provision a third-party SSL certificate and install it on the NLB and on each EC2 instance. Configure the NLB to listen on port 443 and to forward traffic to port 443 on the instances.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "    - web application -> Application Load Balancer\n",
    "    - we need to config SSL in both ALB and instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #50\n",
    "A company wants to migrate its data analytics environment from on premises to AWS. The environment consists of two simple Node.js applications. One of the applications collects sensor data and loads it into a MySQL database. The other application aggregates the data into reports. When the aggregation jobs run, some of the load jobs fail to run correctly.\n",
    "\n",
    "The company must resolve the data loading issue. The company also needs the migration to occur without interruptions or changes for the company’s customers.\n",
    "\n",
    "What should a solutions architect do to meet these requirements?\n",
    "\n",
    "A. Set up an Amazon Aurora MySQL database as a replication target for the on-premises database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind a Network Load Balancer (NLB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, disable the replication job and restart the Aurora Replica as the primary instance. Point the collector DNS record to the NLB.\n",
    "\n",
    "B. Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Move the aggregation jobs to run against the Aurora MySQL database. Set up collection endpoints behind an Application Load Balancer (ALB) as Amazon EC2 instances in an Auto Scaling group. When the databases are synced, point the collector DNS record to the ALDisable the AWS DMS sync task after the cutover from on premises to AWS.\n",
    "\n",
    "C. Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind an Application Load Balancer (ALB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, point the collector DNS record to the ALB. Disable the AWS DMS sync task after the cutover from on premises to AWS.\n",
    "\n",
    "D. Set up an Amazon Aurora MySQL database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as an Amazon Kinesis data stream. Use Amazon Kinesis Data Firehose to replicate the data to the Aurora MySQL database. When the databases are synced, disable the replication job and restart the Aurora Replica as the primary instance. Point the collector DNS record to the Kinesis data stream.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to migrate mysql to managed version to handle analytics workload\n",
    "    - Set up an Amazon Aurora MySQL database\n",
    "    - Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora\n",
    "    - Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica\n",
    "    - Set up collection endpoints as AWS Lambda functions behind an Application Load Balancer (ALB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, point the collector DNS record to the ALB. Disable the AWS DMS sync task after the cutover from on premises to AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #51\n",
    "A health insurance company stores personally identifiable information (PII) in an Amazon S3 bucket. The company uses server-side encryption with S3 managed encryption keys (SSE-S3) to encrypt the objects. According to a new requirement, all current and future objects in the S3 bucket must be encrypted by keys that the company’s security team manages. The S3 bucket does not have versioning enabled.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. In the S3 bucket properties, change the default encryption to SSE-S3 with a customer managed key. Use the AWS CLI to re-upload all objects in the S3 bucket. Set an S3 bucket policy to deny unencrypted PutObject requests.\n",
    "\n",
    "B. In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket.\n",
    "\n",
    "C. In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to automatically encrypt objects on GetObject and PutObject requests.\n",
    "\n",
    "D. In the S3 bucket properties, change the default encryption to AES-256 with a customer managed key. Attach a policy to deny unencrypted PutObject requests to any entities that access the S3 bucket. Use the AWS CLI to re-upload all objects in the S3 bucket.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about server-side encryption and change from default key to customer managed key\n",
    "    - In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS)\n",
    "    - Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #52\n",
    "A company is running a web application in the AWS Cloud. The application consists of dynamic content that is created on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group that is configured as a target group for an Application Load Balancer (ALB).\n",
    "\n",
    "The company is using an Amazon CloudFront distribution to distribute the application globally. The CloudFront distribution uses the ALB as an origin. The company uses Amazon Route 53 for DNS and has created an A record of www.example.com for the CloudFront distribution.\n",
    "\n",
    "A solutions architect must configure the application so that itis highly available and fault tolerant.\n",
    "\n",
    "Which solution meets these requirements?\n",
    "\n",
    "A. Provision a full, secondary application deployment in a different AWS Region. Update the Route 53 A record to be a failover record. Add both of the CloudFront distributions as values. Create Route 53 health checks.\n",
    "\n",
    "B. Provision an ALB, an Auto Scaling group, and EC2 instances in a different AWS Region. Update the CloudFront distribution, and create a second origin for the new ALCreate an origin group for the two origins. Configure one origin as primary and one origin as secondary.\n",
    "\n",
    "C. Provision an Auto Scaling group and EC2 instances in a different AWS Region. Create a second target for the new Auto Scaling group in the ALB. Set up the failover routing algorithm on the ALB.\n",
    "\n",
    "D. Provision a full, secondary application deployment in a different AWS Region. Create a second CloudFront distribution, and add the new application setup as an origin. Create an AWS Global Accelerator accelerator. Add both of the CloudFront distributions as endpoints.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about design system with high available and fault tolerant\n",
    "    -  CloudFront is global resource -> no need to create new one\n",
    "    - we need ALB, scaling group and ec2 instance in second regions -> choose B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #53\n",
    "A company has an organization in AWS Organizations that has a large number of AWS accounts. One of the AWS accounts is designated as a transit account and has a transit gateway that is shared with all of the other AWS accounts. AWS Site-to-Site VPN connections are configured between all of the company’s global offices and the transit account. The company has AWS Config enabled on all of its accounts.\n",
    "\n",
    "The company’s networking team needs to centrally manage a list of internal IP address ranges that belong to the global offices. Developers will reference this list to gain access to their applications securely.\n",
    "\n",
    "Which solution meets these requirements with the LEAST amount of operational overhead?\n",
    "\n",
    "A. Create a JSON file that is hosted in Amazon S3 and that lists all of the internal IP address ranges. Configure an Amazon Simple Notification Service (Amazon SNS) topic in each of the accounts that can be invoked when the JSON file is updated. Subscribe an AWS Lambda function to the SNS topic to update all relevant security group rules with the updated IP address ranges.\n",
    "\n",
    "B. Create a new AWS Config managed rule that contains all of the internal IP address ranges. Use the rule to check the security groups in each of the accounts to ensure compliance with the list of IP address ranges. Configure the rule to automatically remediate any noncompliant security group that is detected.\n",
    "\n",
    "C. In the transit account, create a VPC prefix list with all of the internal IP address ranges. Use AWS Resource Access Manager to share the prefix list with all of the other accounts. Use the shared prefix list to configure security group rules in the other accounts.\n",
    "\n",
    "D. In the transit account, create a security group with all of the internal IP address ranges. Configure the security groups in the other accounts to reference the transit account’s security group by using a nested security group reference of “/sg-1a2b3c4d”.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about design solution which managed centralize internal ip range for large organization which have large number of account\n",
    "    - In the transit account, create a VPC prefix list with all of the internal IP address ranges\n",
    "    - Use AWS Resource Access Manager to share the prefix list with all of the other accounts. Use the shared prefix list to configure security group rules in the other accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #54\n",
    "A company runs a new application as a static website in Amazon S3. The company has deployed the application to a production AWS account and uses Amazon CloudFront to deliver the website. The website calls an Amazon API Gateway REST API. An AWS Lambda function backs each API method.\n",
    "\n",
    "The company wants to create a CSV report every 2 weeks to show each API Lambda function’s recommended configured memory, recommended cost, and the price difference between current configurations and the recommendations. The company will store the reports in an S3 bucket.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST development time?\n",
    "\n",
    "A. Create a Lambda function that extracts metrics data for each API Lambda function from Amazon CloudWatch Logs for the 2-week period. Collate the data into tabular format. Store the data as a .csv file in an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks.\n",
    "\n",
    "B. Opt in to AWS Compute Optimizer. Create a Lambda function that calls the ExportLambdaFunctionRecommendations operation. Export the .csv file to an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks.\n",
    "\n",
    "C. Opt in to AWS Compute Optimizer. Set up enhanced infrastructure metrics. Within the Compute Optimizer console, schedule a job to export the Lambda recommendations to a .csv file. Store the file in an S3 bucket every 2 weeks.\n",
    "\n",
    "D. Purchase the AWS Business Support plan for the production account. Opt in to AWS Compute Optimizer for AWS Trusted Advisor checks. In the Trusted Advisor console, schedule a job to export the cost optimization checks to a .csv file. Store the file in an S3 bucket every 2 weeks.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about optimize config and cost of using lambda function\n",
    "    - using aws compute optimizer + lambda function which call ExportLambdaFunctionRecommendations operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #55\n",
    "A company’s factory and automation applications are running in a single VPC. More than 20 applications run on a combination of Amazon EC2, Amazon Elastic Container Service (Amazon ECS), and Amazon RDS.\n",
    "\n",
    "The company has software engineers spread across three teams. One of the three teams owns each application, and each time is responsible for the cost and performance of all of its applications. Team resources have tags that represent their application and team. The teams use IAM access for daily activities.\n",
    "\n",
    "The company needs to determine which costs on the monthly AWS bill are attributable to each application or team. The company also must be able to create reports to compare costs from the last 12 months and to help forecast costs for the next 12 months. A solutions architect must recommend an AWS Billing and Cost Management solution that provides these cost reports.\n",
    "\n",
    "Which combination of actions will meet these requirements? (Choose three.)\n",
    "\n",
    "A. Activate the user-define cost allocation tags that represent the application and the team.\n",
    "\n",
    "B. Activate the AWS generated cost allocation tags that represent the application and the team.\n",
    "\n",
    "C. Create a cost category for each application in Billing and Cost Management.\n",
    "\n",
    "D. Activate IAM access to Billing and Cost Management.\n",
    "\n",
    "E. Create a cost budget.\n",
    "\n",
    "F. Enable Cost Explorer.\n",
    "Solution:\n",
    "- correct answer is A, C, F\n",
    "- the question is more about analyze and forcast cost\n",
    "    - Activate the user-define cost allocation tags that represent the application and the team\n",
    "    - Create a cost category for each application in Billing and Cost Management\n",
    "    - Enable Cost Explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #56\n",
    "An AWS customer has a web application that runs on premises. The web application fetches data from a third-party API that is behind a firewall. The third party accepts only one public CIDR block in each client’s allow list.\n",
    "\n",
    "The customer wants to migrate their web application to the AWS Cloud. The application will be hosted on a set of Amazon EC2 instances behind an Application Load Balancer (ALB) in a VPC. The ALB is located in public subnets. The EC2 instances are located in private subnets. NAT gateways provide internet access to the private subnets.\n",
    "\n",
    "How should a solutions architect ensure that the web application can continue to call the third-party API after the migration?\n",
    "\n",
    "A. Associate a block of customer-owned public IP addresses to the VPC. Enable public IP addressing for public subnets in the VPC.\n",
    "\n",
    "B. Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC.\n",
    "\n",
    "C. Create Elastic IP addresses from the block of customer-owned IP addresses. Assign the static Elastic IP addresses to the ALB.\n",
    "\n",
    "D. Register a block of customer-owned public IP addresses in the AWS account. Set up AWS Global Accelerator to use Elastic IP addresses from the address block. Set the ALB as the accelerator endpoint.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to make sure web application can get api response from third party which only accept public ip address\n",
    "    - Register a block of customer-owned public IP addresses in the AWS account -> create public ip address block\n",
    "    - Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC -> create ip address and assigned to nat gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #57\n",
    "\n",
    "A company with several AWS accounts is using AWS Organizations and service control policies (SCPs). An administrator created the following SCP and has attached it to an organizational unit (OU) that contains AWS account 1111-1111-1111:\n",
    "\n",
    "\n",
    "\n",
    "Developers working in account 1111-1111-1111 complain that they cannot create Amazon S3 buckets. How should the administrator address this problem?\n",
    "\n",
    "A. Add s3:CreateBucket with “Allow” effect to the SCP.\n",
    "\n",
    "B. Remove the account from the OU, and attach the SCP directly to account 1111-1111-1111.\n",
    "\n",
    "C. Instruct the developers to add Amazon S3 permissions to their IAM entities.\n",
    "\n",
    "D. Remove the SCP from account 1111-1111-1111.\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about the different between permission, role and SCP. In this case, it is a permission problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #58\n",
    "A company has a monolithic application that is critical to the company’s business. The company hosts the application on an Amazon EC2 instance that runs Amazon Linux 2. The company’s application team receives a directive from the legal department to back up the data from the instance’s encrypted Amazon Elastic Block Store (Amazon EBS) volume to an Amazon S3 bucket. The application team does not have the administrative SSH key pair for the instance. The application must continue to serve the users.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Attach a role to the instance with permission to write to Amazon S3. Use the AWS Systems Manager Session Manager option to gain access to the instance and run commands to copy data into Amazon S3.\n",
    "\n",
    "B. Create an image of the instance with the reboot option turned on. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3.\n",
    "\n",
    "C. Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM). Copy the data to Amazon S3.\n",
    "\n",
    "D. Create an image of the instance. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to copy data from running ec2 instance\n",
    "    - update role of the instance which allow to write to s3 bucket\n",
    "    - Use the AWS Systems Manager Session Manager option to gain access to the instance and run commands to copy data into Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #59\n",
    "A solutions architect needs to copy data from an Amazon S3 bucket m an AWS account to a new S3 bucket in a new AWS account. The solutions architect must implement a solution that uses the AWS CLI.\n",
    "\n",
    "Which combination of steps will successfully copy the data? (Choose three.)\n",
    "\n",
    "A. Create a bucket policy to allow the source bucket to list its contents and to put objects and set object ACLs in the destination bucket. Attach the bucket policy to the destination bucket.\n",
    "\n",
    "B. Create a bucket policy to allow a user in the destination account to list the source bucket’s contents and read the source bucket’s objects. Attach the bucket policy to the source bucket.\n",
    "\n",
    "C. Create an IAM policy in the source account. Configure the policy to allow a user in the source account to list contents and get objects in the source bucket, and to list contents, put objects, and set object ACLs in the destination bucket. Attach the policy to the user.\n",
    "\n",
    "D. Create an IAM policy in the destination account. Configure the policy to allow a user in the destination account to list contents and get objects in the source bucket, and to list contents, put objects, and set objectACLs in the destination bucket. Attach the policy to the user.\n",
    "\n",
    "E. Run the aws s3 sync command as a user in the source account. Specify the source and destination buckets to copy the data.\n",
    "\n",
    "F. Run the aws s3 sync command as a user in the destination account. Specify the source and destination buckets to copy the data.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B, D, F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #60\n",
    "A company built an application based on AWS Lambda deployed in an AWS CloudFormation stack. The last production release of the web application introduced an issue that resulted in an outage lasting several minutes. A solutions architect must adjust the deployment process to support a canary release.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Create an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-config parameter to distribute the load.\n",
    "\n",
    "B. Deploy the application into a new CloudFormation stack. Use an Amazon Route 53 weighted routing policy to distribute the load.\n",
    "\n",
    "C. Create a version for every new deployed Lambda function. Use the AWS CLI update-function-configuration command with the routing-config parameter to distribute the load.\n",
    "\n",
    "D. Configure AWS CodeDeploy and use CodeDeployDefault.OneAtATime in the Deployment configuration to distribute the load.\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to config canary release for lambda function\n",
    "    - Create an alias for every new deployed version of the Lambda function -> we know different version of lambda function\n",
    "    - Use the AWS CLI update-alias command with the routing-config parameter to distribute the load -> in CloudFormation stack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #61\n",
    "A finance company hosts a data lake in Amazon S3. The company receives financial data records over SFTP each night from several third parties. The company runs its own SFTP server on an Amazon EC2 instance in a public subnet of a VPC. After the files are uploaded, they are moved to the data lake by a cron job that runs on the same instance. The SFTP server is reachable on DNS sftp.example.com through the use of Amazon Route 53.\n",
    "\n",
    "What should a solutions architect do to improve the reliability and scalability of the SFTP solution?\n",
    "\n",
    "A. Move the EC2 instance into an Auto Scaling group. Place the EC2 instance behind an Application Load Balancer (ALB). Update the DNS record sftp.example.com in Route 53 to point to the ALB.\n",
    "\n",
    "B. Migrate the SFTP server to AWS Transfer for SFTP. Update the DNS record sftp.example.com in Route 53 to point to the server endpoint hostname.\n",
    "\n",
    "C. Migrate the SFTP server to a file gateway in AWS Storage Gateway. Update the DNS record sftp.example.com in Route 53 to point to the file gateway endpoint.\n",
    "\n",
    "D. Place the EC2 instance behind a Network Load Balancer (NLB). Update the DNS record sftp.example.com in Route 53 to point to the NLB.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about migrate of ec2 with SFTP (Secure File Transfer Protocal) to AWS Transfer (a service) for SFTP\n",
    "    - AWS Transfer is a serice which user do not need to worry about scaling problem. AWS will handle it as part of the service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #62\n",
    "\n",
    "A company wants to migrate an application to Amazon EC2 from VMware Infrastructure that runs in an on-premises data center. A solutions architect must preserve the software and configuration settings during the migration.\n",
    "\n",
    "What should the solutions architect do to meet these requirements?\n",
    "\n",
    "A. Configure the AWS DataSync agent to start replicating the data store to Amazon FSx for Windows File Server. Use the SMB share to host the VMware data store. Use VM Import/Export to move the VMs to Amazon EC2.\n",
    "\n",
    "B. Use the VMware vSphere client to export the application as an image in Open Virtualization Format (OVF) format. Create an Amazon S3 bucket to store the image in the destination AWS Region. Create and apply an IAM role for VM Import. Use the AWS CLI to run the EC2 import command.\n",
    "\n",
    "C. Configure AWS Storage Gateway for files service to export a Common Internet File System (CIFS) share. Create a backup copy to the shared folder. Sign in to the AWS Management Console and create an AMI from the backup copy. Launch an EC2 instance that is based on the AMI.\n",
    "\n",
    "D. Create a managed-instance activation for a hybrid environment in AWS Systems Manager. Download and install Systems Manager Agent on the on-premises VM. Register the VM with Systems Manager to be a managed instance. Use AWS Backup to create a snapshot of the VM and create an AMI. Launch an EC2 instance that is based on the AMI.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to migrate application which run on VMare to EC2\n",
    "    - we just need to export application image from VMware and then deploy it to EC2 instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #63\n",
    "A video processing company has an application that downloads images from an Amazon S3 bucket, processes the images, stores a transformed image in a second S3 bucket, and updates metadata about the image in an Amazon DynamoDB table. The application is written in Node.js and runs by using an AWS Lambda function. The Lambda function is invoked when a new image is uploaded to Amazon S3.\n",
    "\n",
    "The application ran without incident for a while. However, the size of the images has grown significantly. The Lambda function is now failing frequently with timeout errors. The function timeout is set to its maximum value. A solutions architect needs to refactor the application’s architecture to prevent invocation failures. The company does not want to manage the underlying infrastructure.\n",
    "\n",
    "Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)\n",
    "\n",
    "A. Modify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon Elastic Container Registry (Amazon ECR).\n",
    "\n",
    "B. Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargate. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.\n",
    "\n",
    "C. Create an AWS Step Functions state machine with a Parallel state to invoke the Lambda function. Increase the provisioned concurrency of the Lambda function.\n",
    "\n",
    "D. Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of Amazon EC2. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.\n",
    "\n",
    "E. Modify the application to store images on Amazon Elastic File System (Amazon EFS) and to store metadata on an Amazon RDS DB instance. Adjust the Lambda function to mount the EFS file share.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, B\n",
    "- the question is about how to run dockerize application with aws service (do not manage underlying infrastructure)\n",
    "    - combine between lambda function and ECS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #64\n",
    "A company has an organization in AWS Organizations. The company is using AWS Control Tower to deploy a landing zone for the organization. The company wants to implement governance and policy enforcement. The company must implement a policy that will detect Amazon RDS DB instances that are not encrypted at rest in the company’s production OU.\n",
    "\n",
    "Which solution will meet this requirement?\n",
    "\n",
    "A. Turn on mandatory guardrails in AWS Control Tower. Apply the mandatory guardrails to the production OU.\n",
    "\n",
    "B. Enable the appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower. Apply the guardrail to the production OU.\n",
    "\n",
    "C. Use AWS Config to create a new mandatory guardrail. Apply the rule to all accounts in the production OU.\n",
    "\n",
    "D. Create a custom SCP in AWS Control Tower. Apply the SCP to the production OU.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to implement a policy that will detect Amazon RDS DB instances that are not encrypted at rest in the company’s production OU\n",
    "    - Enable the appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower. Apply the guardrail to the production OU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #65\n",
    "A startup company hosts a fleet of Amazon EC2 instances in private subnets using the latest Amazon Linux 2 AMI. The company’s engineers rely heavily on SSH access to the instances for troubleshooting.\n",
    "\n",
    "The company’s existing architecture includes the following:\n",
    "\n",
    "- A VPC with private and public subnets, and a NAT gateway.\n",
    "- Site-to-Site VPN for connectivity with the on-premises environment.\n",
    "- EC2 security groups with direct SSH access from the on-premises environment.\n",
    "\n",
    "The company needs to increase security controls around SSH access and provide auditing of commands run by the engineers.\n",
    "\n",
    "Which strategy should a solutions architect use?\n",
    "\n",
    "A. Install and configure EC2 Instance Connect on the fleet of EC2 instances. Remove all security group rules attached to EC2 instances that allow inbound TCP on port 22. Advise the engineers to remotely access the instances by using the EC2 Instance Connect CLI.\n",
    "\n",
    "B. Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer’s devices. Install the Amazon CloudWatch agent on all EC2 instances and send operating system audit logs to CloudWatch Logs.\n",
    "\n",
    "C. Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer’s devices. Enable AWS Config for EC2 security group resource changes. Enable AWS Firewall Manager and apply a security group policy that automatically remediates changes to rules.\n",
    "\n",
    "D. Create an IAM role with the AmazonSSMManagedInstanceCore managed policy attached. Attach the IAM role to all the EC2 instances. Remove all security group rules attached to the EC2 instances that allow inbound TCP on port 22. Have the engineers install the AWS Systems Manager Session Manager plugin for their devices and remotely access the instances by using the start-session API call from Systems Manager.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the question is about ncrease security controls around SSH access and provide auditing of commands run by the engineers\n",
    "    - Create an IAM role with the AmazonSSMManagedInstanceCore managed policy attached. Attach the IAM role to all the EC2 instances\n",
    "    - Remove all security group rules attached to the EC2 instances that allow inbound TCP on port 22\n",
    "    - Have the engineers install the AWS Systems Manager Session Manager plugin for their devices and remotely access the instances by using the start-session API call from Systems Manager\n",
    "    - what is security group\n",
    "        - security group is a set of traffic inbound and outbound rules to control traffic for the resource. it is like a virtual firewall of resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #66\n",
    "A company that uses AWS Organizations allows developers to experiment on AWS. As part of the landing zone that the company has deployed, developers use their company email address to request an account. The company wants to ensure that developers are not launching costly services or running services unnecessarily. The company must give developers a fixed monthly budget to limit their AWS costs.\n",
    "\n",
    "Which combination of steps will meet these requirements? (Choose three.)\n",
    "\n",
    "A. Create an SCP to set a fixed monthly account usage limit. Apply the SCP to the developer accounts.\n",
    "\n",
    "B. Use AWS Budgets to create a fixed monthly budget for each developer’s account as part of the account creation process.\n",
    "\n",
    "C. Create an SCP to deny access to costly services and components. Apply the SCP to the developer accounts.\n",
    "\n",
    "D. Create an IAM policy to deny access to costly services and components. Apply the IAM policy to the developer accounts.\n",
    "\n",
    "E. Create an AWS Budgets alert action to terminate services when the budgeted amount is reached. Configure the action to terminate all services.\n",
    "\n",
    "F. Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount is reached. Invoke an AWS Lambda function to terminate all services.\n",
    "Solution:\n",
    "- correct answer is B, C, F\n",
    "- the question is about how to control cost\n",
    "    - Use AWS Budgets to create a fixed monthly budget for each developer’s account as part of the account creation process\n",
    "    - Create an SCP to deny access to costly services and components. Apply the SCP to the developer accounts\n",
    "    - Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount is reached. Invoke an AWS Lambda function to terminate all services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #67\n",
    "A company has applications in an AWS account that is named Source. The account is in an organization in AWS Organizations. One of the applications uses AWS Lambda functions and stores inventory data in an Amazon Aurora database. The application deploys the Lambda functions by using a deployment package. The company has configured automated backups for Aurora.\n",
    "\n",
    "The company wants to migrate the Lambda functions and the Aurora database to a new AWS account that is named Target. The application processes critical data, so the company must minimize downtime.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account. Share the automated Aurora DB cluster snapshot with the Target account.\n",
    "\n",
    "B. Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account. Share the Aurora DB cluster with the Target account by using AWS Resource Access Manager {AWS RAM). Grant the Target account permission to clone the Aurora DB cluster.\n",
    "\n",
    "C. Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions and the Aurora DB cluster with the Target account. Grant the Target account permission to clone the Aurora DB cluster.\n",
    "\n",
    "D. Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions with the Target account. Share the automated Aurora DB cluster snapshot with the Target account.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how migrate application (lambda function) to different account\n",
    "    - an account is a container whiched used to managed resources in aws\n",
    "    - Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account\n",
    "    - Share the Aurora DB cluster with the Target account by using AWS Resource Access Manager {AWS RAM). Grant the Target account permission to clone the Aurora DB cluster. -> we do need to clone Aurora datbase to reduce downtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #68\n",
    "A company runs a Python script on an Amazon EC2 instance to process data. The script runs every 10 minutes. The script ingests files from an Amazon S3 bucket and processes the files. On average, the script takes approximately 5 minutes to process each file The script will not reprocess a file that the script has already processed.\n",
    "\n",
    "The company reviewed Amazon CloudWatch metrics and noticed that the EC2 instance is idle for approximately 40% of the time because of the file processing speed. The company wants to make the workload highly available and scalable. The company also wants to reduce long-term management overhead.\n",
    "\n",
    "Which solution will meet these requirements MOST cost-effectively?\n",
    "\n",
    "A. Migrate the data processing script to an AWS Lambda function. Use an S3 event notification to invoke the Lambda function to process the objects when the company uploads the objects.\n",
    "\n",
    "B. Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure Amazon S3 to send event notifications to the SQS queue. Create an EC2 Auto Scaling group with a minimum size of one instance. Update the data processing script to poll the SQS queue. Process the S3 objects that the SQS message identifies.\n",
    "\n",
    "C. Migrate the data processing script to a container image. Run the data processing container on an EC2 instance. Configure the container to poll the S3 bucket for new objects and to process the resulting objects.\n",
    "\n",
    "D. Migrate the data processing script to a container image that runs on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Create an AWS Lambda function that calls the Fargate RunTaskAPI operation when the container processes the file. Use an S3 event notification to invoke the Lambda function.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how optimize data analytice task because ec2 instance is not fully utilized\n",
    "    - move from ec2 to lambda function for event driven for short task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #69\n",
    "A financial services company in North America plans to release a new online web application to its customers on AWS. The company will launch the application in the us-east-1 Region on Amazon EC2 instances. The application must be highly available and must dynamically scale to meet user traffic. The company also wants to implement a disaster recovery environment for the application in the us-west-1 Region by using active-passive failover.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Create a VPC in us-east-1 and a VPC in us-west-1. Configure VPC peering. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in both VPCs. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in both VPCs. Place the Auto Scaling group behind the ALB.\n",
    "\n",
    "B. Create a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in that VPC. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPC. Place the Auto Scaling group behind the ALSet up the same configuration in the us-west-1 VPC. Create an Amazon Route 53 hosted zone. Create separate records for each ALEnable health checks to ensure high availability between Regions.\n",
    "\n",
    "C. Create a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in that VPCreate an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPPlace the Auto Scaling group behind the ALB. Set up the same configuration in the us-west-1 VPCreate an Amazon Route 53 hosted zone. Create separate records for each ALB. Enable health checks and configure a failover routing policy for each record.\n",
    "\n",
    "D. Create a VPC in us-east-1 and a VPC in us-west-1. Configure VPC peering. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in both VPCs. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in both VPCs. Place the Auto Scaling group behind the ALB. Create an Amazon Route 53 hosted zone. Create a record for the ALB.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about design disaster recovery environment in two different regions\n",
    "    - Create a VPC in us-east-1 and a VPC in us-west-1\n",
    "    - In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in that VPCreate an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPPlace the Auto Scaling group behind the ALB\n",
    "    - Set up the same configuration in the us-west-1 VPC\n",
    "    - Create an Amazon Route 53 hosted zone\n",
    "    - Create separate records for each ALB. Enable health checks and configure a failover routing policy for each record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #70\n",
    "A company has an environment that has a single AWS account. A solutions architect is reviewing the environment to recommend what the company could improve specifically in terms of access to the AWS Management Console. The company’s IT support workers currently access the console for administrative tasks, authenticating with named IAM users that have been mapped to their job role.\n",
    "\n",
    "The IT support workers no longer want to maintain both their Active Directory and IAM user accounts. They want to be able to access the console by using their existing Active Directory credentials. The solutions architect is using AWS IAM Identity Center (AWS Single Sign-On) to implement this functionality.\n",
    "\n",
    "Which solution will meet these requirements MOST cost-effectively?\n",
    "\n",
    "A. Create an organization in AWS Organizations. Turn on the IAM Identity Center feature in Organizations. Create and configure a directory in AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) with a two-way trust to the company’s on-premises Active Directory. Configure IAM Identity Center and set the AWS Managed Microsoft AD directory as the identity source. Create permission sets and map them to the existing groups within the AWS Managed Microsoft AD directory.\n",
    "\n",
    "B. Create an organization in AWS Organizations. Turn on the IAM Identity Center feature in Organizations. Create and configure an AD Connector to connect to the company’s on-premises Active Directory. Configure IAM Identity Center and select the AD Connector as the identity source. Create permission sets and map them to the existing groups within the company’s Active Directory.\n",
    "\n",
    "C. Create an organization in AWS Organizations. Turn on all features for the organization. Create and configure a directory in AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) with a two-way trust to the company’s on-premises Active Directory. Configure IAM Identity Center and select the AWS Managed Microsoft AD directory as the identity source. Create permission sets and map them to the existing groups within the AWS Managed Microsoft AD directory.\n",
    "\n",
    "D. Create an organization in AWS Organizations. Turn on all features for the organization. Create and configure an AD Connector to connect to the company’s on-premises Active Directory. Configure IAM Identity Center and set the AD Connector as the identity source. Create permission sets and map them to the existing groups within the company’s Active Directory.\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the question is about how to connect current Active Directory with aws IAM Identity Center\n",
    "    - Create an organization in AWS Organizations. Turn on all features for the organization\n",
    "    - Create and configure an AD Connector to connect to the company’s on-premises Active Directory\n",
    "    - Configure IAM Identity Center and set the AD Connector as the identity source\n",
    "    - Create permission sets and map them to the existing groups within the company’s Active Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #71\n",
    "A video streaming company recently launched a mobile app for video sharing. The app uploads various files to an Amazon S3 bucket in the us-east-1 Region. The files range in size from 1 GB to 10 GB.\n",
    "\n",
    "Users who access the app from Australia have experienced uploads that take long periods of time. Sometimes the files fail to completely upload for these users. A solutions architect must improve the app’s performance for these uploads.\n",
    "\n",
    "Which solutions will meet these requirements? (Choose two.)\n",
    "\n",
    "A. Enable S3 Transfer Acceleration on the S3 bucket. Configure the app to use the Transfer Acceleration endpoint for uploads.\n",
    "\n",
    "B. Configure an S3 bucket in each Region to receive the uploads. Use S3 Cross-Region Replication to copy the files to the distribution S3 bucket.\n",
    "\n",
    "C. Set up Amazon Route 53 with latency-based routing to route the uploads to the nearest S3 bucket Region.\n",
    "\n",
    "D. Configure the app to break the video files into chunks. Use a multipart upload to transfer files to Amazon S3.\n",
    "\n",
    "E. Modify the app to add random prefixes to the files before uploading.\n",
    "\n",
    "Solution:\n",
    "- correct answer A, D\n",
    "- the question is about how to upload large file\n",
    "    - Transfer Accelerator + Multi-part uploads for files more 500MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #72\n",
    "An application is using an Amazon RDS for MySQL Multi-AZ DB instance in the us-east-1 Region. After a failover test, the application lost the connections to the database and could not re-establish the connections. After a restart of the application, the application re-established the connections.\n",
    "\n",
    "A solutions architect must implement a solution so that the application can re-establish connections to the database without requiring a restart.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Create an Amazon Aurora MySQL Serverless v1 DB instance. Migrate the RDS DB instance to the Aurora Serverless v1 DB instance. Update the connection settings in the application to point to the Aurora reader endpoint.\n",
    "\n",
    "B. Create an RDS proxy. Configure the existing RDS endpoint as a target. Update the connection settings in the application to point to the RDS proxy endpoint.\n",
    "\n",
    "C. Create a two-node Amazon Aurora MySQL DB cluster. Migrate the RDS DB instance to the Aurora DB cluster. Create an RDS proxy. Configure the existing RDS endpoint as a target. Update the connection settings in the application to point to the RDS proxy endpoint.\n",
    "\n",
    "D. Create an Amazon S3 bucket. Export the database to Amazon S3 by using AWS Database Migration Service (AWS DMS). Configure Amazon Athena to use the S3 bucket as a data store. Install the latest Open Database Connectivity (ODBC) driver for the application. Update the connection settings in the application to point to the Athena endpoint\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how for the application to re-connect with database without restart the app\n",
    "    - using RDS proxy will help with the lost connection and re-connect problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #73\n",
    "A company is building a solution in the AWS Cloud. Thousands or devices will connect to the solution and send data. Each device needs to be able to send and receive data in real time over the MQTT protocol. Each device must authenticate by using a unique X.509 certificate.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Set up AWS IoT Core. For each device, create a corresponding Amazon MQ queue and provision a certificate. Connect each device to Amazon MQ.\n",
    "\n",
    "B. Create a Network Load Balancer (NLB) and configure it with an AWS Lambda authorizer. Run an MQTT broker on Amazon EC2 instances in an Auto Scaling group. Set the Auto Scaling group as the target for the NLConnect each device to the NLB.\n",
    "\n",
    "C. Set up AWS IoT Core. For each device, create a corresponding AWS IoT thing and provision a certificate. Connect each device to AWS IoT Core.\n",
    "\n",
    "D. Set up an Amazon API Gateway HTTP API and a Network Load Balancer (NLB). Create integration between API Gateway and the NLB. Configure a mutual TLS certificate authorizer on the HTTP API. Run an MQTT broker on an Amazon EC2 instance that the NLB targets. Connect each device to the NLB.\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to connect many IOT devices to AWS IOT Core\n",
    "    - MQTT protocol (Message Queuing Telemetry Transport) is the protocal for IOT\n",
    "    - X.509 certificate is certificate for IOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #74\n",
    "A company is running several workloads in a single AWS account. A new company policy states that engineers can provision only approved resources and that engineers must use AWS CloudFormation to provision these resources. A solutions architect needs to create a solution to enforce the new restriction on the IAM role that the engineers use for access.\n",
    "\n",
    "What should the solutions architect do to create the solution?\n",
    "\n",
    "A. Upload AWS CloudFormation templates that contain approved resources to an Amazon S3 bucket. Update the IAM policy for the engineers’ IAM role to only allow access to Amazon S3 and AWS CloudFormation. Use AWS CloudFormation templates to provision resources.\n",
    "\n",
    "B. Update the IAM policy for the engineers’ IAM role with permissions to only allow provisioning of approved resources and AWS CloudFormation. Use AWS CloudFormation templates to create stacks with approved resources.\n",
    "\n",
    "C. Update the IAM policy for the engineers’ IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation.\n",
    "\n",
    "D. Provision resources in AWS CloudFormation stacks. Update the IAM policy for the engineers’ IAM role to only allow access to their own AWS CloudFormation stack.\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to enforce engineer must request resource through CloudForm\n",
    "    - Update the IAM policy for the engineers’ IAM role with permissions to only allow AWS CloudFormation actions -> engineer must use CloufFormation\n",
    "    - Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation -> CloudFormation can create approved resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #75\n",
    "A solutions architect is designing the data storage and retrieval architecture for a new application that a company will be launching soon. The application is designed to ingest millions of small records per minute from devices all around the world. Each record is less than 4 KB in size and needs to be stored in a durable location where it can be retrieved with low latency. The data is ephemeral and the company is required to store the data for 120 days only, after which the data can be deleted.\n",
    "\n",
    "The solutions architect calculates that, during the course of a year, the storage requirements would be about 10-15 TB.\n",
    "\n",
    "Which storage strategy is the MOST cost-effective and meets the design requirements?\n",
    "\n",
    "A. Design the application to store each incoming record as a single .csv file in an Amazon S3 bucket to allow for indexed retrieval. Configure a lifecycle policy to delete data older than 120 days.\n",
    "\n",
    "B. Design the application to store each incoming record in an Amazon DynamoDB table properly configured for the scale. Configure the DynamoDB Time to Live (TTL) feature to delete records older than 120 days.\n",
    "\n",
    "C. Design the application to store each incoming record in a single table in an Amazon RDS MySQL database. Run a nightly cron job that runs a query to delete any records older than 120 days.\n",
    "\n",
    "D. Design the application to batch incoming records before writing them to an Amazon S3 bucket. Update the metadata for the object to contain the list of records in the batch and use the Amazon S3 metadata search feature to retrieve the data. Configure a lifecycle policy to delete the data after 120 days.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about design storage solution which is low-latency, store large data but also cost-effective\n",
    "    - the best option is DynamoDb because it can scale better and cheaper than RDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #76\n",
    "A retail company is hosting an ecommerce website on AWS across multiple AWS Regions. The company wants the website to be operational at all times for online purchases. The website stores data in an Amazon RDS for MySQL DB instance.\n",
    "\n",
    "Which solution will provide the HIGHEST availability for the database?\n",
    "\n",
    "A. Configure automated backups on Amazon RDS. In the case of disruption, promote an automated backup to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source.\n",
    "\n",
    "B. Configure global tables and read replicas on Amazon RDS. Activate the cross-Region scope. In the case of disruption, use AWS Lambda to copy the read replicas from one Region to another Region.\n",
    "\n",
    "C. Configure global tables and automated backups on Amazon RDS. In the case of disruption, use AWS Lambda to copy the read replicas from one Region to another Region.\n",
    "\n",
    "D. Configure read replicas on Amazon RDS. In the case of disruption, promote a cross-Region and read replica to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the question is about design a solution for RDS which is highest availability\n",
    "    - Configure read replicas on Amazon RDS\n",
    "    - In the case of disruption, promote a cross-Region and read replica to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #77\n",
    "Example Corp. has an on-premises data center and a VPC named VPC A in the Example Corp. AWS account. The on-premises network connects to VPC A through an AWS Site-To-Site VPN. The on-premises servers can properly access VPC A. Example Corp. just acquired AnyCompany, which has a VPC named VPC B. There is no IP address overlap among these networks. Example Corp. has peered VPC A and VPC B.\n",
    "\n",
    "Example Corp. wants to connect from its on-premise servers to VPC B. Example Corp. has properly set up the network ACL and security groups.\n",
    "\n",
    "Which solution will meet this requirement with the LEAST operational effort?\n",
    "\n",
    "A. Create a transit gateway. Attach the Site-to-Site VPN, VPC A, and VPC B to the transit gateway. Update the transit gateway route tables for all networks to add IP range routes for all other networks.\n",
    "\n",
    "B. Create a transit gateway. Create a Site-to-Site VPN connection between the on-premises network and VPC B, and connect the VPN connection to the transit gateway. Add a route to direct traffic to the peered VPCs, and add an authorization rule to give clients access to the VPCs A and B.\n",
    "\n",
    "C. Update the route tables for the Site-to-Site VPN and both VPCs for all three networks. Configure BGP propagation for all three networks. Wait for up to 5 minutes for BGP propagation to finish.\n",
    "\n",
    "D. Modify the Site-to-Site VPN’s virtual private gateway definition to include VPC A and VPC B. Split the two routers of the virtual private getaway between the two VPCs.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to connect multiple VPC\n",
    "    - using transit gateway\n",
    "    - Create a transit gateway\n",
    "    - Attach the Site-to-Site VPN, VPC A, and VPC B to the transit gateway\n",
    "    - Update the transit gateway route tables for all networks to add IP range routes for all other networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #78\n",
    "A company recently completed the migration from an on-premises data center to the AWS Cloud by using a replatforming strategy. One of the migrated servers is running a legacy Simple Mail Transfer Protocol (SMTP) service that a critical application relies upon. The application sends outbound email messages to the company’s customers. The legacy SMTP server does not support TLS encryption and uses TCP port 25. The application can use SMTP only.\n",
    "\n",
    "The company decides to use Amazon Simple Email Service (Amazon SES) and to decommission the legacy SMTP server. The company has created and validated the SES domain. The company has lifted the SES limits.\n",
    "\n",
    "What should the company do to modify the application to send email messages from Amazon SES?\n",
    "\n",
    "A. Configure the application to connect to Amazon SES by using TLS Wrapper. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permissions. Attach the IAM role to an Amazon EC2 instance.\n",
    "\n",
    "B. Configure the application to connect to Amazon SES by using STARTTLS. Obtain Amazon SES SMTP credentials. Use the credentials to authenticate with Amazon SES.\n",
    "\n",
    "C. Configure the application to use the SES API to send email messages. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permissions. Use the IAM role as a service role for Amazon SES.\n",
    "\n",
    "D. Configure the application to use AWS SDKs to send email messages. Create an IAM user for Amazon SES. Generate API access keys. Use the access keys to authenticate with Amazon SES.\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to replicae legay SMTP (Simple Mail Transfer Protocal) with SES (Simple Email Service)\n",
    "    - Configure the application to connect to Amazon SES by using STARTTLS. Obtain Amazon SES SMTP credentials. Use the credentials to authenticate with Amazon SES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #79\n",
    "A company recently acquired several other companies. Each company has a separate AWS account with a different billing and reporting method. The acquiring company has consolidated all the accounts into one organization in AWS Organizations. However, the acquiring company has found it difficult to generate a cost report that contains meaningful groups for all the teams.\n",
    "\n",
    "The acquiring company’s finance team needs a solution to report on costs for all the companies through a self-managed application.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a table in Amazon Athena. Create an Amazon QuickSight dataset based on the Athena table. Share the dataset with the finance team.\n",
    "\n",
    "B. Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a specialized template in AWS Cost Explorer that the finance department will use to build reports.\n",
    "\n",
    "C. Create an Amazon QuickSight dataset that receives spending information from the AWS Price List Query API. Share the dataset with the finance team.\n",
    "\n",
    "D. Use the AWS Price List Query API to collect account spending information. Create a specialized template in AWS Cost Explorer that the finance department will use to build reports.\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to design cost reporting for large organization\n",
    "    - Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report\n",
    "    - Create a table in Amazon Athena\n",
    "    - Create an Amazon QuickSight dataset based on the Athena table. Share the dataset with the finance team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #80\n",
    "A company runs an IoT platform on AWS. IoT sensors in various locations send data to the company’s Node.js API servers on Amazon EC2 instances running behind an Application Load Balancer. The data is stored in an Amazon RDS MySQL DB instance that uses a 4 TB General Purpose SSD volume.\n",
    "\n",
    "The number of sensors the company has deployed in the field has increased over time, and is expected to grow significantly. The API servers are consistently overloaded and RDS metrics show high write latency.\n",
    "\n",
    "Which of the following steps together will resolve the issues permanently and enable growth as new sensors are provisioned, while keeping this platform cost-efficient? (Choose two.)\n",
    "\n",
    "A. Resize the MySQL General Purpose SSD storage to 6 TB to improve the volume’s IOPS.\n",
    "\n",
    "B. Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas.\n",
    "\n",
    "C. Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data.\n",
    "\n",
    "D. Use AWS X-Ray to analyze and debug application issues and add more API servers to match the load.\n",
    "\n",
    "E. Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance.\n",
    "Solution:\n",
    "- correct answer is C, E\n",
    "- the question is about design system for IOt device problem to send data to application to db\n",
    "    - change from application to message queue + lambda function\n",
    "    - change from mysql db to dynamodb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #81\n",
    "A company is building an electronic document management system in which users upload their documents. The application stack is entirely serverless and runs on AWS in the eu-central-1 Region. The system includes a web application that uses an Amazon CloudFront distribution for delivery with Amazon S3 as the origin. The web application communicates with Amazon API Gateway Regional endpoints. The API Gateway APIs call AWS Lambda functions that store metadata in an Amazon Aurora Serverless database and put the documents into an S3 bucket.\n",
    "The company is growing steadily and has completed a proof of concept with its largest customer. The company must improve latency outside of Europe.\n",
    "\n",
    "Which combination of actions will meet these requirements? (Choose two.)\n",
    "\n",
    "A. Enable S3 Transfer Acceleration on the S3 bucket. Ensure that the web application uses the Transfer Acceleration signed URLs.\n",
    "\n",
    "B. Create an accelerator in AWS Global Accelerator. Attach the accelerator to the CloudFront distribution.\n",
    "\n",
    "C. Change the API Gateway Regional endpoints to edge-optimized endpoints.\n",
    "\n",
    "D. Provision the entire stack in two other locations that are spread across the world. Use global databases on the Aurora Serverless cluster.\n",
    "\n",
    "E. Add an Amazon RDS proxy between the Lambda functions and the Aurora Serverless database.\n",
    "Solution:\n",
    "- correct answer is A, C\n",
    "- the question is about how to design low-latency access for s3 in multiple regions\n",
    "    - Enable S3 Transfer Acceleration on the S3 bucket. Ensure that the web application uses the Transfer Acceleration signed URLs\n",
    "    - Change the API Gateway Regional endpoints to edge-optimized endpoints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #82\n",
    "An adventure company has launched a new feature on its mobile app. Users can use the feature to upload their hiking and rafting photos and videos anytime. The photos and videos are stored in Amazon S3 Standard storage in an S3 bucket and are served through Amazon CloudFront.\n",
    "\n",
    "The company needs to optimize the cost of the storage. A solutions architect discovers that most of the uploaded photos and videos are accessed infrequently after 30 days. However, some of the uploaded photos and videos are accessed frequently after 30 days. The solutions architect needs to implement a solution that maintains millisecond retrieval availability of the photos and videos at the lowest possible cost.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Configure S3 Intelligent-Tiering on the S3 bucket.\n",
    "\n",
    "B. Configure an S3 Lifecycle policy to transition image objects and video objects from S3 Standard to S3 Glacier Deep Archive after 30 days.\n",
    "\n",
    "C. Replace Amazon S3 with an Amazon Elastic File System (Amazon EFS) file system that is mounted on Amazon EC2 instances.\n",
    "\n",
    "D. Add a Cache-Control: max-age header to the S3 image objects and S3 video objects. Set the header to 30 days.\n",
    "Solution\n",
    "- correct answer is A\n",
    "- the question about how to design latency access to s3 given that access pattern\n",
    "    - Configure S3 Intelligent-Tiering on the S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #83\n",
    "A company uses Amazon S3 to store files and images in a variety of storage classes. The company's S3 costs have increased substantially during the past year.\n",
    "\n",
    "A solutions architect needs to review data trends for the past 12 months and identity the appropriate storage class for the objects.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Download AWS Cost and Usage Reports for the last 12 months of S3 usage. Review AWS Trusted Advisor recommendations for cost savings.\n",
    "\n",
    "B. Use S3 storage class analysis. Import data trends into an Amazon QuickSight dashboard to analyze storage trends.\n",
    "\n",
    "C. Use Amazon S3 Storage Lens. Upgrade the default dashboard to include advanced metrics for storage trends.\n",
    "\n",
    "D. Use Access Analyzer for S3. Download the Access Analyzer for S3 report for the last 12 months. Import the .csv file to an Amazon QuickSight dashboard.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to review data trend, analyze and choose correct class for for s3\n",
    "    - Use Amazon S3 Storage Lens. Upgrade the default dashboard to include advanced metrics for storage trends\n",
    "    - Storage Lens: All S3 Storage Lens metrics are retained for a period of 15 months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #84\n",
    "A company has its cloud infrastructure on AWS. A solutions architect needs to define the infrastructure as code. The infrastructure is currently deployed in one AWS Region. The company’s business expansion plan includes deployments in multiple Regions across multiple AWS accounts.\n",
    "\n",
    "What should the solutions architect do to meet these requirements?\n",
    "\n",
    "A. Use AWS CloudFormation templates. Add IAM policies to control the various accounts, Deploy the templates across the multiple Regions.\n",
    "\n",
    "B. Use AWS Organizations. Deploy AWS CloudFormation templates from the management account Use AWS Control Tower to manage deployments across accounts.\n",
    "\n",
    "C. Use AWS Organizations and AWS CloudFormation StackSets. Deploy a Cloud Formation template from an account that has the necessary IAM permissions.\n",
    "\n",
    "D. Use nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks.\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is infra as code in multiple account and regions\n",
    "    - Use AWS Organizations and AWS CloudFormation StackSets. Deploy a Cloud Formation template from an account that has the necessary IAM permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #85\n",
    "A company has its cloud infrastructure on AWS. A solutions architect needs to define the infrastructure as code. The infrastructure is currently deployed in one AWS Region. The company’s business expansion plan includes deployments in multiple Regions across multiple AWS accounts.\n",
    "\n",
    "What should the solutions architect do to meet these requirements?\n",
    "\n",
    "A. Use AWS CloudFormation templates. Add IAM policies to control the various accounts, Deploy the templates across the multiple Regions.\n",
    "\n",
    "B. Use AWS Organizations. Deploy AWS CloudFormation templates from the management account Use AWS Control Tower to manage deployments across accounts.\n",
    "\n",
    "C. Use AWS Organizations and AWS CloudFormation StackSets. Deploy a Cloud Formation template from an account that has the necessary IAM permissions.\n",
    "\n",
    "D. Use nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #86\n",
    "A company plans to refactor a monolithic application into a modern application design deployed on AWS. The CI/CD pipeline needs to be upgraded to support the modern design for the application with the following requirements:\n",
    "\n",
    "• It should allow changes to be released several times every hour.\n",
    "• It should be able to roll back the changes as quickly as possible.\n",
    "\n",
    "Which design will meet these requirements?\n",
    "\n",
    "A. Deploy a CI/CD pipeline that incorporates AMIs to contain the application and their configurations. Deploy the application by replacing Amazon EC2 instances.\n",
    "\n",
    "B. Specify AWS Elastic Beanstalk to stage in a secondary environment as the deployment target for the CI/CD pipeline of the application. To deploy, swap the staging and production environment URLs.\n",
    "\n",
    "C. Use AWS Systems Manager to re-provision the infrastructure for each deployment. Update the Amazon EC2 user data to pull the latest code artifact from Amazon S3 and use Amazon Route 53 weighted routing to point to the new environment.\n",
    "\n",
    "D. Roll out the application updates as part of an Auto Scaling event using prebuilt AMIs. Use new versions of the AMIs to add instances. and phase out all instances that use the previous AMI version with the configured termination policy during a deployment event.\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about design CI-CD pipeline\n",
    "    - Specify AWS Elastic Beanstalk to stage in a secondary environment as the deployment target for the CI/CD pipeline of the application. To deploy, swap the staging and production environment URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #87\n",
    "A company has an application that runs on Amazon EC2 instances. A solutions architect is designing VPC infrastructure in an AWS Region where the application needs to access an Amazon Aurora DB Cluster. The EC2 instances are all associated with the same security group. The DB cluster is associated with its own security group.\n",
    "\n",
    "The solutions architect needs to add rules to the security groups to provide the application with least privilege access to the DB Cluster.\n",
    "\n",
    "Which combination of steps will meet these requirements? (Choose two.)\n",
    "\n",
    "A. Add an inbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the source over the default Aurora port.\n",
    "\n",
    "B. Add an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora port.\n",
    "\n",
    "C. Add an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port.\n",
    "\n",
    "D. Add an outbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the destination over the default Aurora port.\n",
    "\n",
    "E. Add an outbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the destination over the ephemeral ports.\n",
    "Solution:\n",
    "- correct answer is B, C\n",
    "- the question is about config security group to communicate between ec2 instance and aurora db cluster\n",
    "    - Add an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora port -> for traffic from ec2 instance to db\n",
    "    - Add an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port -> for traffic from db to ec2\n",
    "    - Flow connection: EC2 -> DB. So you need to configure Outbound EC2 and Inbound DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #88\n",
    "A company wants to change its internal cloud billing strategy for each of its business units. Currently, the cloud governance team shares reports for overall cloud spending with the head of each business unit. The company uses AWS Organizations to manage the separate AWS accounts for each business unit. The existing tagging standard in Organizations includes the application, environment, and owner. The cloud governance team wants a centralized solution so each business unit receives monthly reports on its cloud spending. The solution should also send notifications for any cloud spending that exceeds a set threshold.\n",
    "\n",
    "Which solution is the MOST cost-effective way to meet these requirements?\n",
    "\n",
    "A. Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in each account to create monthly reports for each business unit.\n",
    "\n",
    "B. Configure AWS Budgets in the organization's management account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in the organization's management account to create monthly reports for each business unit.\n",
    "\n",
    "C. Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use the AWS Billing and Cost Management dashboard in each account to create monthly reports for each business unit.\n",
    "\n",
    "D. Enable AWS Cost and Usage Reports in the organization's management account and configure reports grouped by application, environment. and owner. Create an AWS Lambda function that processes AWS Cost and Usage Reports, sends budget alerts, and sends monthly reports to each business unit's email list.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about buget alert in organization which have multiple business unit and account\n",
    "    - Configure AWS Budgets in the organization's management account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert\n",
    "    - Use Cost Explorer in the organization's management account to create monthly reports for each business unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #89\n",
    "A company is using AWS CloudFormation to deploy its infrastructure. The company is concerned that, if a production CloudFormation stack is deleted, important data stored in Amazon RDS databases or Amazon EBS volumes might also be deleted.\n",
    "\n",
    "How can the company prevent users from accidentally deleting data in this way?\n",
    "\n",
    "A. Modify the CloudFormation templates to add a DeletionPolicy attribute to RDS and EBS resources.\n",
    "\n",
    "B. Configure a stack policy that disallows the deletion of RDS and EBS resources.\n",
    "\n",
    "C. Modify IAM policies lo deny deleting RDS and EBS resources that are tagged with an \"aws:cloudformation:stack-name\" tag.\n",
    "\n",
    "D. Use AWS Config rules to prevent deleting RDS and EBS resources.\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to protect data in database when using CloudFormation\n",
    "    - Modify the CloudFormation templates to add a DeletionPolicy attribute to RDS and EBS resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #90\n",
    "A company has VPC flow logs enabled for Its NAT gateway. The company is seeing Action = ACCEPT for inbound traffic that comes from public IP address 198.51.100.2 destined for a private Amazon EC2 instance.\n",
    "\n",
    "A solutions architect must determine whether the traffic represents unsolicited inbound connections from the internet. The first two octets of the VPC CIDR block are 203.0.\n",
    "\n",
    "Which set of steps should the solutions architect take to meet these requirements?\n",
    "\n",
    "A. Open the AWS CloudTrail console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interlace. Run a query to filter with the destination address set as \"like 203.0\" and the source address set as \"like 198.51.100.2\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.\n",
    "\n",
    "B. Open the Amazon CloudWatch console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interface. Run a query to filter with the destination address set as \"like 203.0\" and the source address set as \"like 198.51.100.2\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.\n",
    "\n",
    "C. Open the AWS CloudTrail console. Select the log group that contains the NAT gateway's elastic network interface and the private instance’s elastic network interface. Run a query to filter with the destination address set as \"like 198.51.100.2\" and the source address set as \"like 203.0\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.\n",
    "\n",
    "D. Open the Amazon CloudWatch console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interface. Run a query to filter with the destination address set as \"like 198.51.100.2\" and the source address set as \"like 203.0\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #91\n",
    "A company consists or two separate business units. Each business unit has its own AWS account within a single organization in AWS Organizations. The business units regularly share sensitive documents with each other. To facilitate sharing, the company created an Amazon S3 bucket in each account and configured low-way replication between the S3 buckets. The S3 buckets have millions of objects.\n",
    "\n",
    "Recently, a security audit identified that neither S3 bucket has encryption at rest enabled. Company policy requires that all documents must be stored with encryption at rest. The company wants to implement server-side encryption with Amazon S3 managed encryption keys (SSE-S3).\n",
    "\n",
    "What is the MOST operationally efficient solution that meets these requirements?\n",
    "\n",
    "A. Turn on SSE-S3 on both S3 buckets. Use S3 Batch Operations to copy and encrypt the objects in the same location.\n",
    "\n",
    "B. Create an AWS Key Management Service (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on each S3 bucket by using the corresponding KMS key in that AWS account. Encrypt the existing objects by using an S3 copy command in the AWS CLI.\n",
    "\n",
    "C. Turn on SSE-S3 on both S3 buckets. Encrypt the existing objects by using an S3 copy command in the AWS CLI.\n",
    "\n",
    "D. Create an AWS Key Management Service, (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on each S3 bucket by using the corresponding KMS key in that AWS account. Use S3 Batch Operations to copy the objects into the same location.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to encrypt while sync data between large s3 bucket with multiple account\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #92\n",
    "A company is running an application in the AWS Cloud. The application collects and stores a large amount of unstructured data in an Amazon S3 bucket. The S3 bucket contains several terabytes of data and uses the S3 Standard storage class. The data increases in size by several gigabytes every day.\n",
    "\n",
    "The company needs to query and analyze the data. The company does not access data that is more than 1 year old. However, the company must retain all the data indefinitely for compliance reasons.\n",
    "\n",
    "Which solution will meet these requirements MOST cost-effectively?\n",
    "\n",
    "A. Use S3 Select to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.\n",
    "\n",
    "B. Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old 10 S3 Glacier Deep Archive.\n",
    "\n",
    "C. Use an AWS Glue Data Catalog and Amazon Athena to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.\n",
    "\n",
    "D. Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Intelligent-Tiering.\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to design system for data analytic with unstructure data in s3\n",
    "    - Use an AWS Glue Data Catalog and Amazon Athena to query the data\n",
    "    - Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #93\n",
    "A video processing company wants to build a machine learning (ML) model by using 600 TB of compressed data that is stored as thousands of files in the company's on-premises network attached storage system. The company does not have the necessary compute resources on premises for ML experiments and wants to use AWS.\n",
    "\n",
    "The company needs to complete the data transfer to AWS within 3 weeks. The data transfer will be a one-time transfer. The data must be encrypted in transit. The measured upload speed of the company's internet connection is 100 Mbps. and multiple departments share the connection.\n",
    "\n",
    "Which solution will meet these requirements MOST cost-effectively?\n",
    "\n",
    "A. Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management Console. Configure the devices with a destination S3 bucket. Copy the data to the devices. Ship the devices back to AWS.\n",
    "\n",
    "B. Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3.\n",
    "\n",
    "C. Create a VPN connection between the on-premises network attached storage and the nearest AWS Region. Transfer the data over the VPN connection.\n",
    "\n",
    "D. Deploy an AWS Storage Gateway file gateway on premises. Configure the file gateway with a destination S3 bucket. Copy the data to the file gateway.\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to move large amout of data given if choose through internet is so slow\n",
    "    - Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management Console. Configure the devices with a destination S3 bucket. Copy the data to the devices. Ship the devices back to AWS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #94\n",
    "A company has migrated Its forms-processing application to AWS. When users interact with the application, they upload scanned forms as files through a web application. A database stores user metadata and references to files that are stored in Amazon S3. The web application runs on Amazon EC2 instances and an Amazon RDS for PostgreSQL database.\n",
    "\n",
    "When forms are uploaded, the application sends notifications to a team through Amazon Simple Notification Service (Amazon SNS). A team member then logs in and processes each form. The team member performs data validation on the form and extracts relevant data before entering the information into another system that uses an API.\n",
    "\n",
    "A solutions architect needs to automate the manual processing of the forms. The solution must provide accurate form extraction. minimize time to market, and minimize tong-term operational overhead.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Develop custom libraries to perform optical character recognition (OCR) on the forms. Deploy the libraries to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster as an application tier. Use this tier to process the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data into an Amazon DynamoDB table. Submit the data to the target system's APL. Host the new application tier on EC2 instances.\n",
    "\n",
    "B. Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use artificial intelligence and machine learning (AI/ML) models that are trained and hosted on an EC2 instance to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.\n",
    "\n",
    "C. Host a new application tier on EC2 instances. Use this tier to call endpoints that host artificial intelligence and machine teaming (AI/ML) models that are trained and hosted in Amazon SageMaker to perform optical character recognition (OCR) on the forms. Store the output in Amazon ElastiCache. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.\n",
    "\n",
    "D. Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use Amazon Textract and Amazon Comprehend to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the question is about how to automate data validation on the form and extracts relevant data -> use managed service textract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #95\n",
    "A company is refactoring its on-premises order-processing platform in the AWS Cloud. The platform includes a web front end that is hosted on a fleet of VMs, RabbitMQ to connect the front end to the backend, and a Kubernetes cluster to run a containerized backend system to process the orders. The company does not want to make any major changes to the application.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.\n",
    "\n",
    "B. Create a custom AWS Lambda runtime to mimic the web server environment. Create an Amazon API Gateway API to replace the front-end web servers. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.\n",
    "\n",
    "C. Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Install Kubernetes on a fleet of different EC2 instances to host the order-processing backend.\n",
    "\n",
    "D. Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up an Amazon Simple Queue Service (Amazon SQS) queue to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to migrate from on-premises to aws with least change\n",
    "    - Create an AMI of the web server VM\n",
    "    - Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer\n",
    "    - Set up Amazon MQ to replace the on-premises messaging queue\n",
    "    - Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #96\n",
    "A solutions architect needs to implement a client-side encryption mechanism for objects that will be stored in a new Amazon S3 bucket. The solutions architect created a CMK that is stored in AWS Key Management Service (AWS KMS) for this purpose.\n",
    "\n",
    "The solutions architect created the following IAM policy and attached it to an IAM role:\n",
    "\n",
    "\n",
    "\n",
    "During tests, the solutions architect was able to successfully get existing test objects in the S3 bucket. However, attempts to upload a new object resulted in an error message. The error message stated that the action was forbidden.\n",
    "\n",
    "Which action must the solutions architect add to the IAM policy to meet all the requirements?\n",
    "\n",
    "A. kms:GenerateDataKey\n",
    "\n",
    "B. kms:GetKeyPolicy\n",
    "\n",
    "C. kms:GetPublicKey\n",
    "\n",
    "D. kms:Sign\n",
    "Solution: \n",
    "- correct answer is A\n",
    "- the question is about what is the permission required for client-side encrypt for s3 bucket\n",
    "    - we need key for client can encrypt data -> kms:GenerateDataKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #97\n",
    "A company has developed a web application. The company is hosting the application on a group of Amazon EC2 instances behind an Application Load Balancer. The company wants to improve the security posture of the application and plans to use AWS WAF web ACLs. The solution must not adversely affect legitimate traffic to the application.\n",
    "\n",
    "How should a solutions architect configure the web ACLs to meet these requirements?\n",
    "\n",
    "A. Set the action of the web ACL rules to Count. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Count to Block.\n",
    "\n",
    "B. Use only rate-based rules in the web ACLs, and set the throttle limit as high as possible. Temporarily block all requests that exceed the limit. Define nested rules to narrow the scope of the rate tracking.\n",
    "\n",
    "C. Set the action of the web ACL rules to Block. Use only AWS managed rule groups in the web ACLs. Evaluate the rule groups by using Amazon CloudWatch metrics with AWS WAF sampled requests or AWS WAF logs.\n",
    "\n",
    "D. Use only custom rule groups in the web ACLs, and set the action to Allow. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Allow to Block.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- use count and analyze to reduce false positive, modify and then turn to block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #98\n",
    "A company has an organization that has many AWS accounts in AWS Organizations. A solutions architect must improve how the company manages common security group rules for the AWS accounts in the organization.\n",
    "\n",
    "The company has a common set of IP CIDR ranges in an allow list in each AWS account to allow access to and from the company’s on-premises network. Developers within each account are responsible for adding new IP CIDR ranges to their security groups. The security team has its own AWS account. Currently, the security team notifies the owners of the other AWS accounts when changes are made to the allow list.\n",
    "\n",
    "The solutions architect must design a solution that distributes the common set of CIDR ranges across all accounts.\n",
    "\n",
    "Which solution meets these requirements with the LEAST amount of operational overhead?\n",
    "\n",
    "A. Set up an Amazon Simple Notification Service (Amazon SNS) topic in the security team's AWS account. Deploy an AWS Lambda function in each AWS account. Configure the Lambda function to run every time an SNS topic receives a message. Configure the Lambda function to take an IP address as input and add it to a list of security groups in the account. Instruct the security team to distribute changes by publishing messages to its SNS topic.\n",
    "\n",
    "B. Create new customer-managed prefix lists in each AWS account within the organization. Populate the prefix lists in each account with all internal CIDR ranges. Notify the owner of each AWS account to allow the new customer-managed prefix list IDs in their accounts in their security groups. Instruct the security team to share updates with each AWS account owner.\n",
    "\n",
    "C. Create a new customer-managed prefix list in the security team’s AWS account. Populate the customer-managed prefix list with all internal CIDR ranges. Share the customer-managed prefix list with the organization by using AWS Resource Access Manager. Notify the owner of each AWS account to allow the new customer-managed prefix list ID in their security groups.\n",
    "\n",
    "D. Create an IAM role in each account in the organization. Grant permissions to update security groups. Deploy an AWS Lambda function in the security team’s AWS account. Configure the Lambda function to take a list of internal IP addresses as input, assume a role in each organization account, and add the list of IP addresses to the security groups in each account.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about managed common IP CIDR\n",
    "    - prefix list + Resource Access Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #99\n",
    "A company has introduced a new policy that allows employees to work remotely from their homes if they connect by using a VPN. The company is hosting internal applications with VPCs in multiple AWS accounts. Currently, the applications are accessible from the company's on-premises office network through an AWS Site-to-Site VPN connection. The VPC in the company's main AWS account has peering connections established with VPCs in other AWS accounts.\n",
    "\n",
    "A solutions architect must design a scalable AWS Client VPN solution for employees to use while they work from home.\n",
    "\n",
    "What is the MOST cost-effective solution that meets these requirements?\n",
    "\n",
    "A. Create a Client VPN endpoint in each AWS account. Configure required routing that allows access to internal applications.\n",
    "\n",
    "B. Create a Client VPN endpoint in the main AWS account. Configure required routing that allows access to internal applications.\n",
    "\n",
    "C. Create a Client VPN endpoint in the main AWS account. Provision a transit gateway that is connected to each AWS account. Configure required routing that allows access to internal applications.\n",
    "\n",
    "D. Create a Client VPN endpoint in the main AWS account. Establish connectivity between the Client VPN endpoint and the AWS Site-to-Site VPN.\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to config networking with VPN, multiple account with multiple VPC -> using trasit gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #100\n",
    "A company is running an application in the AWS Cloud. Recent application metrics show inconsistent response times and a significant increase in error rates. Calls to third-party services are causing the delays. Currently, the application calls third-party services synchronously by directly invoking an AWS Lambda function.\n",
    "\n",
    "A solutions architect needs to decouple the third-party service calls and ensure that all the calls are eventually completed.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Use an Amazon Simple Queue Service (Amazon SQS) queue to store events and invoke the Lambda function.\n",
    "\n",
    "B. Use an AWS Step Functions state machine to pass events to the Lambda function.\n",
    "\n",
    "C. Use an Amazon EventBridge rule to pass events to the Lambda function.\n",
    "\n",
    "D. Use an Amazon Simple Notification Service (Amazon SNS) topic to store events and Invoke the Lambda function.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to design a system which declouple with the third-party api call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #101\n",
    "A company is running applications on AWS in a multi-account environment. The company's sales team and marketing team use separate AWS accounts in AWS Organizations.\n",
    "\n",
    "The sales team stores petabytes of data in an Amazon S3 bucket. The marketing team uses Amazon QuickSight for data visualizations. The marketing team needs access to data that the sates team stores in the S3 bucket. The company has encrypted the S3 bucket with an AWS Key Management Service (AWS KMS) key. The marketing team has already created the IAM service role for QuickSight to provide QuickSight access in the marketing AWS account. The company needs a solution that will provide secure access to the data in the S3 bucket across AWS accounts.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Create a new S3 bucket in the marketing account. Create an S3 replication rule in the sales account to copy the objects to the new S3 bucket in the marketing account. Update the QuickSight permissions in the marketing account to grant access to the new S3 bucket.\n",
    "\n",
    "B. Create an SCP to grant access to the S3 bucket to the marketing account. Use AWS Resource Access Manager (AWS RAM) to share the KMS key from the sates account with the marketing account. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.\n",
    "\n",
    "C. Update the S3 bucket policy in the marketing account to grant access to the QuickSight role. Create a KMS grant for the encryption key that is used in the S3 bucket. Grant decrypt access to the QuickSight role. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.\n",
    "\n",
    "D. Create an IAM role in the sales account and grant access to the S3 bucket. From the marketing account, assume the IAM role in the sales account to access the S3 bucket. Update the QuickSight rote, to create a trust relationship with the new IAM role in the sales account.\n",
    "Solution:\n",
    "- correct anwer is D\n",
    "- the question is about access s3 data from multiple account\n",
    "    - Create an IAM role in the sales account and grant access to the S3 bucket\n",
    "    - From the marketing account, assume the IAM role in the sales account to access the S3 bucket\n",
    "    - Update the QuickSight rote, to create a trust relationship with the new IAM role in the sales account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #102\n",
    "A company is planning to migrate its business-critical applications from an on-premises data center to AWS. The company has an on-premises installation of a Microsoft SQL Server Always On cluster. The company wants to migrate to an AWS managed database service. A solutions architect must design a heterogeneous database migration on AWS.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Migrate the SQL Server databases to Amazon RDS for MySQL by using backup and restore utilities.\n",
    "\n",
    "B. Use an AWS Snowball Edge Storage Optimized device to transfer data to Amazon S3. Set up Amazon RDS for MySQL. Use S3 integration with SQL Server features, such as BULK INSERT.\n",
    "\n",
    "C. Use the AWS Schema Conversion Tool to translate the database schema to Amazon RDS for MySQL. Then use AWS Database Migration Service (AWS DMS) to migrate the data from on-premises databases to Amazon RDS.\n",
    "\n",
    "D. Use AWS DataSync to migrate data over the network between on-premises storage and Amazon S3. Set up Amazon RDS for MySQL. Use S3 integration with SQL Server features, such as BULK INSERT.\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the questin is about how to migrate Microsoft SQL Server to AWS service\n",
    "    - Use the AWS Schema Conversion Tool to translate the database schema to Amazon RDS for MySQL\n",
    "    - Then use AWS Database Migration Service (AWS DMS) to migrate the data from on-premises databases to Amazon RDS\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #103\n",
    "A publishing company's design team updates the icons and other static assets that an ecommerce web application uses. The company serves the icons and assets from an Amazon S3 bucket that is hosted in the company's production account. The company also uses a development account that members of the design team can access.\n",
    "\n",
    "After the design team tests the static assets in the development account, the design team needs to load the assets into the S3 bucket in the production account. A solutions architect must provide the design team with access to the production account without exposing other parts of the web application to the risk of unwanted changes.\n",
    "\n",
    "Which combination of steps will meet these requirements? (Choose three.)\n",
    "\n",
    "A. In the production account, create a new IAM policy that allows read and write access to the S3 bucket.\n",
    "\n",
    "B. In the development account, create a new IAM policy that allows read and write access to the S3 bucket.\n",
    "\n",
    "C. In the production account, create a role Attach the new policy to the role. Define the development account as a trusted entity.\n",
    "\n",
    "D. In the development account, create a role. Attach the new policy to the role Define the production account as a trusted entity.\n",
    "\n",
    "E. In the development account, create a group that contains all the IAM users of the design team Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role In the production account.\n",
    "\n",
    "F. In the development account, create a group that contains all the IAM users of the design team Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role in the development account.\n",
    "Solution:\n",
    "- correct ansewr is a, c, e\n",
    "- the question is about how to design system which allow development account to load static assets to product account\n",
    "- note: set trusted entity relationship in production account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #104\n",
    "A company developed a pilot application by using AWS Elastic Beanstalk and Java. To save costs during development, the company's development team deployed the application into a single-instance environment. Recent tests indicate that the application consumes more CPU than expected. CPU utilization is regularly greater than 85%, which causes some performance bottlenecks.\n",
    "\n",
    "A solutions architect must mitigate the performance issues before the company launches the application to production.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Create a new Elastic Beanstalk application. Select a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the maximum CPU utilization is over 85% for 5 minutes.\n",
    "\n",
    "B. Create a second Elastic Beanstalk environment. Apply the traffic-splitting deployment policy. Specify a percentage of incoming traffic to direct to the new environment in the average CPU utilization is over 85% for 5 minutes.\n",
    "\n",
    "C. Modify the existing environment’s capacity configuration to use a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the average CPU utilization is over 85% for 5 minutes.\n",
    "\n",
    "D. Select the Rebuild environment action with the load balancing option. Select an Availability Zones. Add a scale-out rule that will run if the sum CPU utilization is over 85% for 5 minutes.\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to config scaling with Elastic Beanstalk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #105\n",
    "A finance company is running its business-critical application on current-generation Linux EC2 instances. The application includes a self-managed MySQL database performing heavy I/O operations. The application is working fine to handle a moderate amount of traffic during the month. However, it slows down during the final three days of each month due to month-end reporting, even though the company is using Elastic Load Balancers and Auto Scaling within its infrastructure to meet the increased demand.\n",
    "\n",
    "Which of the following actions would allow the database to handle the month-end load with the LEAST impact on performance?\n",
    "\n",
    "A. Pre-warming Elastic Load Balancers, using a bigger instance type, changing all Amazon EBS volumes to GP2 volumes.\n",
    "\n",
    "B. Performing a one-time migration of the database cluster to Amazon RDS, and creating several additional read replicas to handle the load during end of month.\n",
    "\n",
    "C. Using Amazon CloudWatch with AWS Lambda to change the type, size, or IOPS of Amazon EBS volumes in the cluster based on a specific CloudWatch metric.\n",
    "\n",
    "D. Replacing all existing Amazon EBS volumes with new PIOPS volumes that have the maximum available storage size and I/O per second by taking snapshots before the end of the month and reverting back afterwards.\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to handle high volume by migrate from self-managed MySQL to amzon rds service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #106\n",
    "A company runs a Java application that has complex dependencies on VMs that are in the company's data center. The application is stable. but the company wants to modernize the technology stack. The company wants to migrate the application to AWS and minimize the administrative overhead to maintain the servers.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST code changes?\n",
    "\n",
    "A. Migrate the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Grant the ECS task execution role permission 10 access the ECR image repository. Configure Amazon ECS to use an Application Load Balancer (ALB). Use the ALB to interact with the application.\n",
    "\n",
    "B. Migrate the application code to a container that runs in AWS Lambda. Build an Amazon API Gateway REST API with Lambda integration. Use API Gateway to interact with the application.\n",
    "\n",
    "C. Migrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) on EKS managed node groups by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Give the EKS nodes permission to access the ECR image repository. Use Amazon API Gateway to interact with the application.\n",
    "\n",
    "D. Migrate the application code to a container that runs in AWS Lambda. Configure Lambda to use an Application Load Balancer (ALB). Use the ALB to interact with the application.\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to migrate java from vm to aws with least code change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #107\n",
    "A company has an asynchronous HTTP application that is hosted as an AWS Lambda function. A public Amazon API Gateway endpoint invokes the Lambda function. The Lambda function and the API Gateway endpoint reside in the us-east-1 Region. A solutions architect needs to redesign the application to support failover to another AWS Region.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Create an API Gateway endpoint in the us-west-2 Region to direct traffic to the Lambda function in us-east-1. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints.\n",
    "\n",
    "B. Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure API Gateway to direct traffic to the SQS queue instead of to the Lambda function. Configure the Lambda function to pull messages from the queue for processing.\n",
    "\n",
    "C. Deploy the Lambda function to the us-west-2 Region. Create an API Gateway endpoint in us-west-2 10 direct traffic to the Lambda function in us-west-2. Configure AWS Global Accelerator and an Application Load Balancer to manage traffic across the two API Gateway endpoints.\n",
    "\n",
    "D. Deploy the Lambda function and an API Gateway endpoint to the us-west-2 Region. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints.\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the question is about design solution for failover with multiple regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #108\n",
    "A retail company has structured its AWS accounts to be part of an organization in AWS Organizations. The company has set up consolidated billing and has mapped its departments to the following OUs: Finance, Sales, Human Resources (HR), Marketing, and Operations. Each OU has multiple AWS accounts, one for each environment within a department. These environments are development, test, pre-production, and production.\n",
    "\n",
    "The HR department is releasing a new system that will launch in 3 months. In preparation, the HR department has purchased several Reserved Instances (RIs) in its production AWS account. The HR department will install the new application on this account. The HR department wants to make sure that other departments cannot share the RI discounts.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. In the AWS Billing and Cost Management console for the HR department's production account turn off RI sharing.\n",
    "\n",
    "B. Remove the HR department's production AWS account from the organization. Add the account 10 the consolidating billing configuration only.\n",
    "\n",
    "C. In the AWS Billing and Cost Management console. use the organization’s management account 10 turn off RI Sharing for the HR departments production AWS account.\n",
    "\n",
    "D. Create an SCP in the organization to restrict access to the RIs. Apply the SCP to the OUs of the other departments.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to disable Reversed Instances discount sharing between multiple OU, account\n",
    "    - In the AWS Billing and Cost Management console. use the organization’s management account 10 turn off RI Sharing for the HR departments production AWS account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #109\n",
    "A large company is running a popular web application. The application runs on several Amazon EC2 Linux instances in an Auto Scaling group in a private subnet. An Application Load Balancer is targeting the instances in the Auto Scaling group in the private subnet. AWS Systems Manager Session Manager is configured, and AWS Systems Manager Agent is running on all the EC2 instances.\n",
    "\n",
    "The company recently released a new version of the application. Some EC2 instances are now being marked as unhealthy and are being terminated. As a result, the application is running at reduced capacity. A solutions architect tries to determine the root cause by analyzing Amazon CloudWatch logs that are collected from the application, but the logs are inconclusive.\n",
    "\n",
    "How should the solutions architect gain access to an EC2 instance to troubleshoot the issue?\n",
    "\n",
    "A. Suspend the Auto Scaling group’s HealthCheck scaling process. Use Session Manager to log in to an instance that is marked as unhealthy.\n",
    "\n",
    "B. Enable EC2 instance termination protection. Use Session Manager to log in to an instance that is marked as unhealthy.\n",
    "\n",
    "C. Set the termination policy to OldestInstance on the Auto Scaling group. Use Session Manager to log in to an instance that is marked an unhealthy.\n",
    "\n",
    "D. Suspend the Auto Scaling group’s Terminate process. Use Session Manager to log in to an instance that is marked as unhealthy.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the question is about how to analyze auto scaling group when instance got delete\n",
    "    - Set the termination policy to OldestInstance on the Auto Scaling group -> stop terminating\n",
    "    - Use Session Manager to log in to an instance that is marked an unhealthy -> investigate the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #110\n",
    "A company wants to deploy an AWS WAF solution to manage AWS WAF rules across multiple AWS accounts. The accounts are managed under different OUs in AWS Organizations.\n",
    "\n",
    "Administrators must be able to add or remove accounts or OUs from managed AWS WAF rule sets as needed. Administrators also must have the ability to automatically update and remediate noncompliant AWS WAF rules in all accounts.\n",
    "\n",
    "Which solution meets these requirements with the LEAST amount of operational overhead?\n",
    "\n",
    "A. Use AWS Firewall Manager to manage AWS WAF rules across accounts in the organization. Use an AWS Systems Manager Parameter Store parameter to store account numbers and OUs to manage. Update the parameter as needed to add or remove accounts or OUs. Use an Amazon EventBridge rule to identify any changes to the parameter and to invoke an AWS Lambda function to update the security policy in the Firewall Manager administrative account.\n",
    "\n",
    "B. Deploy an organization-wide AWS Config rule that requires all resources in the selected OUs to associate the AWS WAF rules. Deploy automated remediation actions by using AWS Lambda to fix noncompliant resources. Deploy AWS WAF rules by using an AWS CloudFormation stack set to target the same OUs where the AWS Config rule is applied.\n",
    "\n",
    "C. Create AWS WAF rules in the management account of the organization. Use AWS Lambda environment variables to store account numbers and OUs to manage. Update environment variables as needed to add or remove accounts or OUs. Create cross-account IAM roles in member accounts. Assume the roles by using AWS Security Token Service (AWS STS) in the Lambda function to create and update AWS WAF rules in the member accounts.\n",
    "\n",
    "D. Use AWS Control Tower to manage AWS WAF rules across accounts in the organization. Use AWS Key Management Service (AWS KMS) to store account numbers and OUs to manage. Update AWS KMS as needed to add or remove accounts or OUs. Create IAM users in member accounts. Allow AWS Control Tower in the management account to use the access key and secret access key to create and update AWS WAF rules in the member accounts.\n",
    "\n",
    "Solution:\n",
    "- correct answer is a\n",
    "- the question is about how to manage AWS WAF rules across multiple AWS accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #111\n",
    "A solutions architect is auditing the security setup or an AWS Lambda function for a company. The Lambda function retrieves, the latest changes from an Amazon Aurora database. The Lambda function and the database run in the same VPC. Lambda environment variables are providing the database credentials to the Lambda function.\n",
    "\n",
    "The Lambda function aggregates data and makes the data available in an Amazon S3 bucket that is configured for server-side encryption with AWS KMS managed encryption keys (SSE-KMS). The data must not travel across the Internet. If any database credentials become compromised, the company needs a solution that minimizes the impact of the compromise.\n",
    "\n",
    "What should the solutions architect recommend to meet these requirements?\n",
    "\n",
    "A. Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Deploy a gateway VPC endpoint for Amazon S3 in the VPC.\n",
    "\n",
    "B. Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Enforce HTTPS on the connection to Amazon S3 during data transfers.\n",
    "\n",
    "C. Save the database credentials in AWS Systems Manager Parameter Store. Set up password rotation on the credentials in Parameter Store. Change the IAM role for the Lambda function to allow the function to access Parameter Store. Modify the Lambda function to retrieve the credentials from Parameter Store. Deploy a gateway VPC endpoint for Amazon S3 in the VPC.\n",
    "\n",
    "D. Save the database credentials in AWS Secrets Manager. Set up password rotation on the credentials in Secrets Manager. Change the IAM role for the Lambda function to allow the function to access Secrets Manager. Modify the Lambda function to retrieve the credentials from Secrets Manager. Enforce HTTPS on the connection to Amazon S3 during data transfers.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about authenticate lambda function and database in the same vpc\n",
    "    - Enable IAM database authentication on the Aurora DB cluster\n",
    "    - Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication\n",
    "    - Deploy a gateway VPC endpoint for Amazon S3 in the VPC\n",
    "    - they key word: The data must not travel across the Internet. Therefore solution is a, d is not mention to handle that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #112\n",
    "A large mobile gaming company has successfully migrated all of its on-premises infrastructure to the AWS Cloud. A solutions architect is reviewing the environment to ensure that it was built according to the design and that it is running in alignment with the Well-Architected Framework.\n",
    "\n",
    "While reviewing previous monthly costs in Cost Explorer, the solutions architect notices that the creation and subsequent termination of several large instance types account for a high proportion of the costs. The solutions architect finds out that the company’s developers are launching new Amazon EC2 instances as part of their testing and that the developers are not using the appropriate instance types.\n",
    "\n",
    "The solutions architect must implement a control mechanism to limit the instance types that only the developers can launch.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Create a desired-instance-type managed rule in AWS Config. Configure the rule with the instance types that are allowed. Attach the rule to an event to run each time a new EC2 instance is launched.\n",
    "\n",
    "B. In the EC2 console, create a launch template that specifies the instance types that are allowed. Assign the launch template to the developers’ IAM accounts.\n",
    "\n",
    "C. Create a new IAM policy. Specify the instance types that are allowed. Attach the policy to an IAM group that contains the IAM accounts for the developers\n",
    "\n",
    "D. Use EC2 Image Builder to create an image pipeline for the developers and assist them in the creation of a golden image.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- Create a new IAM policy. Specify the instance types that are allowed. Attach the policy to an IAM group that contains the IAM accounts for the developers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #113\n",
    "A company is developing and hosting several projects in the AWS Cloud. The projects are developed across multiple AWS accounts under the same organization in AWS Organizations. The company requires the cost for cloud infrastructure to be allocated to the owning project. The team responsible for all of the AWS accounts has discovered that several Amazon EC2 instances are lacking the Project tag used for cost allocation.\n",
    "\n",
    "Which actions should a solutions architect lake to resolve the problem and prevent it from happening in the future? (Choose three.)\n",
    "\n",
    "A. Create an AWS Config rule in each account to find resources with missing tags.\n",
    "\n",
    "B. Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing.\n",
    "\n",
    "C. Use Amazon Inspector in the organization to find resources with missing tags.\n",
    "\n",
    "D. Create an IAM policy in each account with a deny action for ec2:RunInstances if the Project tag is missing.\n",
    "\n",
    "E. Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag.\n",
    "\n",
    "F. Use AWS Security Hub to aggregate a list of EC2 instances with the missing Project tag.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, B, E\n",
    "- the question is about cost allocated\n",
    "    - Create an AWS Config rule in each account to find resources with missing tags\n",
    "    - Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing\n",
    "    - Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #114\n",
    "A company has an on-premises monitoring solution using a PostgreSQL database for persistence of events. The database is unable to scale due to heavy ingestion and it frequently runs out of storage.\n",
    "\n",
    "The company wants to create a hybrid solution and has already set up a VPN connection between its network and AWS. The solution should include the following attributes:\n",
    "\n",
    "• Managed AWS services to minimize operational complexity.\n",
    "\n",
    "• A buffer that automatically scales to match the throughput of data and requires no ongoing administration.\n",
    "\n",
    "• A visualization tool to create dashboards to observe events in near-real time.\n",
    "\n",
    "• Support for semi-structured JSON data and dynamic schemas.\n",
    "\n",
    "Which combination of components will enable the company to create a monitoring solution that will satisfy these requirements? (Choose two.)\n",
    "\n",
    "A. Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events.\n",
    "\n",
    "B. Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events.\n",
    "\n",
    "C. Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.\n",
    "\n",
    "D. Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards.\n",
    "\n",
    "E. Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, D\n",
    "- the question is about how to design monitoring solution\n",
    "    - Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events\n",
    "    - Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards\n",
    "    - because of the requirement\n",
    "        - semi-structure json and dynamic schemas -> can not use relational database -> use elastic search\n",
    "        - persisten of event -> use kinesis firehose instead of kinesis stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #115\n",
    "A team collects and routes behavioral data for an entire company. The company runs a Multi-AZ VPC environment with public subnets, private subnets, and in internet gateway. Each public subnet also contains a NAT gateway. Most of the company’s applications read from and write to Amazon Kinesis Data Streams. Most of the workloads run in private subnets.\n",
    "\n",
    "A solutions architect must review the infrastructure. The solution architect needs to reduce costs and maintain the function of the applications. The solutions architect uses Cost Explorer and notices that the cost in the EC2-Other category is consistently high. A further review shows that NatGateway-Bytes charges are increasing the cost in the EC2-Other category.\n",
    "\n",
    "What should the solutions architect do to meet these requirements?\n",
    "\n",
    "A. Enable VPC Flow Logs. Use Amazon Athena to analyze the logs for traffic that can be removed. Ensure that security groups are blocking traffic that is responsible for high costs.\n",
    "\n",
    "B. Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that applications have the correct IAM permissions to use the interface VPC endpoint.\n",
    "\n",
    "C. Enable VPC Flow Logs and Amazon Detective. Review Detective findings for traffic that is not related to Kinesis Data Streams. Configure security groups to block that traffic.\n",
    "\n",
    "D. Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that the VPC endpoint policy allows traffic from the applications.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- VPC endpoints to mitigate NAT gateway huge data transfer costs especially in Kinesis usecase where large data is passed thru With a VPC endpoint policy, you can define rules to control access to the VPC endpoint. You can specify the source IP address or IP address range that is allowed to access the endpoint, as well as the type of traffic that is allowed, such as HTTP, HTTPS, or custom TCP ports. You can also specify the resources that can be accessed through the VPC endpoint, such as an Amazon S3 bucket or an Amazon DynamoDB table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #116\n",
    "A retail company has an on-premises data center in Europe. The company also has a multi-Region AWS presence that includes the eu-west-1 and us-east-1 Regions. The company wants to be able to route network traffic from its on-premises infrastructure into VPCs in either of those Regions. The company also needs to support traffic that is routed directly between VPCs in those Regions. No single points of failure can exist on the network.\n",
    "\n",
    "The company already has created two 1 Gbps AWS Direct Connect connections from its on-premises data center. Each connection goes into a separate Direct Connect location in Europe for high availability. These two locations are named DX-A and DX-B, respectively. Each Region has a single AWS Transit Gateway that is configured to route all inter-VPC traffic within that Region.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Create a private VIF from the DX-A connection into a Direct Connect gateway. Create a private VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with the Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing.\n",
    "\n",
    "B. Create a transit VIF from the DX-A connection into a Direct Connect gateway. Associate the eu-west-1 transit gateway with this Direct Connect gateway. Create a transit VIF from the DX-8 connection into a separate Direct Connect gateway. Associate the us-east-1 transit gateway with this separate Direct Connect gateway. Peer the Direct Connect gateways with each other to support high availability and cross-Region routing.\n",
    "\n",
    "C. Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Configure the Direct Connect gateway to route traffic between the transit gateways.\n",
    "\n",
    "D. Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the question is about config networking between on-premises with multiple VPC in multiple regions\n",
    "    - Create a transit VIF from the DX-A connection into a Direct Connect gateway\n",
    "    - Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability\n",
    "    - Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway\n",
    "    - Peer the transit gateways with each other to support cross-Region routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #117\n",
    "A company is running an application in the AWS Cloud. The company's security team must approve the creation of all new IAM users. When a new IAM user is created, all access for the user must be removed automatically. The security team must then receive a notification to approve the user. The company has a multi-Region AWS CloudTrail trail in the AWS account.\n",
    "\n",
    "Which combination of steps will meet these requirements? (Choose three.)\n",
    "\n",
    "A. Create an Amazon EventBridge (Amazon CloudWatch Events) rule. Define a pattern with the detail-type value set to AWS API Call via CloudTrail and an eventName of CreateUser.\n",
    "\n",
    "B. Configure CloudTrail to send a notification for the CreateUser event to an Amazon Simple Notification Service (Amazon SNS) topic.\n",
    "\n",
    "C. Invoke a container that runs in Amazon Elastic Container Service (Amazon ECS) with AWS Fargate technology to remove access.\n",
    "\n",
    "D. Invoke an AWS Step Functions state machine to remove access.\n",
    "\n",
    "E. Use Amazon Simple Notification Service (Amazon SNS) to notify the security team.\n",
    "\n",
    "F. Use Amazon Pinpoint to notify the security team.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, D, E\n",
    "- the question is about process to remove access for newly created iam user\n",
    "    - Create an Amazon EventBridge (Amazon CloudWatch Events) rule. Define a pattern with the detail-type value set to AWS API Call via CloudTrail and an eventName of CreateUser\n",
    "    - Invoke an AWS Step Functions state machine to remove access\n",
    "    - Use Amazon Simple Notification Service (Amazon SNS) to notify the security team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #118\n",
    "A company wants to migrate to AWS. The company wants to use a multi-account structure with centrally managed access to all accounts and applications. The company also wants to keep the traffic on a private network. Multi-factor authentication (MFA) is required at login, and specific roles are assigned to user groups.\n",
    "\n",
    "The company must create separate accounts for development. staging, production, and shared network. The production account and the shared network account must have connectivity to all accounts. The development account and the staging account must have access only to each other.\n",
    "\n",
    "Which combination of steps should a solutions architect take 10 meet these requirements? (Choose three.)\n",
    "\n",
    "A. Deploy a landing zone environment by using AWS Control Tower. Enroll accounts and invite existing accounts into the resulting organization in AWS Organizations.\n",
    "\n",
    "B. Enable AWS Security Hub in all accounts to manage cross-account access. Collect findings through AWS CloudTrail to force MFA login.\n",
    "\n",
    "C. Create transit gateways and transit gateway VPC attachments in each account. Configure appropriate route tables.\n",
    "\n",
    "D. Set up and enable AWS IAM Identity Center (AWS Single Sign-On). Create appropriate permission sets with required MFA for existing accounts.\n",
    "\n",
    "E. Enable AWS Control Tower in all accounts to manage routing between accounts. Collect findings through AWS CloudTrail to force MFA login.\n",
    "\n",
    "F. Create IAM users and groups. Configure MFA for all users. Set up Amazon Cognoto user pools and Identity pools to manage access to accounts and between accounts.\n",
    "\n",
    "Solution:\n",
    "- correct answer is a, c, d\n",
    "- the question is about how to design private network for multiple account (development, staging, production)\n",
    "    - Deploy a landing zone environment by using AWS Control Tower. Enroll accounts and invite existing accounts into the resulting organization in AWS Organizations -> allows for a centralized management of access to all accounts and applications.\n",
    "    - Create transit gateways and transit gateway VPC attachments in each account. Configure appropriate route tables -> allows for private network traffic, and ensures that the production account and shared network account have connectivity to all accounts, while the development and staging accounts have access only to each other\n",
    "    - Set up and enable AWS IAM Identity Center (AWS Single Sign-On). Create appropriate permission sets with required MFA for existing accounts -> allows for multi-factor authentication at login and specific roles to be assigned to user groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #119\n",
    "A company runs its application in the eu-west-1 Region and has one account for each of its environments: development, testing, and production. All the environments are running 24 hours a day, 7 days a week by using stateful Amazon EC2 instances and Amazon RDS for MySQL databases. The databases are between 500 GB and 800 GB in size.\n",
    "\n",
    "The development team and testing team work on business days during business hours, but the production environment operates 24 hours a day, 7 days a week. The company wants to reduce costs. All resources are tagged with an environment tag with either development, testing, or production as the key.\n",
    "\n",
    "What should a solutions architect do to reduce costs with the LEAST operational effort?\n",
    "\n",
    "A. Create an Amazon EventBridge rule that runs once every day. Configure the rule to invoke one AWS Lambda function that starts or slops instances based on me tag, day, and time.\n",
    "\n",
    "B. Create an Amazon EventBridge rule that runs every business day in the evening. Configure the rule to invoke an AWS Lambda function that stops instances based on the tag. Create a second EventBridge rule that runs every business day in the morning. Configure the second rule lo invoke another Lambda function that starts instances based on the tag.\n",
    "\n",
    "C. Create an Amazon EventBridge rule that runs every business day in the evening, Configure the rule to invoke an AWS Lambda function that terminates, instances based on the lag. Create a second EventBridge rule that runs every business day in the morning. Configure the second rule lo invoke another Lambda function that restores the instances from their last backup based on the tag.\n",
    "\n",
    "D. Create an Amazon EventBridge rule that runs every hour. Configure the rule to invoke one AWS Lambda function that terminates or restores instances from their last backup based on the tag. day, and time.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to reduce cost base on environment account\n",
    "    - Create an Amazon EventBridge rule that runs every business day in the evening. Configure the rule to invoke an AWS Lambda function that stops instances based on the tag\n",
    "    - Create a second EventBridge rule that runs every business day in the morning. Configure the second rule lo invoke another Lambda function that starts instances based on the tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #120\n",
    "A company is building a software-as-a-service (SaaS) solution on AWS. The company has deployed an Amazon API Gateway REST API with AWS Lambda integration in multiple AWS Regions and in the same production account.\n",
    "\n",
    "The company offers tiered pricing that gives customers the ability to pay for the capacity to make a certain number of API calls per second. The premium tier offers up to 3,000 calls per second, and customers are identified by a unique API key. Several premium tier customers in various Regions report that they receive error responses of 429 Too Many Requests from multiple API methods during peak usage hours. Logs indicate that the Lambda function is never invoked.\n",
    "\n",
    "What could be the cause of the error messages for these customers?\n",
    "\n",
    "A. The Lambda function reached its concurrency limit.\n",
    "\n",
    "B. The Lambda function its Region limit for concurrency.\n",
    "\n",
    "C. The company reached its API Gateway account limit for calls per second.\n",
    "\n",
    "D. The company reached its API Gateway default per-method limit for calls per second.\n",
    "Solution:\n",
    "- correct answer is c\n",
    "- the question is about possible cause for 29 Too Many Requests\n",
    "    - The company reached its API Gateway account limit for calls per second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #121\n",
    "A financial company is planning to migrate its web application from on premises to AWS. The company uses a third-party security tool to monitor the inbound traffic to the application. The company has used the security tool for the last 15 years, and the tool has no cloud solutions available from its vendor. The company's security team is concerned about how to integrate the security tool with AWS technology.\n",
    "\n",
    "The company plans to deploy the application migration to AWS on Amazon EC2 instances. The EC2 instances will run in an Auto Scaling group in a dedicated VPC. The company needs to use the security tool to inspect all packets that come in and out of the VPC. This inspection must occur in real time and must not affect the application's performance. A solutions architect must design a target architecture on AWS that is highly available within an AWS Region.\n",
    "\n",
    "Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)\n",
    "\n",
    "A. Deploy the security tool on EC2 instances m a new Auto Scaling group in the existing VPC\n",
    "\n",
    "B. Deploy the web application behind a Network Load Balancer\n",
    "\n",
    "C. Deploy an Application Load Balancer in front of the security tool instances\n",
    "\n",
    "D. Provision a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool\n",
    "\n",
    "E. Provision a transit gateway to facilitate communication between VPCs.\n",
    "Solution:\n",
    "- correct answer is A, D\n",
    "- the question is about how to deploy security tools which do not have cloud version\n",
    "    - Deploy the security tool on EC2 instances m a new Auto Scaling group in the existing VPC\n",
    "    - Provision a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool -> all traffic will reach security for inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #122\n",
    "A company has purchased appliances from different vendors. The appliances all have IoT sensors. The sensors send status information in the vendors' proprietary formats to a legacy application that parses the information into JSON. The parsing is simple, but each vendor has a unique format. Once daily, the application parses all the JSON records and stores the records in a relational database for analysis.\n",
    "\n",
    "The company needs to design a new data analysis solution that can deliver faster and optimize costs.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Connect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function to parse the information and save a .csv file to Amazon. S3 Use AWS Glue to catalog the files. Use Amazon Athena and Amazon QuickSight for analysis.\n",
    "\n",
    "B. Migrate the application server to AWS Fargate, which will receive the information from IoT sensors and parse the information into a relational format. Save the parsed information to Amazon Redshlft for analysis.\n",
    "\n",
    "C. Create an AWS Transfer for SFTP server. Update the IoT sensor code to send the information as a .csv file through SFTP to the server. Use AWS Glue to catalog the files. Use Amazon Athena for analysis.\n",
    "\n",
    "D. Use AWS Snowball Edge to collect data from the IoT sensors directly to perform local analysis. Periodically collect the data into Amazon Redshift to perform global analysis.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about solution for IOT devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #123\n",
    "A company is migrating some of its applications to AWS. The company wants to migrate and modernize the applications quickly after it finalizes networking and security strategies. The company has set up an AWS Direct Connect connection in a central network account.\n",
    "\n",
    "The company expects to have hundreds of AWS accounts and VPCs in the near future. The corporate network must be able to access the resources on AWS seamlessly and also must be able to communicate with all the VPCs. The company also wants to route its cloud resources to the internet through its on-premises data center.\n",
    "\n",
    "Which combination of steps will meet these requirements? (Choose three.)\n",
    "\n",
    "A. Create a Direct Connect gateway in the central account. In each of the accounts, create an association proposal by using the Direct Connect gateway and the account ID for every virtual private gateway.\n",
    "\n",
    "B. Create a Direct Connect gateway and a transit gateway in the central network account. Attach the transit gateway to the Direct Connect gateway by using a transit VIF.\n",
    "\n",
    "C. Provision an internet gateway. Attach the internet gateway to subnets. Allow internet traffic through the gateway.\n",
    "\n",
    "D. Share the transit gateway with other accounts. Attach VPCs to the transit gateway.\n",
    "\n",
    "E. Provision VPC peering as necessary.\n",
    "\n",
    "F. Provision only private subnets. Open the necessary route on the transit gateway and customer gateway to allow outbound internet traffic from AWS to flow through NAT services that run in the data center.\n",
    "\n",
    "\n",
    "Solution:\n",
    "- correct answer is B, D, F\n",
    "- the question is about how to design networking solution for large corporation\n",
    "    - Create a Direct Connect gateway and a transit gateway in the central network account. Attach the transit gateway to the Direct Connect gateway by using a transit VIF\n",
    "    - Share the transit gateway with other accounts. Attach VPCs to the transit gateway\n",
    "    - Provision only private subnets. Open the necessary route on the transit gateway and customer gateway to allow outbound internet traffic from AWS to flow through NAT services that run in the data center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #124\n",
    "A company has hundreds of AWS accounts. The company recently implemented a centralized internal process for purchasing new Reserved Instances and modifying existing Reserved Instances. This process requires all business units that want to purchase or modify Reserved Instances to submit requests to a dedicated team for procurement. Previously, business units directly purchased or modified Reserved Instances in their own respective AWS accounts autonomously.\n",
    "\n",
    "A solutions architect needs to enforce the new process in the most secure way possible.\n",
    "\n",
    "Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)\n",
    "\n",
    "A. Ensure that all AWS accounts are part of an organization in AWS Organizations with all features enabled.\n",
    "\n",
    "B. Use AWS Config to report on the attachment of an IAM policy that denies access to the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action.\n",
    "\n",
    "C. In each AWS account, create an IAM policy that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action.\n",
    "\n",
    "D. Create an SCP that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action. Attach the SCP to each OU of the organization.\n",
    "\n",
    "E. Ensure that all AWS accounts are part of an organization in AWS Organizations that uses the consolidated billing feature.\n",
    "\n",
    "Solution:\n",
    "- correct answer is a, d\n",
    "- the question is about how to centralize resource precurement\n",
    "    - Ensure that all AWS accounts are part of an organization in AWS Organizations with all features enabled\n",
    "    - Create an SCP that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action. Attach the SCP to each OU of the organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #125\n",
    "A company is running a critical application that uses an Amazon RDS for MySQL database to store data. The RDS DB instance is deployed in Multi-AZ mode.\n",
    "\n",
    "A recent RDS database failover test caused a 40-second outage to the application. A solutions architect needs to design a solution to reduce the outage time to less than 20 seconds.\n",
    "\n",
    "Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)\n",
    "\n",
    "A. Use Amazon ElastiCache for Memcached in front of the database\n",
    "\n",
    "B. Use Amazon ElastiCache for Redis in front of the database\n",
    "\n",
    "C. Use RDS Proxy in front of the database.\n",
    "\n",
    "D. Migrate the database to Amazon Aurora MySQL.\n",
    "\n",
    "E. Create an Amazon Aurora Replica.\n",
    "\n",
    "F. Create an RDS for MySQL read replica\n",
    "\n",
    "Solution:\n",
    "- correct answer is C, D, E\n",
    "- the question is about how to design relational database which reduce the outage time to less than 20 seconds\n",
    "    - Use RDS Proxy in front of the database\n",
    "    - Migrate the database to Amazon Aurora MySQL\n",
    "    - create an Amazon Aurora Replica\n",
    "    - to repduce recovery time: use proxy + change from RDS to Aurora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #126\n",
    "An AWS partner company is building a service in AWS Organizations using its organization named org1. This service requires the partner company to have access to AWS resources in a customer account, which is in a separate organization named org2. The company must establish least privilege security access using an API or command line tool to the customer account.\n",
    "\n",
    "What is the MOST secure way to allow org1 to access resources in org2?\n",
    "\n",
    "A. The customer should provide the partner company with their AWS account access keys to log in and perform the required tasks.\n",
    "\n",
    "B. The customer should create an IAM user and assign the required permissions to the IAM user. The customer should then provide the credentials to the partner company to log in and perform the required tasks.\n",
    "\n",
    "C. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role’s Amazon Resource Name (ARN) when requesting access to perform the required tasks.\n",
    "\n",
    "D. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role’s Amazon Resource Name (ARN), including the external ID in the IAM role’s trust policy, when requesting access to perform the required tasks.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the question is about how to establish least privilege security access using an API or command line tool to the customer account which in different org\n",
    "    - The customer should create an IAM role and assign the required permissions to the IAM role\n",
    "    - The partner company should then use the IAM role’s Amazon Resource Name (ARN), including the external ID in the IAM role’s trust policy, when requesting access to perform the required tasks -> use external id to config trust policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #127\n",
    "A delivery company needs to migrate its third-party route planning application to AWS. The third party supplies a supported Docker image from a public registry. The image can run in as many containers as required to generate the route map.\n",
    "\n",
    "The company has divided the delivery area into sections with supply hubs so that delivery drivers travel the shortest distance possible from the hubs to the customers. To reduce the time necessary to generate route maps, each section uses its own set of Docker containers with a custom configuration that processes orders only in the section's area.\n",
    "\n",
    "The company needs the ability to allocate resources cost-effectively based on the number of running containers.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on Amazon EC2. Use the Amazon EKS CLI to launch the planning application in pods by using the --tags option to assign a custom tag to the pod.\n",
    "\n",
    "B. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on AWS Fargate. Use the Amazon EKS CLI to launch the planning application. Use the AWS CLI tag-resource API call to assign a custom tag to the pod.\n",
    "\n",
    "C. Create an Amazon Elastic Container Service (Amazon ECS) cluster on Amazon EC2. Use the AWS CLI with run-tasks set to true to launch the planning application by using the --tags option to assign a custom tag to the task.\n",
    "\n",
    "D. Create an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargate. Use the AWS CLI run-task command and set enableECSManagedTags to true to launch the planning application. Use the --tags option to assign a custom tag to the task.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the question is about running cost-effective container application\n",
    "    - prefer to use ECS over EKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #128\n",
    "A software company hosts an application on AWS with resources in multiple AWS accounts and Regions. The application runs on a group of Amazon EC2 instances in an application VPC located in the us-east-1 Region with an IPv4 CIDR block of 10.10.0.0/16. In a different AWS account, a shared services VPC is located in the us-east-2 Region with an IPv4 CIDR block of 10.10.10.0/24. When a cloud engineer uses AWS CloudFormation to attempt to peer the application VPC with the shared services VPC, an error message indicates a peering failure.\n",
    "\n",
    "Which factors could cause this error? (Choose two.)\n",
    "\n",
    "A. The IPv4 CIDR ranges of the two VPCs overlap\n",
    "\n",
    "B. The VPCs are not in the same Region\n",
    "\n",
    "C. One or both accounts do not have access to an Internet gateway\n",
    "\n",
    "D. One of the VPCs was not shared through AWS Resource Access Manager\n",
    "\n",
    "E. The IAM role in the peer accepter account does not have the correct permissions\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, E\n",
    "- the question is about when peer the application VPC with the shared services VPC what is possible cause of error\n",
    "    - The IPv4 CIDR ranges of the two VPCs overlap\n",
    "    - The IAM role in the peer accepter account does not have the correct permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #129\n",
    "An external audit of a company’s serverless application reveals IAM policies that grant too many permissions. These policies are attached to the company's AWS Lambda execution roles. Hundreds of the company's Lambda functions have broad access permissions such as full access to Amazon S3 buckets and Amazon DynamoDB tables. The company wants each function to have only the minimum permissions that the function needs to complete its task.\n",
    "\n",
    "A solutions architect must determine which permissions each Lambda function needs.\n",
    "\n",
    "What should the solutions architect do to meet this requirement with the LEAST amount of effort?\n",
    "\n",
    "A. Set up Amazon CodeGuru to profile the Lambda functions and search for AWS API calls. Create an inventory of the required API calls and resources for each Lambda function. Create new IAM access policies for each Lambda function. Review the new policies to ensure that they meet the company's business requirements.\n",
    "\n",
    "B. Turn on AWS CloudTrail logging for the AWS account. Use AWS Identity and Access Management Access Analyzer to generate IAM access policies based on the activity recorded in the CloudTrail log. Review the generated policies to ensure that they meet the company's business requirements.\n",
    "\n",
    "C. Turn on AWS CloudTrail logging for the AWS account. Create a script to parse the CloudTrail log, search for AWS API calls by Lambda execution role, and create a summary report. Review the report. Create IAM access policies that provide more restrictive permissions for each Lambda function.\n",
    "\n",
    "D. Turn on AWS CloudTrail logging for the AWS account. Export the CloudTrail logs to Amazon S3. Use Amazon EMR to process the CloudTrail logs in Amazon S3 and produce a report of API calls and resources used by each execution role. Create a new IAM access policy for each role. Export the generated roles to an S3 bucket. Review the generated policies to ensure that they meet the company’s business requirements.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to manage permission -> avoid one application role have to many permission\n",
    "    - Turn on AWS CloudTrail logging for the AWS account. Use AWS Identity and Access Management Access Analyzer to generate IAM access policies based on the activity recorded in the CloudTrail lo\n",
    "    - Review the generated policies to ensure that they meet the company's business requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #130\n",
    "A solutions architect must analyze a company’s Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) volumes to determine whether the company is using resources efficiently. The company is running several large, high-memory EC2 instances to host database clusters that are deployed in active/passive configurations. The utilization of these EC2 instances varies by the applications that use the databases, and the company has not identified a pattern.\n",
    "\n",
    "The solutions architect must analyze the environment and take action based on the findings.\n",
    "\n",
    "Which solution meets these requirements MOST cost-effectively?\n",
    "\n",
    "A. Create a dashboard by using AWS Systems Manager OpsCenter. Configure visualizations for Amazon CloudWatch metrics that are associated with the EC2 instances and their EBS volumes. Review the dashboard periodically, and identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics.\n",
    "\n",
    "B. Turn on Amazon CloudWatch detailed monitoring for the EC2 instances and their EBS volumes. Create and review a dashboard that is based on the metrics. Identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics.\n",
    "\n",
    "C. Install the Amazon CloudWatch agent on each of the EC2 instances. Turn on AWS Compute Optimizer, and let it run for at least 12 hours. Review the recommendations from Compute Optimizer, and rightsize the EC2 instances as directed.\n",
    "\n",
    "D. Sign up for the AWS Enterprise Support plan. Turn on AWS Trusted Advisor. Wait 12 hours. Review the recommendations from Trusted Advisor, and rightsize the EC2 instances as directed.\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to analyze resource using\n",
    "    - Install the Amazon CloudWatch agent on each of the EC2 instances\n",
    "    - Turn on AWS Compute Optimizer, and let it run for at least 12 hours\n",
    "    - Review the recommendations from Compute Optimizer, and rightsize the EC2 instances as directed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #131\n",
    "A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company uses AWS Control Tower for governance and uses AWS Transit Gateway for VPC connectivity across accounts.\n",
    "\n",
    "In an AWS application account, the company’s application team has deployed a web application that uses AWS Lambda and Amazon RDS. The company's database administrators have a separate DBA account and use the account to centrally manage all the databases across the organization. The database administrators use an Amazon EC2 instance that is deployed in the DBA account to access an RDS database that is deployed m the application account.\n",
    "\n",
    "The application team has stored the database credentials as secrets in AWS Secrets Manager in the application account. The application team is manually sharing the secrets with the database administrators. The secrets are encrypted by the default AWS managed key for Secrets Manager in the application account. A solutions architect needs to implement a solution that gives the database administrators access to the database and eliminates the need to manually share the secrets.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Use AWS Resource Access Manager (AWS RAM) to share the secrets from the application account with the DBA account. In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the shared secrets. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.\n",
    "\n",
    "B. In the application account, create an IAM role that is named DBA-Secret. Grant the role the required permissions to access the secrets. In the DBA account, create an IAM role that is named DBA-Admin. Grant the DBA-Admin role the required permissions to assume the DBA-Secret role in the application account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets\n",
    "\n",
    "C. In the DBA account create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets and the default AWS managed key in the application account. In the application account, attach resource-based policies to the key to allow access from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.\n",
    "\n",
    "D. In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets in the application account. Attach an SCP to the application account to allow access to the secrets from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to manage shared secret instead\n",
    "    - In the application account, create an IAM role that is named DBA-Secret, Grant the role the required permissions to access the secrets\n",
    "    - In the DBA account, create an IAM role that is named DBA-Admin. Grant the DBA-Admin role the required permissions to assume the DBA-Secret role in the application account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets\n",
    "    - application account own the database and screte -> we need role for permission. dba account need to role to assume role of appliacaiton role. attache dba role to ec2 instance to access cross-account secret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #132\n",
    "A company manages multiple AWS accounts by using AWS Organizations. Under the root OU, the company has two OUs: Research and DataOps.\n",
    "\n",
    "Because of regulatory requirements, all resources that the company deploys in the organization must reside in the ap-northeast-1 Region. Additionally, EC2 instances that the company deploys in the DataOps OU must use a predefined list of instance types.\n",
    "\n",
    "A solutions architect must implement a solution that applies these restrictions. The solution must maximize operational efficiency and must minimize ongoing maintenance.\n",
    "\n",
    "Which combination of steps will meet these requirements? (Choose two.)\n",
    "\n",
    "A. Create an IAM role in one account under the DataOps OU. Use the ec2:InstanceType condition key in an inline policy on the role to restrict access to specific instance type.\n",
    "\n",
    "B. Create an IAM user in all accounts under the root OU. Use the aws:RequestedRegion condition key in an inline policy on each user to restrict access to all AWS Regions except ap-northeast-1.\n",
    "\n",
    "C. Create an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU.\n",
    "\n",
    "D. Create an SCP. Use the ec2:Region condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU, the DataOps OU, and the Research OU.\n",
    "\n",
    "E. Create an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU.\n",
    "Solution:\n",
    "- correct answer is C, E\n",
    "- the question is about how to config constraint\n",
    "    - all resources that the company deploys in the organization must reside in the ap-northeast-1 Region -> Create an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU\n",
    "    - C2 instances that the company deploys in the DataOps OU must use a predefined list of instance types -> Create an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #133\n",
    "A company runs a serverless application in a single AWS Region. The application accesses external URLs and extracts metadata from those sites. The company uses an Amazon Simple Notification Service (Amazon SNS) topic to publish URLs to an Amazon Simple Queue Service (Amazon SQS) queue. An AWS Lambda function uses the queue as an event source and processes the URLs from the queue. Results are saved to an Amazon S3 bucket.\n",
    "\n",
    "The company wants to process each URL in other Regions to compare possible differences in site localization. URLs must be published from the existing Region. Results must be written to the existing S3 bucket in the current Region.\n",
    "\n",
    "Which combination of changes will produce multi-Region deployment that meets these requirements? (Choose two.)\n",
    "\n",
    "A. Deploy the SQS queue with the Lambda function to other Regions.\n",
    "\n",
    "B. Subscribe the SNS topic in each Region to the SQS queue.\n",
    "\n",
    "C. Subscribe the SQS queue in each Region to the SNS topic.\n",
    "\n",
    "D. Configure the SQS queue to publish URLs to SNS topics in each Region.\n",
    "\n",
    "E. Deploy the SNS topic and the Lambda function to other Regions.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, C\n",
    "- the question is about multi-Region deployment\n",
    "    - Deploy the SQS queue with the Lambda function to other Regions -> llow the application to process URLs in those regions and compare differences in site localization\n",
    "    - Subscribe the SQS queue in each Region to the SNS topic -> Allow the application to publish URLs to the existing SNS topic and have those URLs processed in other regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #134\n",
    "A company runs a proprietary stateless ETL application on an Amazon EC2 Linux instances. The application is a Linux binary, and the source code cannot be modified. The application is single-threaded, uses 2 GB of RAM, and is highly CPU intensive. The application is scheduled to run every 4 hours and runs for up to 20 minutes. A solutions architect wants to revise the architecture for the solution.\n",
    "\n",
    "Which strategy should the solutions architect use?\n",
    "\n",
    "A. Use AWS Lambda to run the application. Use Amazon CloudWatch Logs to invoke the Lambda function every 4 hours.\n",
    "\n",
    "B. Use AWS Batch to run the application. Use an AWS Step Functions state machine to invoke the AWS Batch job every 4 hours.\n",
    "\n",
    "C. Use AWS Fargate to run the application. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours.\n",
    "\n",
    "D. Use Amazon EC2 Spot Instances to run the application. Use AWS CodeDeploy to deploy and run the application every 4 hours.\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about which compute service to use for schedule task\n",
    "    - lambda is for less than or equal to 15 min\n",
    "    - EventBridge is event driven, it is also use to schedule event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #135\n",
    "A company is creating a sequel for a popular online game. A large number of users from all over the world will play the game within the first week after launch. Currently, the game consists of the following components deployed in a single AWS Region:\n",
    "\n",
    "• Amazon S3 bucket that stores game assets\n",
    "\n",
    "• Amazon DynamoDB table that stores player scores\n",
    "\n",
    "A solutions architect needs to design a multi-Region solution that will reduce latency, improve reliability, and require the least effort to implement.\n",
    "\n",
    "What should the solutions architect do to meet these requirements?\n",
    "\n",
    "A. Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Cross-Region Replication. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables.\n",
    "\n",
    "B. Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Same-Region Replication. Create a new DynamoDB table in a new Region. Configure asynchronous replication between the DynamoDB tables by using AWS Database Migration Service (AWS DMS) with change data capture (CDC).\n",
    "\n",
    "C. Create another S3 bucket in a new Region, and configure S3 Cross-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets in each Region. Configure DynamoDB global tables by enabling Amazon DynamoDB Streams, and add a replica table in a new Region.\n",
    "\n",
    "D. Create another S3 bucket in the sine Region, and configure S3 Same-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to design system with multiple region to reduce latency, improve reliability\n",
    "    - Create another S3 bucket in a new Region, and configure S3 Cross-Region Replication between the buckets\n",
    "    - Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets in each Region\n",
    "    - Configure DynamoDB global tables by enabling Amazon DynamoDB Streams, and add a replica table in a new Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #136\n",
    "A company has an on-premises website application that provides real estate information for potential renters and buyers. The website uses a Java backend and a NoSQL MongoDB database to store subscriber data.\n",
    "\n",
    "The company needs to migrate the entire application to AWS with a similar structure. The application must be deployed for high availability, and the company cannot make changes to the application.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Use an Amazon Aurora DB cluster as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.\n",
    "\n",
    "B. Use MongoDB on Amazon EC2 instances as the database for the subscriber data. Deploy EC2 instances in an Auto Scaling group in a single Availability Zone for the Java backend application.\n",
    "\n",
    "C. Configure Amazon DocumentDB (with MongoDB compatibility) with appropriately sized instances in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.\n",
    "\n",
    "D. Configure Amazon DocumentDB (with MongoDB compatibility) in on-demand capacity mode in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to migrate from on-premises to aws with less change in code\n",
    "    - Configure Amazon DocumentDB (with MongoDB compatibility) with appropriately sized instances in multiple Availability Zones as the database for the subscriber data\n",
    "    - Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application\n",
    "    - can not choose d because there is no on-deman capacity mode but only have on-demend instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #137\n",
    "A digital marketing company has multiple AWS accounts that belong to various teams. The creative team uses an Amazon S3 bucket in its AWS account to securely store images and media files that are used as content for the company’s marketing campaigns. The creative team wants to share the S3 bucket with the strategy team so that the strategy team can view the objects.\n",
    "\n",
    "A solutions architect has created an IAM role that is named strategy_reviewer in the Strategy account. The solutions architect also has set up a custom AWS Key Management Service (AWS KMS) key in the Creative account and has associated the key with the S3 bucket. However, when users from the Strategy account assume the IAM role and try to access objects in the S3 bucket, they receive an Access Denied error.\n",
    "\n",
    "The solutions architect must ensure that users in the Strategy account can access the S3 bucket. The solution must provide these users with only the minimum permissions that they need.\n",
    "\n",
    "Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)\n",
    "\n",
    "A. Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to the account ID of the Strategy account.\n",
    "\n",
    "B. Update the strategy_reviewer IAM role to grant full permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key.\n",
    "\n",
    "C. Update the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role.\n",
    "\n",
    "D. Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to an anonymous user.\n",
    "\n",
    "E. Update the custom KMS key policy in the Creative account to grant encrypt permissions to the strategy_reviewer IAM role.\n",
    "\n",
    "F. Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, C, F\n",
    "- the question is about how to design access to s3 between multiple accounts\n",
    "    - Update the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role\n",
    "    - Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key\n",
    "    - Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to the account ID of the Strategy account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #138\n",
    "A life sciences company is using a combination of open source tools to manage data analysis workflows and Docker containers running on servers in its on-premises data center to process genomics data. Sequencing data is generated and stored on a local storage area network (SAN), and then the data is processed. The research and development teams are running into capacity issues and have decided to re-architect their genomics analysis platform on AWS to scale based on workload demands and reduce the turnaround time from weeks to days.\n",
    "\n",
    "The company has a high-speed AWS Direct Connect connection. Sequencers will generate around 200 GB of data for each genome, and individual jobs can take several hours to process the data with ideal compute capacity. The end result will be stored in Amazon S3. The company is expecting 10-15 job requests each day.\n",
    "\n",
    "Which solution meets these requirements?\n",
    "\n",
    "A. Use regularly scheduled AWS Snowball Edge devices to transfer the sequencing data into AWS. When AWS receives the Snowball Edge device and the data is loaded into Amazon S3, use S3 events to trigger an AWS Lambda function to process the data.\n",
    "\n",
    "B. Use AWS Data Pipeline to transfer the sequencing data to Amazon S3. Use S3 events to trigger an Amazon EC2 Auto Scaling group to launch custom-AMI EC2 instances running the Docker containers to process the data.\n",
    "\n",
    "C. Use AWS DataSync to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow. Store the Docker images in Amazon Elastic Container Registry (Amazon ECR) and trigger AWS Batch to run the container and process the sequencing data.\n",
    "\n",
    "D. Use an AWS Storage Gateway file gateway to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Batch job that executes on Amazon EC2 instances running the Docker containers to process the data.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to architect gene processing data\n",
    "    - Use AWS DataSync to transfer the sequencing data to Amazon S3\n",
    "    - Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow. Store the Docker images in Amazon Elastic Container Registry (Amazon ECR) and trigger AWS Batch to run the container and process the sequencing data\n",
    "    - large data is processed at ec2 and then move to s3. For moving large data problem choose data sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #139\n",
    "A company runs a content management application on a single Windows Amazon EC2 instance in a development environment. The application reads and writes static content to a 2 TB Amazon Elastic Block Store (Amazon EBS) volume that is attached to the instance as the root device. The company plans to deploy this application in production as a highly available and fault-tolerant solution that runs on at least three EC2 instances across multiple Availability Zones.\n",
    "\n",
    "A solutions architect must design a solution that joins all the instances that run the application to an Active Directory domain. The solution also must implement Windows ACLs to control access to file contents. The application always must maintain exactly the same content on all running instances at any given point in time.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST management overhead?\n",
    "\n",
    "A. Create an Amazon Elastic File System (Amazon EFS) file share. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application, join the instance to the AD domain, and mount the EFS file share.\n",
    "\n",
    "B. Create a new AMI from the current EC2 Instance that is running. Create an Amazon FSx for Lustre file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to join the instance to the AD domain and mount the FSx for Lustre file system.\n",
    "\n",
    "C. Create an Amazon FSx for Windows File Server file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application and mount the FSx for Windows File Server file system. Perform a seamless domain join to join the instance to the AD domain.\n",
    "\n",
    "D. Create a new AMI from the current EC2 instance that is running. Create an Amazon Elastic File System (Amazon EFS) file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three Instances. Perform a seamless domain join to join the instance to the AD domain.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to design a content management application which is highly available and fault-tolerant solution that runs on at least three EC2 instances across multiple Availability Zones\n",
    "    - Create an Amazon FSx for Windows File Server file system -> replace file system with window file system\n",
    "    - Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application and mount the FSx for Windows File Server file system. Perform a seamless domain join to join the instance to the AD domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #140\n",
    "A software as a service (SaaS) based company provides a case management solution to customers A3 part of the solution. The company uses a standalone Simple Mail Transfer Protocol (SMTP) server to send email messages from an application. The application also stores an email template for acknowledgement email messages that populate customer data before the application sends the email message to the customer.\n",
    "\n",
    "The company plans to migrate this messaging functionality to the AWS Cloud and needs to minimize operational overhead.\n",
    "\n",
    "Which solution will meet these requirements MOST cost-effectively?\n",
    "\n",
    "A. Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message.\n",
    "\n",
    "B. Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message.\n",
    "\n",
    "C. Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in Amazon Simple Email Service (Amazon SES) with parameters for the customer data. Create an AWS Lambda function to call the SES template and to pass customer data to replace the parameters. Use the AWS Marketplace SMTP server to send the email message.\n",
    "\n",
    "D. Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template on Amazon SES with parameters for the customer data. Create an AWS Lambda function to call the SendTemplatedEmail API operation and to pass customer data to replace the parameters and the email destination.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the question is about how to migrate FROM SMTP to Simple Email Service\n",
    "    - Set up Amazon Simple Email Service (Amazon SES) to send email messages\n",
    "    - Store the email template on Amazon SES with parameters for the customer data\n",
    "    - Create an AWS Lambda function to call the SendTemplatedEmail API operation and to pass customer data to replace the parameters and the email destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #141\n",
    "A company is processing videos in the AWS Cloud by Using Amazon EC2 instances in an Auto Scaling group. It takes 30 minutes to process a video Several EC2 instances scale in and out depending on the number of videos in an Amazon Simple Queue Service (Amazon SQS) queue.\n",
    "\n",
    "The company has configured the SQS queue with a redrive policy that specifies a target dead-letter queue and a maxReceiveCount of 1. The company has set the visibility timeout for the SQS queue to 1 hour. The company has set up an Amazon CloudWatch alarm to notify the development team when there are messages in the dead-letter queue.\n",
    "\n",
    "Several times during the day. the development team receives notification that messages are in the dead-letter queue and that videos have not been processed property. An investigation finds no errors m the application logs.\n",
    "\n",
    "How can the company solve this problem?\n",
    "\n",
    "A. Turn on termination protection tor the EC2 Instances\n",
    "\n",
    "B. Update the visibility timeout for the SQS queue to 3 hours\n",
    "\n",
    "C. Configure scale-in protection for the instances during processing\n",
    "\n",
    "D. Update the redrive policy and set maxReceiveCount to 0.\n",
    "\n",
    "Solution:\n",
    "- correct answr is C\n",
    "- the problem is about we have no error but have dead-letter queue\n",
    "    - Configure scale-in protection for the instances during processing\n",
    "    - The company can solve the problem by configuring scale-in protection for the instances during processing. This will ensure that the instances are not terminated while they are processing videos. This will prevent the messages from moving to the dead-letter queue and ensure that videos are processed properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #143\n",
    "A weather service provides high-resolution weather maps from a web application hosted on AWS in the eu-west-1 Region. The weather maps are updated frequently and stored in Amazon S3 along with static HTML content. The web application is fronted by Amazon CloudFront.\n",
    "\n",
    "The company recently expanded to serve users in the us-east-1 Region, and these new users report that viewing their respective weather maps is slow from time to time.\n",
    "\n",
    "Which combination of steps will resolve the us-east-1 performance issues? (Choose two.)\n",
    "\n",
    "A. Configure the AWS Global Accelerator endpoint for the S3 bucket in eu-west-1. Configure endpoint groups for TCP ports 80 and 443 in us-east-1.\n",
    "\n",
    "B. Create a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1.\n",
    "\n",
    "C. Use Lambda@Edge to modify requests from North America to use the S3 Transfer Acceleration endpoint in us-east-1.\n",
    "\n",
    "D. Use Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1.\n",
    "\n",
    "E. Configure the AWS Global Accelerator endpoint for us-east-1 as an origin on the CloudFront distribution. Use Lambda@Edge to modify requests from North America to use the new origin.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B, D\n",
    "- the question is about expand application to other region\n",
    "    - Create a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1\n",
    "    - Use Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1\n",
    "    - Global Accelerator can't have an s3 bucket as endpoint\n",
    "    - An accelerator as cloudfront origin does not make much sense, because cloudfront is already using the AWS network. Global Accelerator is usually for Layer 4 networking and/or static anycast IPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #144\n",
    "A solutions architect is investigating an issue in which a company cannot establish new sessions in Amazon Workspaces. An initial analysis indicates that the issue involves user profiles. The Amazon Workspaces environment is configured to use Amazon FSx for Windows File Server as the profile share storage. The FSx for Windows File Server file system is configured with 10 TB of storage.\n",
    "\n",
    "The solutions architect discovers that the file system has reached Its maximum capacity. The solutions architect must ensure that users can regain access. The solution also must prevent the problem from occurring again.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Remove old user profiles to create space. Migrate the user profiles to an Amazon FSx for Lustre file system.\n",
    "\n",
    "B. Increase capacity by using the update-file-system command. Implement an Amazon CloudWatch metric that monitors free space. Use Amazon EventBridge to invoke an AWS Lambda function to increase capacity as required.\n",
    "\n",
    "C. Monitor the file system by using the FreeStorageCapacity metric in Amazon CloudWatch. Use AWS Step Functions to increase the capacity as required.\n",
    "\n",
    "D. Remove old user profiles to create space. Create an additional FSx for Windows File Server file system. Update the user profile redirection for 50% of the users to use the new file system.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to solve the file system has reached Its maximum capacity\n",
    "    - Increase capacity by using the update-file-system command. Implement an Amazon CloudWatch metric that monitors free space. Use Amazon EventBridge to invoke an AWS Lambda function to increase capacity as required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #145\n",
    "An international delivery company hosts a delivery management system on AWS. Drivers use the system to upload confirmation of delivery. Confirmation includes the recipient’s signature or a photo of the package with the recipient. The driver’s handheld device uploads signatures and photos through FTP to a single Amazon EC2 instance. Each handheld device saves a file in a directory based on the signed-in user, and the file name matches the delivery number. The EC2 instance then adds metadata to the file after querying a central database to pull delivery information. The file is then placed in Amazon S3 for archiving.\n",
    "\n",
    "As the company expands, drivers report that the system is rejecting connections. The FTP server is having problems because of dropped connections and memory issues in response to these problems, a system engineer schedules a cron task to reboot the EC2 instance every 30 minutes. The billing team reports that files are not always in the archive and that the central system is not always updated.\n",
    "\n",
    "A solutions architect needs to design a solution that maximizes scalability to ensure that the archive always receives the files and that systems are always updated. The handheld devices cannot be modified, so the company cannot deploy a new application.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Create an AMI of the existing EC2 instance. Create an Auto Scaling group of EC2 instances behind an Application Load Balancer. Configure the Auto Scaling group to have a minimum of three instances.\n",
    "\n",
    "B. Use AWS Transfer Family to create an FTP server that places the files in Amazon Elastic File System (Amazon EFS). Mount the EFS volume to the existing EC2 instance. Point the EC2 instance to the new path for file processing.\n",
    "\n",
    "C. Use AWS Transfer Family to create an FTP server that places the files in Amazon S3. Use an S3 event notification through Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function. Configure the Lambda function to add the metadata and update the delivery system.\n",
    "\n",
    "D. Update the handheld devices to place the files directly in Amazon S3. Use an S3 event notification through Amazon Simple Queue Service (Amazon SQS) to invoke an AWS Lambda function. Configure the Lambda function to add the metadata and update the delivery system.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to re-architect and replace FTP with aws service\n",
    "    - Use AWS Transfer Family to create an FTP server that places the files in Amazon S3\n",
    "    - Use an S3 event notification through Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function\n",
    "    - Configure the Lambda function to add the metadata and update the delivery system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #146\n",
    "A company is running an application in the AWS Cloud. The application runs on containers m an Amazon Elastic Container Service (Amazon ECS) cluster. The ECS tasks use the Fargate launch type. The application's data is relational and is stored in Amazon Aurora MySQL. To meet regulatory requirements, the application must be able to recover to a separate AWS Region in the event of an application failure. In case of a failure, no data can be lost.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST amount of operational overhead?\n",
    "\n",
    "A. Provision an Aurora Replica in a different Region.\n",
    "\n",
    "B. Set up AWS DataSync for continuous replication of the data to a different Region.\n",
    "\n",
    "C. Set up AWS Database Migration Service (AWS DMS) to perform a continuous replication of the data to a different Region.\n",
    "\n",
    "D. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule a snapshot every 5 minutes.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to config fail over for relational database\n",
    "    - Provision an Aurora Replica in a different Region\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #147\n",
    "A financial services company receives a regular data feed from its credit card servicing partner. Approximately 5,000 records are sent every 15 minutes in plaintext, delivered over HTTPS directly into an Amazon S3 bucket with server-side encryption. This feed contains sensitive credit card primary account number (PAN) data. The company needs to automatically mask the PAN before sending the data to another S3 bucket for additional internal processing. The company also needs to remove and merge specific fields, and then transform the record into JSON format. Additionally, extra feeds are likely to be added in the future, so any design needs to be easily expandable.\n",
    "\n",
    "Which solutions will meet these requirements?\n",
    "\n",
    "A. Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Invoke another Lambda function when new messages arrive in the SQS queue to process the records, writing the results to a temporary location in Amazon S3. Invoke a final Lambda function once the SQS queue is empty to transform the records into JSON format and send the results to another S3 bucket for internal processing.\n",
    "\n",
    "B. Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Configure an AWS Fargate container application to automatically scale to a single instance when the SQS queue contains messages. Have the application process each record, and transform the record into JSON format. When the queue is empty, send the results to another S3 bucket for internal processing and scale down the AWS Fargate instance.\n",
    "\n",
    "C. Create an AWS Glue crawler and custom classifier based on the data feed formats and build a table definition to match. Invoke an AWS Lambda function on file delivery to start an AWS Glue ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, have the ETL job send the results to another S3 bucket for internal processing.\n",
    "\n",
    "D. Create an AWS Glue crawler and custom classifier based upon the data feed formats and build a table definition to match. Perform an Amazon Athena query on file delivery to start an Amazon EMR ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, send the results to another S3 bucket for internal processing and scale down the EMR cluster.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about process data from s3 bucket\n",
    "    - Extract Data from S3 + mask + Send to another S3 + Transform/Process + Load into S3. All these are ETL, ELT tasks which should ring Glue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #148\n",
    "\n",
    "A company wants to use AWS to create a business continuity solution in case the company's main on-premises application fails. The application runs on physical servers that also run other applications. The on-premises application that the company is planning to migrate uses a MySQL database as a data store. All the company's on-premises applications use operating systems that are compatible with Amazon EC2.\n",
    "\n",
    "Which solution will achieve the company's goal with the LEAST operational overhead?\n",
    "\n",
    "A. Install the AWS Replication Agent on the source servers, including the MySQL servers. Set up replication for all servers. Launch test instances for regular drills. Cut over to the test instances to fail over the workload in the case of a failure event.\n",
    "\n",
    "B. Install the AWS Replication Agent on the source servers, including the MySQL servers. Initialize AWS Elastic Disaster Recovery in the target AWS Region. Define the launch settings. Frequently perform failover and fallback from the most recent point in time.\n",
    "\n",
    "C. Create AWS Database Migration Service (AWS DMS) replication servers and a target Amazon Aurora MySQL DB cluster to host the database. Create a DMS replication task to copy the existing data to the target DB cluster. Create a local AWS Schema Conversion Tool (AWS SCT) change data capture (CDC) task to keep the data synchronized. Install the rest of the software on EC2 instances by starting with a compatible base AMI.\n",
    "\n",
    "D. Deploy an AWS Storage Gateway Volume Gateway on premises. Mount volumes on all on-premises servers. Install the application and the MySQL database on the new volumes. Take regular snapshots. Install all the software on EC2 Instances by starting with a compatible base AMI. Launch a Volume Gateway on an EC2 instance. Restore the volumes from the latest snapshot. Mount the new volumes on the EC2 instances in the case of a failure event.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- how to design a system which will handle request when on-premises application fail\n",
    "    - Install the AWS Replication Agent on the source servers, including the MySQL servers\n",
    "    - Initialize AWS Elastic Disaster Recovery in the target AWS Region. Define the launch settings. Frequently perform failover and fallback from the most recent point in time\n",
    "    - The company is looking for a disaster recovery solution and not a full migration to cloud. In my view the answer should use Elastic Disaster Recovery and not DMS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #149\n",
    "A company is subject to regulatory audits of its financial information. External auditors who use a single AWS account need access to the company's AWS account. A solutions architect must provide the auditors with secure, read-only access to the company's AWS account. The solution must comply with AWS security best practices.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. In the company's AWS account, create resource policies for all resources in the account to grant access to the auditors' AWS account. Assign a unique external ID to the resource policy.\n",
    "\n",
    "B. In the company's AWS account, create an IAM role that trusts the auditors' AWS account. Create an IAM policy that has the required permissions. Attach the policy to the role. Assign a unique external ID to the role's trust policy.\n",
    "\n",
    "C. In the company's AWS account, create an IAM user. Attach the required IAM policies to the IAM user. Create API access keys for the IAM user. Share the access keys with the auditors.\n",
    "\n",
    "D. In the company's AWS account, create an IAM group that has the required permissions. Create an IAM user in the company's account for each auditor. Add the IAM users to the IAM group.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to design read-only access for another account (auditor account)\n",
    "    - In the company's AWS account, create an IAM role that trusts the auditors'. Create an IAM policy that has the required permissions. ttach the policy to the role\n",
    "    - Assign a unique external ID to the role's trust policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #150\n",
    "A company has a latency-sensitive trading platform that uses Amazon DynamoDB as a storage backend. The company configured the DynamoDB table to use on-demand capacity mode. A solutions architect needs to design a solution to improve the performance of the trading platform. The new solution must ensure high availability for the trading platform.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST latency?\n",
    "\n",
    "A. Create a two-node DynamoDB Accelerator (DAX) cluster. Configure an application to read and write data by using DAX.\n",
    "\n",
    "B. Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table.\n",
    "\n",
    "C. Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data directly from the DynamoDB table and to write data by using DAX.\n",
    "\n",
    "D. Create a single-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about design low-latency and high availability DynamoDb\n",
    "    - Create a three-node DynamoDB Accelerator (DAX) cluster\n",
    "    - configure an application to read data by using DAX and to write data directly to the DynamoDB table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #151\n",
    "A company has migrated an application from on premises to AWS. The application frontend is a static website that runs on two Amazon EC2 instances behind an Application Load Balancer (ALB). The application backend is a Python application that runs on three EC2 instances behind another ALB. The EC2 instances are large, general purpose On-Demand Instances that were sized to meet the on-premises specifications for peak usage of the application.\n",
    "\n",
    "The application averages hundreds of thousands of requests each month. However, the application is used mainly during lunchtime and receives minimal traffic during the rest of the day.\n",
    "\n",
    "A solutions architect needs to optimize the infrastructure cost of the application without negatively affecting the application availability.\n",
    "\n",
    "Which combination of steps will meet these requirements? (Choose two.)\n",
    "\n",
    "A. Change all the EC2 instances to compute optimized instances that have the same number of cores as the existing EC2 instances.\n",
    "\n",
    "B. Move the application frontend to a static website that is hosted on Amazon S3.\n",
    "\n",
    "C. Deploy the application frontend by using AWS Elastic Beanstalk. Use the same instance type for the nodes.\n",
    "\n",
    "D. Change all the backend EC2 instances to Spot Instances.\n",
    "\n",
    "E. Deploy the backend Python application to general purpose burstable EC2 instances that have the same number of cores as the existing EC2 instances.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B, E\n",
    "- the question is optimize cost for application (ec2 + alb)\n",
    "    - Move the application frontend to a static website that is hosted on Amazon S3\n",
    "    - Deploy the backend Python application to general purpose burstable EC2 instances that have the same number of cores as the existing EC2 instances\n",
    "    - Burstable instances let you save costs, you pay for some baseline - say 40 percent, if the instance is utilized less - credits get accumulated. So, it is good for workloads with changing CPU loads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #152\n",
    "A company is running an event ticketing platform on AWS and wants to optimize the platform's cost-effectiveness. The platform is deployed on Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 and is backed by an Amazon RDS for MySQL DB instance. The company is developing new application features to run on Amazon EKS with AWS Fargate.\n",
    "\n",
    "The platform experiences infrequent high peaks in demand. The surges in demand depend on event dates.\n",
    "\n",
    "Which solution will provide the MOST cost-effective setup for the platform?\n",
    "\n",
    "A. Purchase Standard Reserved Instances for the EC2 instances that the EKS cluster uses in its baseline load. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet predicted peak load for the year.\n",
    "\n",
    "B. Purchase Compute Savings Plans for the predicted medium load of the EKS cluster. Scale the cluster with On-Demand Capacity Reservations based on event dates for peaks. Purchase 1-year No Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale out database read replicas during peaks.\n",
    "\n",
    "C. Purchase EC2 Instance Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale up the DB instance manually during peaks.\n",
    "\n",
    "D. Purchase Compute Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale up the DB instance manually during peaks.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- what is the cost-effective set up for EKS\n",
    "    - Purchase Compute Savings Plans for the predicted medium load of the EKS cluster\n",
    "    - Scale the cluster with On-Demand Capacity Reservations based on event dates for peaks\n",
    "    - Purchase 1-year No Upfront Reserved Instances for the database to meet the predicted base load\n",
    "    - Temporarily scale out database read replicas during peaks\n",
    "    - A spot instance is not appropriate for a production server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #153\n",
    "A company has deployed an application on AWS Elastic Beanstalk. The application uses Amazon Aurora for the database layer. An Amazon CloudFront distribution serves web requests and includes the Elastic Beanstalk domain name as the origin server. The distribution is configured with an alternate domain name that visitors use when they access the application.\n",
    "\n",
    "Each week, the company takes the application out of service for routine maintenance. During the time that the application is unavailable, the company wants visitors to receive an informational message instead of a CloudFront error message.\n",
    "\n",
    "A solutions architect creates an Amazon S3 bucket as the first step in the process.\n",
    "\n",
    "Which combination of steps should the solutions architect take next to meet the requirements? (Choose three.)\n",
    "\n",
    "A. Upload static informational content to the S3 bucket.\n",
    "\n",
    "B. Create a new CloudFront distribution. Set the S3 bucket as the origin.\n",
    "\n",
    "C. Set the S3 bucket as a second origin in the original CloudFront distribution. Configure the distribution and the S3 bucket to use an origin access identity (OAI).\n",
    "\n",
    "\n",
    "D. During the weekly maintenance, edit the default cache behavior to use the S3 origin. Revert the change when the maintenance is complete.\n",
    "\n",
    "E. During the weekly maintenance, create a cache behavior for the S3 origin on the new distribution. Set the path pattern to \\ Set the precedence to 0. Delete the cache \n",
    "behavior when the maintenance is complete.\n",
    "\n",
    "F. During the weekly maintenance, configure Elastic Beanstalk to serve traffic from the S3 bucket.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, C, D\n",
    "- the question is about config custom error message for CloudFront\n",
    "    - Upload static informational content to the S3 bucket\n",
    "    - Set the S3 bucket as a second origin in the original CloudFront distribution. Configure the distribution and the S3 bucket to use an origin access identity (OAI)\n",
    "    - During the weekly maintenance, edit the default cache behavior to use the S3 origin. Revert the change when the maintenance is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #154\n",
    "A company gives users the ability to upload images from a custom application. The upload process invokes an AWS Lambda function that processes and stores the image in an Amazon S3 bucket. The application invokes the Lambda function by using a specific function version ARN.\n",
    "\n",
    "The Lambda function accepts image processing parameters by using environment variables. The company often adjusts the environment variables of the Lambda function to achieve optimal image processing output. The company tests different parameters and publishes a new function version with the updated environment variables after validating results. This update process also requires frequent changes to the custom application to invoke the new function version ARN. These changes cause interruptions for users.\n",
    "\n",
    "A solutions architect needs to simplify this process to minimize disruption to users.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Directly modify the environment variables of the published Lambda function version. Use the SLATEST version to test image processing parameters.\n",
    "\n",
    "B. Create an Amazon DynamoDB table to store the image processing parameters. Modify the Lambda function to retrieve the image processing parameters from the DynamoDB table.\n",
    "\n",
    "C. Directly code the image processing parameters within the Lambda function and remove the environment variables. Publish a new function version when the company updates the parameters.\n",
    "\n",
    "D. Create a Lambda function alias. Modify the client application to use the function alias ARN. Reconfigure the Lambda alias to point to new versions of the function when the company finishes testing.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the question is about how to manage multiple version of lambda function\n",
    "    - Create a Lambda function alias\n",
    "    - Modify the client application to use the function alias ARN\n",
    "    - Reconfigure the Lambda alias to point to new versions of the function when the company finishes testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #155\n",
    "A global media company is planning a multi-Region deployment of an application. Amazon DynamoDB global tables will back the deployment to keep the user experience consistent across the two continents where users are concentrated. Each deployment will have a public Application Load Balancer (ALB). The company manages public DNS internally. The company wants to make the application available through an apex domain.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST effort?\n",
    "\n",
    "A. Migrate public DNS to Amazon Route 53. Create CNAME records for the apex domain to point to the ALB. Use a geolocation routing policy to route traffic based on user location.\n",
    "\n",
    "B. Place a Network Load Balancer (NLB) in front of the ALMigrate public DNS to Amazon Route 53. Create a CNAME record for the apex domain to point to the NLB’s static IP address. Use a geolocation routing policy to route traffic based on user location.\n",
    "\n",
    "C. Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions. Use the accelerator’s static IP address to create a record in public DNS for the apex domain.\n",
    "\n",
    "D. Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS Regions. Configure a Lambda function to route traffic to application deployments by using the round robin method. Create CNAME records for the apex domain to point to the API's URL.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to make the application available through an apex domain\n",
    "    - Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions. Use the accelerator’s static IP address to create a record in public DNS for the apex domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #156\n",
    "A company is developing a new serverless API by using Amazon API Gateway and AWS Lambda. The company integrated the Lambda functions with API Gateway to use several shared libraries and custom classes.\n",
    "\n",
    "A solutions architect needs to simplify the deployment of the solution and optimize for code reuse.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Deploy the shared libraries and custom classes into a Docker image. Store the image in an S3 bucket. Create a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.\n",
    "\n",
    "B. Deploy the shared libraries and custom classes to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Create a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.\n",
    "\n",
    "C. Deploy the shared libraries and custom classes to a Docker container in Amazon Elastic Container Service (Amazon ECS) by using the AWS Fargate launch type. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the deployed container as a Lambda layer.\n",
    "\n",
    "D. Deploy the shared libraries, custom classes, and code for the API's Lambda functions to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Configure the API's Lambda functions to use the Docker image as the deployment package.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the question is about how to ntegrated the Lambda functions with API Gateway to use several shared libraries and custom classes\n",
    "    - Deploy the shared libraries, custom classes, and code for the API's Lambda functions to a Docker image\n",
    "    - Upload the image to Amazon Elastic Container Registry (Amazon ECR)\n",
    "    - Configure the API's Lambda functions to use the Docker image as the deployment package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #157\n",
    "A manufacturing company is building an inspection solution for its factory. The company has IP cameras at the end of each assembly line. The company has used Amazon SageMaker to train a machine learning (ML) model to identify common defects from still images.\n",
    "\n",
    "The company wants to provide local feedback to factory workers when a defect is detected. The company must be able to provide this feedback even if the factory’s internet connectivity is down. The company has a local Linux server that hosts an API that provides local feedback to the workers.\n",
    "\n",
    "How should the company deploy the ML model to meet these requirements?\n",
    "\n",
    "A. Set up an Amazon Kinesis video stream from each IP camera to AWS. Use Amazon EC2 instances to take still images of the streams. Upload the images to an Amazon S3 bucket. Deploy a SageMaker endpoint with the ML model. Invoke an AWS Lambda function to call the inference endpoint when new images are uploaded. Configure the Lambda function to call the local API when a defect is detected.\n",
    "\n",
    "B. Deploy AWS IoT Greengrass on the local server. Deploy the ML model to the Greengrass server. Create a Greengrass component to take still images from the cameras and run inference. Configure the component to call the local API when a defect is detected.\n",
    "\n",
    "C. Order an AWS Snowball device. Deploy a SageMaker endpoint the ML model and an Amazon EC2 instance on the Snowball device. Take still images from the cameras. Run inference from the EC2 instance. Configure the instance to call the local API when a defect is detected.\n",
    "\n",
    "D. Deploy Amazon Monitron devices on each IP camera. Deploy an Amazon Monitron Gateway on premises. Deploy the ML model to the Amazon Monitron devices. Use Amazon Monitron health state alarms to call the local API from an AWS Lambda function when a defect is detected.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to deploy ML model for inspection\n",
    "    - Deploy AWS IoT Greengrass on the local server.\n",
    "    - Deploy the ML model to the Greengrass server\n",
    "    - Create a Greengrass component to take still images from the cameras and run inference. Configure the component to call the local API when a defect is detected\n",
    "    - Offline operation: AWS IoT Greengrass supports offline operation by enabling devices to continue processing data even when they are disconnected from the internet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #158\n",
    "A solutions architect must create a business case for migration of a company's on-premises data center to the AWS Cloud. The solutions architect will use a configuration management database (CMDB) export of all the company's servers to create the case.\n",
    "\n",
    "Which solution will meet these requirements MOST cost-effectively?\n",
    "\n",
    "A. Use AWS Well-Architected Tool to import the CMDB data to perform an analysis and generate recommendations.\n",
    "\n",
    "B. Use Migration Evaluator to perform an analysis. Use the data import template to upload the data from the CMDB export.\n",
    "\n",
    "C. Implement resource matching rules. Use the CMDB export and the AWS Price List Bulk API to query CMDB data against AWS services in bulk.\n",
    "\n",
    "D. Use AWS Application Discovery Service to import the CMDB data to perform an analysis.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how migrate data from on-premises configuration management database (CMDB) export \n",
    "    - Use Migration Evaluator to perform an analysis. Use the data import template to upload the data from the CMDB export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #159\n",
    "A company has a website that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB is associated with an AWS WAF web ACL.\n",
    "\n",
    "The website often encounters attacks in the application layer. The attacks produce sudden and significant increases in traffic on the application server. The access logs show that each attack originates from different IP addresses. A solutions architect needs to implement a solution to mitigate these attacks.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Create an Amazon CloudWatch alarm that monitors server access. Set a threshold based on access by IP address. Configure an alarm action that adds the IP address to the web ACL’s deny list.\n",
    "\n",
    "B. Deploy AWS Shield Advanced in addition to AWS WAF. Add the ALB as a protected resource.\n",
    "\n",
    "C. Create an Amazon CloudWatch alarm that monitors user IP addresses. Set a threshold based on access by IP address. Configure the alarm to invoke an AWS Lambda function to add a deny rule in the application server’s subnet route table for any IP addresses that activate the alarm.\n",
    "\n",
    "D. Inspect access logs to find a pattern of IP addresses that launched the attacks. Use an Amazon Route 53 geolocation routing policy to deny traffic from the countries that host those IP addresses.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about what is the service to protect again attact\n",
    "    - AWS Shield Advanced is focused on protecting against DDoS attacks, while AWS WAF is focused on protecting against web exploits. However, both services can be used together to provide comprehensive protection for your applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #160\n",
    "A company has a critical application in which the data tier is deployed in a single AWS Region. The data tier uses an Amazon DynamoDB table and an Amazon Aurora MySQL DB cluster. The current Aurora MySQL engine version supports a global database. The application tier is already deployed in two Regions.\n",
    "\n",
    "Company policy states that critical applications must have application tier components and data tier components deployed across two Regions. The RTO and RPO must be no more than a few minutes each. A solutions architect must recommend a solution to make the data tier compliant with company policy.\n",
    "\n",
    "Which combination of steps will meet these requirements? (Choose two.)\n",
    "\n",
    "A. Add another Region to the Aurora MySQL DB cluster\n",
    "\n",
    "B. Add another Region to each table in the Aurora MySQL DB cluster\n",
    "\n",
    "C. Set up scheduled cross-Region backups for the DynamoDB table and the Aurora MySQL DB cluster\n",
    "\n",
    "D. Convert the existing DynamoDB table to a global table by adding another Region to its configuration\n",
    "\n",
    "E. Use Amazon Route 53 Application Recovery Controller to automate database backup and recovery to the secondary Region\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, D\n",
    "- the question is about how to design application with multiple regions\n",
    "    - RTO stands for Recovery Time Objective and is a measure of how quickly after an outage an application must be available again\n",
    "    - RPO, or Recovery Point Objective, refers to how much data loss your application can tolerate\n",
    "    - Add another Region to the Aurora MySQL DB cluster\n",
    "    - Convert the existing DynamoDB table to a global table by adding another Region to its configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #161\n",
    "A telecommunications company is running an application on AWS. The company has set up an AWS Direct Connect connection between the company's on-premises data center and AWS. The company deployed the application on Amazon EC2 instances in multiple Availability Zones behind an internal Application Load Balancer (ALB). The company's clients connect from the on-premises network by using HTTPS. The TLS terminates in the ALB. The company has multiple target groups and uses path-based routing to forward requests based on the URL path.\n",
    "\n",
    "The company is planning to deploy an on-premises firewall appliance with an allow list that is based on IP address. A solutions architect must develop a solution to allow traffic flow to AWS from the on-premises network so that the clients can continue to access the application.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Configure the existing ALB to use static IP addresses. Assign IP addresses in multiple Availability Zones to the ALB. Add the ALB IP addresses to the firewall appliance.\n",
    "\n",
    "B. Create a Network Load Balancer (NLB). Associate the NLB with one static IP addresses in multiple Availability Zones. Create an ALB-type target group for the NLB and add the existing ALAdd the NLB IP addresses to the firewall appliance. Update the clients to connect to the NLB.\n",
    "\n",
    "C. Create a Network Load Balancer (NLB). Associate the LNB with one static IP addresses in multiple Availability Zones. Add the existing target groups to the NLB. Update the clients to connect to the NLB. Delete the ALB Add the NLB IP addresses to the firewall appliance.\n",
    "\n",
    "D. Create a Gateway Load Balancer (GWLB). Assign static IP addresses to the GWLB in multiple Availability Zones. Create an ALB-type target group for the GWLB and add the existing ALB. Add the GWLB IP addresses to the firewall appliance. Update the clients to connect to the GWLB.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to config network which allow traffic from on-premises to aws\n",
    "    - Create a Network Load Balancer (NLB)\n",
    "    - Associate the NLB with one static IP addresses in multiple Availability Zones -> we need NLB to create statis ip and then attach to ALB so that we can pass through on-premises firewall\n",
    "    - Create an ALB-type target group for the NLB and add the existing ALAdd the NLB IP addresses to the firewall appliance\n",
    "    - Update the clients to connect to the NLB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #162\n",
    "A company runs an application on a fleet of Amazon EC2 instances that are in private subnets behind an internet-facing Application Load Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. An AWS WAF web ACL that contains various AWS managed rules is associated with the CloudFront distribution.\n",
    "\n",
    "The company needs a solution that will prevent internet traffic from directly accessing the ALB.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Create a new web ACL that contains the same rules that the existing web ACL contains. Associate the new web ACL with the ALB.\n",
    "\n",
    "B. Associate the existing web ACL with the ALB.\n",
    "\n",
    "C. Add a security group rule to the ALB to allow traffic from the AWS managed prefix list for CloudFront only.\n",
    "\n",
    "D. Add a security group rule to the ALB to allow only the various CloudFront IP address ranges.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the problem is about how to prevent internet traffic from directly accessing the ALB\n",
    "    - Add a security group rule to the ALB to allow traffic from the AWS managed prefix list for CloudFront only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #163\n",
    "A company is running an application that uses an Amazon ElastiCache for Redis cluster as a caching layer. A recent security audit revealed that the company has configured encryption at rest for ElastiCache. However, the company did not configure ElastiCache to use encryption in transit. Additionally, users can access the cache without authentication.\n",
    "\n",
    "A solutions architect must make changes to require user authentication and to ensure that the company is using end-to-end encryption.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Create an AUTH token. Store the token in AWS System Manager Parameter Store, as an encrypted parameter. Create a new cluster with AUTH, and configure encryption in transit. Update the application to retrieve the AUTH token from Parameter Store when necessary and to use the AUTH token for authentication.\n",
    "\n",
    "B. Create an AUTH token. Store the token in AWS Secrets Manager. Configure the existing cluster to use the AUTH token, and configure encryption in transit. Update the application to retrieve the AUTH token from Secrets Manager when necessary and to use the AUTH token for authentication.\n",
    "\n",
    "C. Create an SSL certificate. Store the certificate in AWS Secrets Manager. Create a new cluster, and configure encryption in transit. Update the application to retrieve the SSL certificate from Secrets Manager when necessary and to use the certificate for authentication.\n",
    "\n",
    "D. Create an SSL certificate. Store the certificate in AWS Systems Manager Parameter Store, as an encrypted advanced parameter. Update the existing cluster to configure encryption in transit. Update the application to retrieve the SSL certificate from Parameter Store when necessary and to use the certificate for authentication.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to config authentication and en-to-end encryption for Amazon ElastiCache for Redis cluster\n",
    "    - Create an AUTH token\n",
    "    - Store the token in AWS Secrets Manager\n",
    "    - Configure the existing cluster to use the AUTH token, and configure encryption in transit\n",
    "    - Update the application to retrieve the AUTH token from Secrets Manager when necessary and to use the AUTH token for authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #164\n",
    "\n",
    "A company is running a compute workload by using Amazon EC2 Spot Instances that are in an Auto Scaling group. The launch template uses two placement groups and a single instance type.\n",
    "\n",
    "Recently, a monitoring system reported Auto Scaling instance launch failures that correlated with longer wait times for system users. The company needs to improve the overall reliability of the workload.\n",
    "\n",
    "Which solution will meet this requirement?\n",
    "\n",
    "A. Replace the launch template with a launch configuration to use an Auto Scaling group that uses attribute-based instance type selection.\n",
    "\n",
    "B. Create a new launch template version that uses attribute-based instance type selection. Configure the Auto Scaling group to use the new launch template version.\n",
    "\n",
    "C. Update the launch template Auto Scaling group to increase the number of placement groups.\n",
    "\n",
    "D. Update the launch template to use a larger instance type.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to increase reliability of spot instance in auto scaling group\n",
    "    - Amazon EC2 Auto Scaling can select from a wide range of instance types for launching Spot Instances. This meets the Spot best practice of being flexible about instance types, which gives the Amazon EC2 Spot service a better chance of finding and allocating your required amount of compute capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #165\n",
    "A company is migrating a document processing workload to AWS. The company has updated many applications to natively use the Amazon S3 API to store, retrieve, and modify documents that a processing server generates at a rate of approximately 5 documents every second. After the document processing is finished, customers can download the documents directly from Amazon S3.\n",
    "\n",
    "During the migration, the company discovered that it could not immediately update the processing server that generates many documents to support the S3 API. The server runs on Linux and requires fast local access to the files that the server generates and modifies. When the server finishes processing, the files must be available to the public for download within 30 minutes.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST amount of effort?\n",
    "\n",
    "A. Migrate the application to an AWS Lambda function. Use the AWS SDK for Java to generate, modify, and access the files that the company stores directly in Amazon S3.\n",
    "\n",
    "B. Set up an Amazon S3 File Gateway and configure a file share that is linked to the document store. Mount the file share on an Amazon EC2 instance by using NFS. When changes occur in Amazon S3, initiate a RefreshCache API call to update the S3 File Gateway.\n",
    "\n",
    "C. Configure Amazon FSx for Lustre with an import and export policy. Link the new file system to an S3 bucket. Install the Lustre client and mount the document store to an Amazon EC2 instance by using NFS.\n",
    "\n",
    "D. Configure AWS DataSync to connect to an Amazon EC2 instance. Configure a task to synchronize the generated files to and from Amazon S3.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to config fast transfer from processing machine (Linux) to S3\n",
    "    - Set up an Amazon S3 File Gateway and configure a file share that is linked to the document store\n",
    "    - Mount the file share on an Amazon EC2 instance by using NFS\n",
    "    - Then changes occur in Amazon S3, initiate a RefreshCache API call to update the S3 File Gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #166\n",
    "A delivery company is running a serverless solution in the AWS Cloud. The solution manages user data, delivery information, and past purchase details. The solution consists of several microservices. The central user service stores sensitive data in an Amazon DynamoDB table. Several of the other microservices store a copy of parts of the sensitive data in different storage services.\n",
    "\n",
    "The company needs the ability to delete user information upon request. As soon as the central user service deletes a user, every other microservice must also delete its copy of the data immediately.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Activate DynamoDB Streams on the DynamoDB table. Create an AWS Lambda trigger for the DynamoDB stream that will post events about user deletion in an Amazon Simple Queue Service (Amazon SQS) queue. Configure each microservice to poll the queue and delete the user from the DynamoDB table.\n",
    "\n",
    "B. Set up DynamoDB event notifications on the DynamoDB table. Create an Amazon Simple Notification Service (Amazon SNS) topic as a target for the DynamoDB event notification. Configure each microservice to subscribe to the SNS topic and to delete the user from the DynamoDB table.\n",
    "\n",
    "C. Configure the central user service to post an event on a custom Amazon EventBridge event bus when the company deletes a user. Create an EventBridge rule for each microservice to match the user deletion event pattern and invoke logic in the microservice to delete the user from the DynamoDB table.\n",
    "\n",
    "D. Configure the central user service to post a message on an Amazon Simple Queue Service (Amazon SQS) queue when the company deletes a user. Configure each microservice to create an event filter on the SQS queue and to delete the user from the DynamoDB table.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the questin is about how to config such that when central user service deletes a user, every other microservice must also delete its copy of the data immediately\n",
    "    - Configure the central user service to post an event on a custom Amazon EventBridge event bus when the company deletes a user\n",
    "    - Create an EventBridge rule for each microservice to match the user deletion event pattern and invoke logic in the microservice to delete the user from the DynamoDB table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #167\n",
    "A company is running a web application in a VPC. The web application runs on a group of Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is using AWS WAF.\n",
    "\n",
    "An external customer needs to connect to the web application. The company must provide IP addresses to all external customers.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Replace the ALB with a Network Load Balancer (NLB). Assign an Elastic IP address to the NLB.\n",
    "\n",
    "B. Allocate an Elastic IP address. Assign the Elastic IP address to the ALProvide the Elastic IP address to the customer.\n",
    "\n",
    "C. Create an AWS Global Accelerator standard accelerator. Specify the ALB as the accelerator's endpoint. Provide the accelerator's IP addresses to the customer.\n",
    "\n",
    "D. Configure an Amazon CloudFront distribution. Set the ALB as the origin. Ping the distribution's DNS name to determine the distribution's public IP address. Provide the IP address to the customer.\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about generate statis api so that cliet can access our application in a VPC\n",
    "    - static IP can made below method:\n",
    "        - NLB (replace NLB from ALB)\n",
    "        - NLB + ALB\n",
    "        - global accelarator + ALB\n",
    "        - original load balancer (ex. made by EC2 + nginx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #168\n",
    "A company has a few AWS accounts for development and wants to move its production application to AWS. The company needs to enforce Amazon Elastic Block Store (Amazon EBS) encryption at rest current production accounts and future production accounts only. The company needs a solution that includes built-in blueprints and guardrails.\n",
    "\n",
    "Which combination of steps will meet these requirements? (Choose three.)\n",
    "\n",
    "A. Use AWS CloudFormation StackSets to deploy AWS Config rules on production accounts.\n",
    "\n",
    "B. Create a new AWS Control Tower landing zone in an existing developer account. Create OUs for accounts. Add production and development accounts to production and development OUs, respectively.\n",
    "\n",
    "C. Create a new AWS Control Tower landing zone in the company’s management account. Add production and development accounts to production and development OUs. respectively.\n",
    "\n",
    "D. Invite existing accounts to join the organization in AWS Organizations. Create SCPs to ensure compliance.\n",
    "\n",
    "E. Create a guardrail from the management account to detect EBS encryption.\n",
    "\n",
    "F. Create a guardrail for the production OU to detect EBS encryption.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C, D, F\n",
    "- the question is about how to manage aws accounts with EBS encryption\n",
    "    - Create a new AWS Control Tower landing zone in the company’s management account. Add production and development accounts to production and development OUs. respectively\n",
    "    - Invite existing accounts to join the organization in AWS Organizations. Create SCPs to ensure compliance\n",
    "    - Create a guardrail for the production OU to detect EBS encryption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #169\n",
    "A company is running a critical stateful web application on two Linux Amazon EC2 instances behind an Application Load Balancer (ALB) with an Amazon RDS for MySQL database. The company hosts the DNS records for the application in Amazon Route 53. A solutions architect must recommend a solution to improve the resiliency of the application.\n",
    "\n",
    "The solution must meet the following objectives:\n",
    "\n",
    "• Application tier: RPO of 2 minutes. RTO of 30 minutes\n",
    "\n",
    "• Database tier: RPO of 5 minutes. RTO of 30 minutes\n",
    "\n",
    "The company does not want to make significant changes to the existing application architecture. The company must ensure optimal latency after a failover.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Configure the EC2 instances to use AWS Elastic Disaster Recovery. Create a cross-Region read replica for the RDS DB instance. Create an ALB in a second AWS Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs. Update DNS records to point to the Global Accelerator endpoint.\n",
    "\n",
    "B. Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Configure RDS automated backups. Configure backup replication to a second AWS Region. Create an ALB in the second Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs. Update DNS records to point to the Global Accelerator endpoint.\n",
    "\n",
    "C. Create a backup plan in AWS Backup for the EC2 instances and RDS DB instance. Configure backup replication to a second AWS Region. Create an ALB in the second Region. Configure an Amazon CloudFront distribution in front of the ALB. Update DNS records to point to CloudFront.\n",
    "\n",
    "D. Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Create a cross-Region read replica for the RDS DB instance. Create an ALB in a second AWS Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs.\n",
    "\n",
    "Solution:\n",
    "- correct ansewr is A\n",
    "- improve the resiliency of the application\n",
    "    - Configure the EC2 instances to use AWS Elastic Disaster Recovery\n",
    "    -  Create a cross-Region read replica for the RDS DB instance\n",
    "    - Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs\n",
    "    - Update DNS records to point to the Global Accelerator endpoint\n",
    "    - You can use AWS DRS to recover all of your applications and databases that run on supported Windows and Linux operating system versions. This includes critical databases such as Oracle, MySQL, and SQL Server, and enterprise applications such as SAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #170\n",
    "A solutions architect wants to cost-optimize and appropriately size Amazon EC2 instances in a single AWS account. The solutions architect wants to ensure that the instances are optimized based on CPU, memory, and network metrics.\n",
    "\n",
    "Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)\n",
    "\n",
    "A. Purchase AWS Business Support or AWS Enterprise Support for the account.\n",
    "\n",
    "B. Turn on AWS Trusted Advisor and review any “Low Utilization Amazon EC2 Instances” recommendations.\n",
    "\n",
    "C. Install the Amazon CloudWatch agent and configure memory metric collection on the EC2 instances.\n",
    "\n",
    "D. Configure AWS Compute Optimizer in the AWS account to receive findings and optimization recommendations.\n",
    "\n",
    "E. Create an EC2 Instance Savings Plan for the AWS Regions, instance families, and operating systems of interest.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C, D\n",
    "- the question is about cost-optimized for ec2 instances\n",
    "    - Install the Amazon CloudWatch agent and configure memory metric collection on the EC2 instances\n",
    "    - Configure AWS Compute Optimizer in the AWS account to receive findings and optimization recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #171\n",
    "A company uses an AWS CodeCommit repository. The company must store a backup copy of the data that is in the repository in a second AWS Region.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Configure AWS Elastic Disaster Recovery to replicate the CodeCommit repository data to the second Region.\n",
    "\n",
    "B. Use AWS Backup to back up the CodeCommit repository on an hourly schedule. Create a cross-Region copy in the second Region.\n",
    "\n",
    "C. Create an Amazon EventBridge rule to invoke AWS CodeBuild when the company pushes code to the repository. Use CodeBuild to clone the repository. Create a .zip file of the content. Copy the file to an S3 bucket in the second Region.\n",
    "\n",
    "D. Create an AWS Step Functions workflow on an hourly schedule to take a snapshot of the CodeCommit repository. Configure the workflow to copy the snapshot to an S3 bucket in the second Region\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to backup code for Code Commit\n",
    "    - use event push and clone strategy\n",
    "    -  Create an Amazon EventBridge rule to invoke AWS CodeBuild when the company pushes code to the repository\n",
    "    - Use CodeBuild to clone the repository. Create a .zip file of the content\n",
    "    - Create a .zip file of the content. Copy the file to an S3 bucket in the second Region.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #172\n",
    "A company has multiple business units that each have separate accounts on AWS. Each business unit manages its own network with several VPCs that have CIDR ranges that overlap. The company’s marketing team has created a new internal application and wants to make the application accessible to all the other business units. The solution must use private IP addresses only.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Instruct each business unit to add a unique secondary CIDR range to the business unit's VPC. Peer the VPCs and use a private NAT gateway in the secondary range to route traffic to the marketing team.\n",
    "\n",
    "B. Create an Amazon EC2 instance to serve as a virtual appliance in the marketing account's VPC. Create an AWS Site-to-Site VPN connection between the marketing team and each business unit's VPC. Perform NAT where necessary.\n",
    "\n",
    "C. Create an AWS PrivateLink endpoint service to share the marketing application. Grant permission to specific AWS accounts to connect to the service. Create interface VPC endpoints in other accounts to access the application by using private IP addresses.\n",
    "\n",
    "D. Create a Network Load Balancer (NLB) in front of the marketing application in a private subnet. Create an API Gateway API. Use the Amazon API Gateway private integration to connect the API to the NLB. Activate IAM authorization for the API. Grant access to the accounts of the other business units.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- Create an AWS PrivateLink endpoint service to share the marketing application. Grant permission to specific AWS accounts to connect to the service. Create interface VPC endpoints in other accounts to access the application by using private IP addresses\n",
    "- Private link is the solution for IP Overlapping and Securely access the app between accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #173\n",
    "A company needs to audit the security posture of a newly acquired AWS account. The company’s data security team requires a notification only when an Amazon S3 bucket becomes publicly exposed. The company has already established an Amazon Simple Notification Service (Amazon SNS) topic that has the data security team's email address subscribed.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Create an S3 event notification on all S3 buckets for the isPublic event. Select the SNS topic as the target for the event notifications.\n",
    "\n",
    "B. Create an analyzer in AWS Identity and Access Management Access Analyzer. Create an Amazon EventBridge rule for the event type “Access Analyzer Finding” with a filter for “isPublic: true.” Select the SNS topic as the EventBridge rule target.\n",
    "\n",
    "C. Create an Amazon EventBridge rule for the event type “Bucket-Level API Call via CloudTrail” with a filter for “PutBucketPolicy.” Select the SNS topic as the EventBridge rule target.\n",
    "\n",
    "D. Activate AWS Config and add the cloudtrail-s3-dataevents-enabled rule. Create an Amazon EventBridge rule for the event type “Config Rules Re-evaluation Status” with a filter for “NON_COMPLIANT.” Select the SNS topic as the EventBridge rule target.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to config alert when s3 bucket is public\n",
    "    - A. No, because Amazon S3 can NOT currently publish notifications for isPublic events\n",
    "    - Yes, because IAM Access Analyzer for S3 alerts you to S3 buckets that are configured to allow access to anyone on the internet or other AWS accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #174\n",
    "A solutions architect needs to assess a newly acquired company’s portfolio of applications and databases. The solutions architect must create a business case to migrate the portfolio to AWS. The newly acquired company runs applications in an on-premises data center. The data center is not well documented. The solutions architect cannot immediately determine how many applications and databases exist. Traffic for the applications is variable. Some applications are batch processes that run at the end of each month.\n",
    "\n",
    "The solutions architect must gain a better understanding of the portfolio before a migration to AWS can begin.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Use AWS Server Migration Service (AWS SMS) and AWS Database Migration Service (AWS DMS) to evaluate migration. Use AWS Service Catalog to understand application and database dependencies.\n",
    "\n",
    "B. Use AWS Application Migration Service. Run agents on the on-premises infrastructure. Manage the agents by using AWS Migration Hub. Use AWS Storage Gateway to assess local storage needs and database dependencies.\n",
    "\n",
    "C. Use Migration Evaluator to generate a list of servers. Build a report for a business case. Use AWS Migration Hub to view the portfolio. Use AWS Application Discovery Service to gain an understanding of application dependencies.\n",
    "\n",
    "D. Use AWS Control Tower in the destination account to generate an application portfolio. Use AWS Server Migration Service (AWS SMS) to generate deeper reports and a business case. Use a landing zone for core accounts and resources.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to gain better understand of on-premises application\n",
    "    - Use Migration Evaluator to generate a list of servers. Build a report for a business case\n",
    "    - Use AWS Migration Hub to view the portfolio. Use AWS Application Discovery Service to gain an understanding of application dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #175\n",
    "A company has an application that runs as a ReplicaSet of multiple pods in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster has nodes in multiple Availability Zones. The application generates many small files that must be accessible across all running instances of the application. The company needs to back up the files and retain the backups for 1 year.\n",
    "\n",
    "Which solution will meet these requirements while providing the FASTEST storage performance?\n",
    "\n",
    "A. Create an Amazon Elastic File System (Amazon EFS) file system and a mount target for each subnet that contains nodes in the EKS cluster. Configure the ReplicaSet to mount the file system. Direct the application to store files in the file system. Configure AWS Backup to back up and retain copies of the data for 1 year.\n",
    "\n",
    "B. Create an Amazon Elastic Block Store (Amazon EBS) volume. Enable the EBS Multi-Attach feature. Configure the ReplicaSet to mount the EBS volume. Direct the application to store files in the EBS volume. Configure AWS Backup to back up and retain copies of the data for 1 year.\n",
    "\n",
    "C. Create an Amazon S3 bucket. Configure the ReplicaSet to mount the S3 bucket. Direct the application to store files in the S3 bucket. Configure S3 Versioning to retain copies of the data. Configure an S3 Lifecycle policy to delete objects after 1 year.\n",
    "\n",
    "D. Configure the ReplicaSet to use the storage available on each of the running application pods to store the files locally. Use a third-party tool to back up the EKS cluster for 1 year.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to config storage for application running on EKS which have fasted storage performace\n",
    "    - EFS = Fastest storage performance compare to S3/EBS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #176\n",
    "A company runs a customer service center that accepts calls and automatically sends all customers a managed, interactive, two-way experience survey by text message. The applications that support the customer service center run on machines that the company hosts in an on-premises data center. The hardware that the company uses is old, and the company is experiencing downtime with the system. The company wants to migrate the system to AWS to improve reliability.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST ongoing operational overhead?\n",
    "\n",
    "A. Use Amazon Connect to replace the old call center hardware. Use Amazon Pinpoint to send text message surveys to customers.\n",
    "\n",
    "B. Use Amazon Connect to replace the old call center hardware. Use Amazon Simple Notification Service (Amazon SNS) to send text message surveys to customers.\n",
    "\n",
    "C. Migrate the call center software to Amazon EC2 instances that are in an Auto Scaling group. Use the EC2 instances to send text message surveys to customers.\n",
    "\n",
    "D. Use Amazon Pinpoint to replace the old call center hardware and to send text message surveys to customers.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about what is solution for old call center\n",
    "    - Use Amazon Connect to replace the old call center hardware\n",
    "    - Use Amazon Pinpoint to send text message surveys to customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #177\n",
    "A company is building a call center by using Amazon Connect. The company’s operations team is defining a disaster recovery (DR) strategy across AWS Regions. The contact center has dozens of contact flows, hundreds of users, and dozens of claimed phone numbers.\n",
    "\n",
    "Which solution will provide DR with the LOWEST RTO?\n",
    "\n",
    "A. Create an AWS Lambda function to check the availability of the Amazon Connect instance and to send a notification to the operations team in case of unavailability. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. After notification, instruct the operations team to use the AWS Management Console to provision a new Amazon Connect instance in a second Region. Deploy the contact flows, users, and claimed phone numbers by using an AWS CloudFormation template.\n",
    "\n",
    "B. Provision a new Amazon Connect instance with all existing users in a second Region. Create an AWS Lambda function to check the availability of the Amazon Connect instance. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. In the event of an issue, configure the Lambda function to deploy an AWS CloudFormation template that provisions contact flows and claimed numbers in the second Region.\n",
    "\n",
    "C. Provision a new Amazon Connect instance with all existing contact flows and claimed phone numbers in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions all users. Configure the alarm to invoke the Lambda function.\n",
    "\n",
    "D. Provision a new Amazon Connect instance with all existing users and contact flows in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions claimed phone numbers. Configure the alarm to invoke the Lambda function.\n",
    "\n",
    "Solution:\n",
    "- correct answer is D\n",
    "- the question is about how to design disaster recovery architect for call center which have the lowest RTO (Recovery Time Objective)\n",
    "    - Provision a new Amazon Connect instance with all existing users and contact flows in a second Region\n",
    "    - Create an Amazon Route 53 health check for the URL of the Amazon Connect instance\n",
    "    - Create an Amazon CloudWatch alarm for failed health checks\n",
    "    - Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions claimed phone numbers. Configure the alarm to invoke the Lambda function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #178\n",
    "A company runs an application on AWS. The company curates data from several different sources. The company uses proprietary algorithms to perform data transformations and aggregations. After the company performs ETL processes, the company stores the results in Amazon Redshift tables. The company sells this data to other companies. The company downloads the data as files from the Amazon Redshift tables and transmits the files to several data customers by using FTP. The number of data customers has grown significantly. Management of the data customers has become difficult.\n",
    "\n",
    "The company will use AWS Data Exchange to create a data product that the company can use to share data with customers. The company wants to confirm the identities of the customers before the company shares data. The customers also need access to the most recent data when the company publishes the data.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Use AWS Data Exchange for APIs to share data with customers. Configure subscription verification. In the AWS account of the company that produces the data, create an Amazon API Gateway Data API service integration with Amazon Redshift. Require the data customers to subscribe to the data product.\n",
    "\n",
    "B. In the AWS account of the company that produces the data, create an AWS Data Exchange datashare by connecting AWS Data Exchange to the Redshift cluster. Configure subscription verification. Require the data customers to subscribe to the data product.\n",
    "\n",
    "C. Download the data from the Amazon Redshift tables to an Amazon S3 bucket periodically. Use AWS Data Exchange for S3 to share data with customers. Configure subscription verification. Require the data customers to subscribe to the data product.\n",
    "\n",
    "D. Publish the Amazon Redshift data to an Open Data on AWS Data Exchange. Require the customers to subscribe to the data product in AWS Data Exchange. In the AWS account of the company that produces the data, attach IAM resource-based policies to the Amazon Redshift tables to allow access only to verified AWS accounts.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to use Data Exchange to share data on Redshift cluster (include authenticate problem)\n",
    "    - In the AWS account of the company that produces the data, create an AWS Data Exchange datashare by connecting AWS Data Exchange to the Redshift cluster\n",
    "    - Configure subscription verification. Require the data customers to subscribe to the data product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #179\n",
    "A solutions architect is designing a solution to process events. The solution must have the ability to scale in and out based on the number of events that the solution receives. If a processing error occurs, the event must move into a separate queue for review.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Send event details to an Amazon Simple Notification Service (Amazon SNS) topic. Configure an AWS Lambda function as a subscriber to the SNS topic to process the events. Add an on-failure destination to the function. Set an Amazon Simple Queue Service (Amazon SQS) queue as the target.\n",
    "\n",
    "B. Publish events to an Amazon Simple Queue Service (Amazon SQS) queue. Create an Amazon EC2 Auto Scaling group. Configure the Auto Scaling group to scale in and out based on the ApproximateAgeOfOldestMessage metric of the queue. Configure the application to write failed messages to a dead-letter queue.\n",
    "\n",
    "C. Write events to an Amazon DynamoDB table. Configure a DynamoDB stream for the table. Configure the stream to invoke an AWS Lambda function. Configure the Lambda function to process the events.\n",
    "\n",
    "D. Publish events to an Amazon EventBndge event bus. Create and run an application on an Amazon EC2 instance with an Auto Scaling group that is behind an Application Load Balancer (ALB). Set the ALB as the event bus target. Configure the event bus to retry events. Write messages to a dead-letter queue if the application cannot process the messages.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to design architect for events drivens which can handle error, also can scale in and scale out\n",
    "    - Configuring scaling based on the age of the oldest message is nowhere near as good as scaling based on size of the Queue for this use case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #180\n",
    "A company runs a processing engine in the AWS Cloud. The engine processes environmental data from logistics centers to calculate a sustainability index. The company has millions of devices in logistics centers that are spread across Europe. The devices send information to the processing engine through a RESTful API.\n",
    "\n",
    "The API experiences unpredictable bursts of traffic. The company must implement a solution to process all data that the devices send to the processing engine. Data loss is unacceptable.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Create an Application Load Balancer (ALB) for the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create a listener and a target group for the ALB Add the SQS queue as the target. Use a container that runs in Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type to process messages in the queue.\n",
    "\n",
    "B. Create an Amazon API Gateway HTTP API that implements the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create an API Gateway service integration with the SQS queue. Create an AWS Lambda function to process messages in the SQS queue.\n",
    "\n",
    "C. Create an Amazon API Gateway REST API that implements the RESTful API. Create a fleet of Amazon EC2 instances in an Auto Scaling group. Create an API Gateway Auto Scaling group proxy integration. Use the EC2 instances to process incoming data.\n",
    "\n",
    "D. Create an Amazon CloudFront distribution for the RESTful API. Create a data stream in Amazon Kinesis Data Streams. Set the data stream as the origin for the distribution. Create an AWS Lambda function to consume and process data in the data stream.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to design rest application which do not alow lost data\n",
    "    - do not allow lost data -> using queue\n",
    "    - Option A is incorrect because Application Load Balancer (ALB) can't directly target an Amazon SQS queue.\n",
    "    - the correct solution is B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #181\n",
    "A company is designing its network configuration in the AWS Cloud. The company uses AWS Organizations to manage a multi-account setup. The company has three OUs. Each OU contains more than 100 AWS accounts. Each account has a single VPC, and all the VPCs in each OU are in the same AWS Region.\n",
    "\n",
    "The CIDR ranges for all the AWS accounts do not overlap. The company needs to implement a solution in which VPCs in the same OU can communicate with each other but cannot communicate with VPCs in other OUs.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Create an AWS CloudFormation stack set that establishes VPC peering between accounts in each OU. Provision the stack set in each OU.\n",
    "\n",
    "B. In each OU, create a dedicated networking account that has a single VPC. Share this VPC with all the other accounts in the OU by using AWS Resource Access Manager (AWS RAM). Create a VPC peering connection between the networking account and each account in the OU.\n",
    "\n",
    "C. Provision a transit gateway in an account in each OU. Share the transit gateway across the organization by using AWS Resource Access Manager (AWS RAM). Create transit gateway VPC attachments for each VPC.\n",
    "\n",
    "D. In each OU, create a dedicated networking account that has a single VPC. Establish a VPN connection between the networking account and the other accounts in the OU. Use third-party routing software to route transitive traffic between the VPCs.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to config network for large orgnaization which is in the same region\n",
    "    - Transit Gateway is a managed service from AWS that acts as a hub interconnecting VPCs and VPN connections within a single region. It allows you to build more complex networks without the need for VPC peering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #182\n",
    "A company is migrating an application to AWS. It wants to use fully managed services as much as possible during the migration. The company needs to store large important documents within the application with the following requirements:\n",
    "\n",
    "1. The data must be highly durable and available\n",
    "2. The data must always be encrypted at rest and in transit\n",
    "3. The encryption key must be managed by the company and rotated periodically\n",
    "\n",
    "Which of the following solutions should the solutions architect recommend?\n",
    "\n",
    "A. Deploy the storage gateway to AWS in file gateway mode. Use Amazon EBS volume encryption using an AWS KMS key to encrypt the storage gateway volumes.\n",
    "\n",
    "B. Use Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforce server-side encryption and AWS KMS for object encryption.\n",
    "\n",
    "C. Use Amazon DynamoDB with SSL to connect to DynamoDB. Use an AWS KMS key to encrypt DynamoDB objects at rest.\n",
    "\n",
    "D. Deploy instances with Amazon EBS volumes attached to store this data. Use EBS volume encryption using an AWS KMS key to encrypt the data.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about storage solution which is encrypted\n",
    "    - Use Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforce server-side encryption and AWS KMS for object encryption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #183\n",
    "A company’s public API runs as tasks on Amazon Elastic Container Service (Amazon ECS). The tasks run on AWS Fargate behind an Application Load Balancer (ALB) and are configured with Service Auto Scaling for the tasks based on CPU utilization. This service has been running well for several months.\n",
    "\n",
    "Recently, API performance slowed down and made the application unusable. The company discovered that a significant number of SQL injection attacks had occurred against the API and that the API service had scaled to its maximum amount.\n",
    "\n",
    "A solutions architect needs to implement a solution that prevents SQL injection attacks from reaching the ECS API service. The solution must allow legitimate traffic through and must maximize operational efficiency.\n",
    "\n",
    "Which solution meets these requirements?\n",
    "\n",
    "A. Create a new AWS WAF web ACL to monitor the HTTP requests and HTTPS requests that are forwarded to the ALB in front of the ECS tasks.\n",
    "\n",
    "B. Create a new AWS WAF Bot Control implementation. Add a rule in the AWS WAF Bot Control managed rule group to monitor traffic and allow only legitimate traffic to the ALB in front of the ECS tasks.\n",
    "\n",
    "C. Create a new AWS WAF web ACL. Add a new rule that blocks requests that match the SQL database rule group. Set the web ACL to allow all other traffic that does not match those rules. Attach the web ACL to the ALB in front of the ECS tasks.\n",
    "\n",
    "D. Create a new AWS WAF web ACL. Create a new empty IP set in AWS WAF. Add a new rule to the web ACL to block requests that originate from IP addresses in the new IP set. Create an AWS Lambda function that scrapes the API logs for IP addresses that send SQL injection attacks, and add those IP addresses to the IP set. Attach the web ACL to the ALB in front of the ECS tasks.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is how to protect again SQL injection\n",
    "    - Create a new AWS WAF web ACL. Add a new rule that blocks requests that match the SQL database rule group. Set the web ACL to allow all other traffic that does not match those rules\n",
    "    - Attach the web ACL to the ALB in front of the ECS tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #184\n",
    "An environmental company is deploying sensors in major cities throughout a country to measure air quality. The sensors connect to AWS IoT Core to ingest timeseries data readings. The company stores the data in Amazon DynamoDB.\n",
    "\n",
    "For business continuity, the company must have the ability to ingest and store data in two AWS Regions.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Create an Amazon Route 53 alias failover routing policy with values for AWS IoT Core data endpoints in both Regions Migrate data to Amazon Aurora global tables.\n",
    "\n",
    "B. Create a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 latency-based routing policy. Use AWS IoT Core data endpoints in both Regions as values. Migrate the data to Amazon MemoryDB for Redis and configure cross-Region replication.\n",
    "\n",
    "C. Create a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 health check that evaluates domain configuration health. Create a failover routing policy with values for the domain name from the AWS IoT Core domain configurations. Update the DynamoDB table to a global table.\n",
    "\n",
    "D. Create an Amazon Route 53 latency-based routing policy. Use AWS IoT Core data endpoints in both Regions as values. Configure DynamoDB streams and cross-Region data replication.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about the design which allow ingest and store data in two regions\n",
    "    - Create a domain configuration for AWS IoT Core in each Region\n",
    "    - Create an Amazon Route 53 health check that evaluates domain configuration health. Create a failover routing policy with values for the domain name from the AWS IoT Core domain configurations\n",
    "    - Update the DynamoDB table to a global table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #185\n",
    "A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company's finance team has a data processing application that uses AWS Lambda and Amazon DynamoDB. The company's marketing team wants to access the data that is stored in the DynamoDB table.\n",
    "\n",
    "The DynamoDB table contains confidential data. The marketing team can have access to only specific attributes of data in the DynamoDB table. The finance team and the marketing team have separate AWS accounts.\n",
    "\n",
    "What should a solutions architect do to provide the marketing team with the appropriate access to the DynamoDB table?\n",
    "\n",
    "A. Create an SCP to grant the marketing team's AWS account access to the specific attributes of the DynamoDB table. Attach the SCP to the OU of the finance team.\n",
    "\n",
    "B. Create an IAM role in the finance team's account by using IAM policy conditions for specific DynamoDB attributes (fine-grained access control). Establish trust with the marketing team's account. In the marketing team's account, create an IAM role that has permissions to assume the IAM role in the finance team's account.\n",
    "\n",
    "C. Create a resource-based IAM policy that includes conditions for specific DynamoDB attributes (fine-grained access control). Attach the policy to the DynamoDB table. In the marketing team's account, create an IAM role that has permissions to access the DynamoDB table in the finance team's account.\n",
    "\n",
    "D. Create an IAM role in the finance team's account to access the DynamoDB table. Use an IAM permissions boundary to limit the access to the specific attributes. In the marketing team's account, create an IAM role that has permissions to assume the IAM role in the finance team's account.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to config account to DyanmoDB in fiance account from marketing account\n",
    "    - Create an IAM role in the finance team's account by using IAM policy conditions for specific DynamoDB attributes (fine-grained access control). Establish trust with the marketing team's account\n",
    "    - In the marketing team's account, create an IAM role that has permissions to assume the IAM role in the finance team's account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #186\n",
    "A solutions architect is creating an application that stores objects in an Amazon S3 bucket. The solutions architect must deploy the application in two AWS Regions that will be used simultaneously. The objects in the two S3 buckets must remain synchronized with each other.\n",
    "\n",
    "Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)\n",
    "\n",
    "A. Create an S3 Multi-Region Access Point Change the application to refer to the Multi-Region Access Point\n",
    "\n",
    "B. Configure two-way S3 Cross-Region Replication (CRR) between the two S3 buckets\n",
    "\n",
    "C. Modify the application to store objects in each S3 bucket\n",
    "\n",
    "D. Create an S3 Lifecycle rule for each S3 bucket to copy objects from one S3 bucket to the other S3 bucket\n",
    "\n",
    "E. Enable S3 Versioning for each S3 bucket\n",
    "\n",
    "F. Configure an event notification for each S3 bucket to invoke an AWS Lambda function to copy objects from one S3 bucket to the other S3 bucket\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, B, E\n",
    "- the question is about how to config s3 buckets in two regions and used simultaneously\n",
    "    - Create an S3 Multi-Region Access Point Change the application to refer to the Multi-Region Access Point\n",
    "    - Configure two-way S3 Cross-Region Replication (CRR) between the two S3 buckets\n",
    "    - Versioning must be enabled for Replication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #187\n",
    "A company has an IoT platform that runs in an on-premises environment. The platform consists of a server that connects to IoT devices by using the MQTT protocol. The platform collects telemetry data from the devices at least once every 5 minutes. The platform also stores device metadata in a MongoDB cluster.\n",
    "\n",
    "An application that is installed on an on-premises machine runs periodic jobs to aggregate and transform the telemetry and device metadata. The application creates reports that users view by using another web application that runs on the same on-premises machine. The periodic jobs take 120-600 seconds to run. However, the web application is always running.\n",
    "\n",
    "The company is moving the platform to AWS and must reduce the operational overhead of the stack.\n",
    "\n",
    "Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)\n",
    "\n",
    "A. Use AWS Lambda functions to connect to the IoT devices\n",
    "\n",
    "B. Configure the IoT devices to publish to AWS IoT Core\n",
    "\n",
    "C. Write the metadata to a self-managed MongoDB database on an Amazon EC2 instance\n",
    "\n",
    "D. Write the metadata to Amazon DocumentDB (with MongoDB compatibility)\n",
    "\n",
    "E. Use AWS Step Functions state machines with AWS Lambda tasks to prepare the reports and to write the reports to Amazon S3. Use Amazon CloudFront with an S3 origin to serve the reports\n",
    "\n",
    "F. Use an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 instances to prepare the reports. Use an ingress controller in the EKS cluster to serve the reports\n",
    "\n",
    "Solution:\n",
    "- correct answer is B, D, E\n",
    "- the question is about move iot application to aws\n",
    "    - Configure the IoT devices to publish to AWS IoT Core\n",
    "    - Write the metadata to Amazon DocumentDB (with MongoDB compatibility)\n",
    "    - Use AWS Step Functions state machines with AWS Lambda tasks to prepare the reports and to write the reports to Amazon S3. Use Amazon CloudFront with an S3 origin to serve the reports (for short time job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #188\n",
    "A global manufacturing company plans to migrate the majority of its applications to AWS. However, the company is concerned about applications that need to remain within a specific country or in the company's central on-premises data center because of data regulatory requirements or requirements for latency of single-digit milliseconds. The company also is concerned about the applications that it hosts in some of its factory sites, where limited network infrastructure exists.\n",
    "\n",
    "The company wants a consistent developer experience so that its developers can build applications once and deploy on premises, in the cloud, or in a hybrid architecture. The developers must be able to use the same tools, APIs, and services that are familiar to them.\n",
    "\n",
    "Which solution will provide a consistent hybrid experience to meet these requirements?\n",
    "\n",
    "A. Migrate all applications to the closest AWS Region that is compliant. Set up an AWS Direct Connect connection between the central on-premises data center and AWS. Deploy a Direct Connect gateway.\n",
    "\n",
    "B. Use AWS Snowball Edge Storage Optimized devices for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds. Retain the devices on premises. Deploy AWS Wavelength to host the workloads in the factory sites.\n",
    "\n",
    "C. Install AWS Outposts for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds. Use AWS Snowball Edge Compute Optimized devices to host the workloads in the factory sites.\n",
    "\n",
    "D. Migrate the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds to an AWS Local Zone. Deploy AWS Wavelength to host the workloads in the factory sites.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about what is the solution for hybrid experience (cloud + on-premiese) where application is on specific country\n",
    "    - Install AWS Outposts for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds. Use AWS Snowball Edge Compute Optimized devices to host the workloads in the factory sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #189\n",
    "A company is updating an application that customers use to make online orders. The number of attacks on the application by bad actors has increased recently.\n",
    "\n",
    "The company will host the updated application on an Amazon Elastic Container Service (Amazon ECS) cluster. The company will use Amazon DynamoDB to store application data. A public Application Load Balancer (ALB) will provide end users with access to the application. The company must prevent attacks and ensure business continuity with minimal service interruptions during an ongoing attack.\n",
    "\n",
    "Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)\n",
    "\n",
    "A. Create an Amazon CloudFront distribution with the ALB as the origin. Add a custom header and random value on the CloudFront domain. Configure the ALB to conditionally forward traffic if the header and value match.\n",
    "\n",
    "B. Deploy the application in two AWS Regions. Configure Amazon Route 53 to route to both Regions with equal weight.\n",
    "\n",
    "C. Configure auto scaling for Amazon ECS tasks Create a DynamoDB Accelerator (DAX) cluster.\n",
    "\n",
    "D. Configure Amazon ElastiCache to reduce overhead on DynamoDB.\n",
    "\n",
    "E. Deploy an AWS WAF web ACL that includes an appropriate rule group. Associate the web ACL with the Amazon CloudFront distribution.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, E\n",
    "- the question is about the architect which protect again increasing attact\n",
    "    - Create an Amazon CloudFront distribution with the ALB as the origin. Add a custom header and random value on the CloudFront domain. Configure the ALB to conditionally forward traffic if the header and value match\n",
    "    - Deploy an AWS WAF web ACL that includes an appropriate rule group. Associate the web ACL with the Amazon CloudFront distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #190\n",
    "A company runs a web application on AWS. The web application delivers static content from an Amazon S3 bucket that is behind an Amazon CloudFront distribution. The application serves dynamic content by using an Application Load Balancer (ALB) that distributes requests to a fleet of Amazon EC2 instances in Auto Scaling groups. The application uses a domain name setup in Amazon Route 53.\n",
    "\n",
    "Some users reported occasional issues when the users attempted to access the website during peak hours. An operations team found that the ALB sometimes returned HTTP 503 Service Unavailable errors. The company wants to display a custom error message page when these errors occur. The page should be displayed immediately for this error code.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Set up a Route 53 failover routing policy. Configure a health check to determine the status of the ALB endpoint and to fail over to the failover S3 bucket endpoint.\n",
    "\n",
    "B. Create a second CloudFront distribution and an S3 static website to host the custom error page. Set up a Route 53 failover routing policy. Use an active-passive configuration between the two distributions.\n",
    "\n",
    "C. Create a CloudFront origin group that has two origins. Set the ALB endpoint as the primary origin. For the secondary origin, set an S3 bucket that is configured to host a static website Set up origin failover for the CloudFront distribution. Update the S3 static website to incorporate the custom error page.\n",
    "\n",
    "D. Create a CloudFront function that validates each HTTP response code that the ALB returns. Create an S3 static website in an S3 bucket. Upload the custom error page to the S3 bucket as a failover. Update the function to read the S3 bucket and to serve the error page to the end users.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to config which application display custom error instad of default ALB error\n",
    "    - Create a CloudFront origin group that has two origins\n",
    "    - Set the ALB endpoint as the primary origin. For the secondary origin, set an S3 bucket that is configured to host a static website Set up origin failover for the CloudFront distribution\n",
    "    - Update the S3 static website to incorporate the custom error page\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #191\n",
    "A company is planning to migrate an application to AWS. The application runs as a Docker container and uses an NFS version 4 file share.\n",
    "\n",
    "A solutions architect must design a secure and scalable containerized solution that does not require provisioning or management of the underlying infrastructure.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon Elastic File System (Amazon EFS) for shared storage. Reference the EFS file system ID, container mount point, and EFS authorization IAM role in the ECS task definition.\n",
    "\n",
    "B. Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon FSx for Lustre for shared storage. Reference the FSx for Lustre file system ID, container mount point, and FSx for Lustre authorization IAM role in the ECS task definition.\n",
    "\n",
    "C. Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto scaling turned on. Use Amazon Elastic File System (Amazon EFS) for shared storage. Mount the EFS file system on the ECS container instances. Add the EFS authorization IAM role to the EC2 instance profile.\n",
    "\n",
    "D. Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto scaling turned on. Use Amazon Elastic Block Store (Amazon EBS) volumes with Multi-Attach enabled for shared storage. Attach the EBS volumes to ECS container instances. Add the EBS authorization IAM role to an EC2 instance profile.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is about how to migrate application (docer + NFS)\n",
    "    - to replace docker -> Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type\n",
    "    - NFS -> Use Amazon Elastic File System (Amazon EFS) for shared storage. Reference the EFS file system ID, container mount point, and EFS authorization IAM role in the ECS task definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #192\n",
    "A company is running an application in the AWS Cloud. The core business logic is running on a set of Amazon EC2 instances in an Auto Scaling group. An Application Load Balancer (ALB) distributes traffic to the EC2 instances. Amazon Route 53 record api.example.com is pointing to the ALB.\n",
    "\n",
    "The company's development team makes major updates to the business logic. The company has a rule that when changes are deployed, only 10% of customers can receive the new logic during a testing window. A customer must use the same version of the business logic during the testing window.\n",
    "\n",
    "How should the company deploy the updates to meet these requirements?\n",
    "\n",
    "A. Create a second ALB, and deploy the new logic to a set of EC2 instances in a new Auto Scaling group. Configure the ALB to distribute traffic to the EC2 instances. Update the Route 53 record to use weighted routing, and point the record to both of the ALBs.\n",
    "\n",
    "B. Create a second target group that is referenced by the ALDeploy the new logic to EC2 instances in this new target group. Update the ALB listener rule to use weighted target groups. Configure ALB target group stickiness.\n",
    "\n",
    "C. Create a new launch configuration for the Auto Scaling group. Specify the launch configuration to use the AutoScalingRollingUpdate policy, and set the MaxBatchSize option to 10. Replace the launch configuration on the Auto Scaling group. Deploy the changes.\n",
    "\n",
    "D. Create a second Auto Scaling group that is referenced by the ALB. Deploy the new logic on a set of EC2 instances in this new Auto Scaling group. Change the ALB routing algorithm to least outstanding requests (LOR). Configure ALB session stickiness.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to setup canary deployment\n",
    "    - B is better option considering the fact that a customer should get same business logic during testing window. This means we need session stickiness that only option B can provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #193\n",
    "A large education company recently introduced Amazon Workspaces to provide access to internal applications across multiple universities. The company is storing user profiles on an Amazon FSx for Windows File Server file system. The file system is configured with a DNS alias and is connected to a self-managed Active Directory. As more users begin to use the Workspaces, login time increases to unacceptable levels.\n",
    "\n",
    "An investigation reveals a degradation in performance of the file system. The company created the file system on HDD storage with a throughput of 16 MBps. A solutions architect must improve the performance of the file system during a defined maintenance window.\n",
    "\n",
    "What should the solutions architect do to meet these requirements with the LEAST administrative effort?\n",
    "\n",
    "A. Use AWS Backup to create a point-in-time backup of the file system. Restore the backup to a new FSx for Windows File Server file system. Select SSD as the storage type. Select 32 MBps as the throughput capacity. When the backup and restore process is completed, adjust the DNS alias accordingly. Delete the original file system.\n",
    "\n",
    "B. Disconnect users from the file system. In the Amazon FSx console, update the throughput capacity to 32 MBps. Update the storage type to SSD. Reconnect users to the file system.\n",
    "\n",
    "C. Deploy an AWS DataSync agent onto a new Amazon EC2 instance. Create a task. Configure the existing file system as the source location. Configure a new FSx for Windows File Server file system with SSD storage and 32 MBps of throughput as the target location. Schedule the task. When the task is completed, adjust the DNS alias accordingly. Delete the original file system.\n",
    "\n",
    "D. Enable shadow copies on the existing file system by using a Windows PowerShell command. Schedule the shadow copy job to create a point-in-time backup of the file system. Choose to restore previous versions. Create a new FSx for Windows File Server file system with SSD storage and 32 MBps of throughput. When the copy job is completed, adjust the DNS alias. Delete the original file system.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A\n",
    "- the question is how to imporve Amazon FSx for Windows File Server file system through put\n",
    "    - Use AWS Backup to create a point-in-time backup of the file system. Restore the backup to a new FSx for Windows File Server file system. Select SSD as the storage type. Select 32 MBps as the throughput capacity. When the backup and restore process is completed, adjust the DNS alias accordingly. Delete the original file system\n",
    "    - change from HDD -> SSD and increase throug put from 16 -> 32 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #194\n",
    "A company hosts an application on AWS. The application reads and writes objects that are stored in a single Amazon S3 bucket. The company must modify the application to deploy the application in two AWS Regions.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Set up an Amazon CloudFront distribution with the S3 bucket as an origin. Deploy the application to a second Region Modify the application to use the CloudFront distribution. Use AWS Global Accelerator to access the data in the S3 bucket.\n",
    "\n",
    "B. Create a new S3 bucket in a second Region. Set up bidirectional S3 Cross-Region Replication (CRR) between the original S3 bucket and the new S3 bucket. Configure an S3 Multi-Region Access Point that uses both S3 buckets. Deploy a modified application to both Regions.\n",
    "\n",
    "C. Create a new S3 bucket in a second Region Deploy the application in the second Region. Configure the application to use the new S3 bucket. Set up S3 Cross-Region Replication (CRR) from the original S3 bucket to the new S3 bucket.\n",
    "\n",
    "D. Set up an S3 gateway endpoint with the S3 bucket as an origin. Deploy the application to a second Region. Modify the application to use the new S3 gateway endpoint. Use S3 Intelligent-Tiering on the S3 bucket.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about how to config application to use s3 bucket sync in 2 regions\n",
    "    - Create a new S3 bucket in a second Region. Set up bidirectional S3 Cross-Region Replication (CRR) between the original S3 bucket and the new S3 bucket\n",
    "    - Configure an S3 Multi-Region Access Point that uses both S3 buckets. Deploy a modified application to both Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #195\n",
    "An online gaming company needs to rehost its gaming platform on AWS. The company's gaming application requires high performance computing (HPC) processing and has a leaderboard that changes frequently. An Ubuntu instance that is optimized for compute generation hosts a Node.js application for game display. Game state is tracked in an on-premises Redis instance.\n",
    "\n",
    "The company needs a migration strategy that optimizes application performance.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Create an Auto Scaling group of m5.large Amazon EC2 Spot Instances behind an Application Load Balancer. Use an Amazon ElastlCache for Redis cluster to maintain the leaderboard.\n",
    "\n",
    "B. Create an Auto Scaling group of c5.large Amazon EC2 Spot Instances behind an Application Load Balancer. Use an Amazon OpenSearch Service cluster to maintain the leaderboard.\n",
    "\n",
    "C. Create an Auto Scaling group of c5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon ElastiCache for Redis cluster to maintain the leaderboard.\n",
    "\n",
    "D. Create an Auto Scaling group of m5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon DynamoDB table to maintain the leaderboard.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about how to use Amazon ElastiCache for Redis cluster to replace on-premis redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #196\n",
    "A solutions architect is designing an application to accept timesheet entries from employees on their mobile devices. Timesheets will be submitted weekly, with most of the submissions occurring on Friday. The data must be stored in a format that allows payroll administrators to run monthly reports. The infrastructure must be highly available and scale to match the rate of incoming data and reporting requests.\n",
    "\n",
    "Which combination of steps meets these requirements while minimizing operational overhead? (Choose two.)\n",
    "\n",
    "A. Deploy the application to Amazon EC2 On-Demand Instances with load balancing across multiple Availability Zones. Use scheduled Amazon EC2 Auto Scaling to add capacity before the high volume of submissions on Fridays.\n",
    "\n",
    "B. Deploy the application in a container using Amazon Elastic Container Service (Amazon ECS) with load balancing across multiple Availability Zones. Use scheduled Service\n",
    "Auto Scaling to add capacity before the high volume of submissions on Fridays.\n",
    "\n",
    "C. Deploy the application front end to an Amazon S3 bucket served by Amazon CloudFront. Deploy the application backend using Amazon API Gateway with an AWS Lambda proxy integration.\n",
    "\n",
    "D. Store the timesheet submission data in Amazon Redshift. Use Amazon QuickSight to generate the reports using Amazon Redshift as the data source.\n",
    "\n",
    "E. Store the timesheet submission data in Amazon S3. Use Amazon Athena and Amazon QuickSight to generate the reports using Amazon S3 as the data source.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C, E\n",
    "- the question is about how to design timesheet application which minimizing operational overhead\n",
    "    - Deploy the application front end to an Amazon S3 bucket served by Amazon CloudFront. Deploy the application backend using Amazon API Gateway with an AWS Lambda proxy integration\n",
    "    - Store the timesheet submission data in Amazon S3. Use Amazon Athena and Amazon QuickSight to generate the reports using Amazon S3 as the data source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #197\n",
    "A company is storing sensitive data in an Amazon S3 bucket. The company must log all activities for objects in the S3 bucket and must keep the logs for 5 years. The company's security team also must receive an email notification every time there is an attempt to delete data in the S3 bucket.\n",
    "\n",
    "Which combination of steps will meet these requirements MOST cost-effectively? (Choose three.)\n",
    "\n",
    "A. Configure AWS CloudTrail to log S3 data events.\n",
    "\n",
    "B. Configure S3 server access logging for the S3 bucket.\n",
    "\n",
    "C. Configure Amazon S3 to send object deletion events to Amazon Simple Email Service (Amazon SES).\n",
    "\n",
    "D. Configure Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishes to an Amazon Simple Notification Service (Amazon SNS) topic.\n",
    "\n",
    "E. Configure Amazon S3 to send the logs to Amazon Timestream with data storage tiering.\n",
    "\n",
    "F. Configure a new S3 bucket to store the logs with an S3 Lifecycle policy.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, D, F\n",
    "- how to config s3 bucket which keep logs of all activity and send notification when attemp to delete\n",
    "    - Configure AWS CloudTrail to log S3 data events\n",
    "    - Configure Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishes to an Amazon Simple Notification Service (Amazon SNS) topic\n",
    "    - Configure a new S3 bucket to store the logs with an S3 Lifecycle policy\n",
    "    - Using server access logging provides basic access logs for requests made to the S3 bucket, but it is not as comprehensive for auditing purposes as CloudTrail and can result in a large volume of data, increasing costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #198\n",
    "A company is building a hybrid environment that includes servers in an on-premises data center and in the AWS Cloud. The company has deployed Amazon EC2 instances in three VPCs. Each VPC is in a different AWS Region. The company has established an AWS Direct. Connect connection to the data center from the Region that is closest to the data center.\n",
    "\n",
    "The company needs the servers in the on-premises data center to have access to the EC2 instances in all three VPCs. The servers in the on-premises data center also must have access to AWS public services.\n",
    "\n",
    "Which combination of steps will meet these requirements with the LEAST cost? (Choose two.)\n",
    "\n",
    "A. Create a Direct Connect gateway in the Region that is closest to the data center. Attach the Direct Connect connection to the Direct Connect gateway. Use the Direct Connect gateway to connect the VPCs in the other two Regions.\n",
    "\n",
    "B. Set up additional Direct Connect connections from the on-premises data center to the other two Regions.\n",
    "\n",
    "C. Create a private VIF. Establish an AWS Site-to-Site VPN connection over the private VIF to the VPCs in the other two Regions.\n",
    "\n",
    "D. Create a public VIF. Establish an AWS Site-to-Site VPN connection over the public VIF to the VPCs in the other two Regions.\n",
    "\n",
    "E. Use VPC peering to establish a connection between the VPCs across the Regions Create a private VIF with the existing Direct Connect connection to connect to the peered VPCs.\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, D\n",
    "- the question is about how to config network for multiple vpc, multiple regions also have access to aws public services\n",
    "    - Create a Direct Connect gateway in the Region that is closest to the data center. Attach the Direct Connect connection to the Direct Connect gateway. Use the Direct Connect gateway to connect the VPCs in the other two Regions\n",
    "    - Create a public VIF. Establish an AWS Site-to-Site VPN connection over the public VIF to the VPCs in the other two Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #199\n",
    "A company is using an organization in AWS Organizations to manage hundreds of AWS accounts. A solutions architect is working on a solution to provide baseline protection for the Open Web Application Security Project (OWASP) top 10 web application vulnerabilities. The solutions architect is using AWS WAF for all existing and new Amazon CloudFront distributions that are deployed within the organization.\n",
    "\n",
    "Which combination of steps should the solutions architect take to provide the baseline protection? (Choose three.)\n",
    "\n",
    "A. Enable AWS Config in all accounts\n",
    "\n",
    "B. Enable Amazon GuardDuty in all accounts\n",
    "\n",
    "C. Enable all features for the organization\n",
    "\n",
    "D. Use AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions\n",
    "\n",
    "E. Use AWS Shield Advanced to deploy AWS WAF rules in all accounts for all CloudFront distributions\n",
    "\n",
    "F. Use AWS Security Hub to deploy AWS WAF rules in all accounts for all CloudFront distributions\n",
    "\n",
    "Solution:\n",
    "- correct answer is A, C, D\n",
    "- the question is about baseline protection\n",
    "    - Enable AWS Config in all accounts\n",
    "    - Enable all features for the organization\n",
    "    - Use AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #200\n",
    "A solutions architect has implemented a SAML 2.0 federated identity solution with their company's on-premises identity provider (IdP) to authenticate users' access to the AWS environment. When the solutions architect tests authentication through the federated identity web portal, access to the AWS environment is granted. However, when test users attempt to authenticate through the federated identity web portal, they are not able to access the AWS environment.\n",
    "\n",
    "Which items should the solutions architect check to ensure identity federation is properly configured? (Choose three.)\n",
    "\n",
    "A. The IAM user's permissions policy has allowed the use of SAML federation for that user.\n",
    "\n",
    "B. The IAM roles created for the federated users' or federated groups' trust policy have set the SAML provider as the principal.\n",
    "\n",
    "B. Test users are not in the AWSFederatedUsers group in the company's IdP.\n",
    "\n",
    "C. The web portal calls the AWS STS AssumeRoleWithSAML API with the ARN of the SAML provider, the ARN of the IAM role, and the SAML assertion from IdP.\n",
    "\n",
    "D. The on-premises IdP's DNS hostname is reachable from the AWS environment VPCs.\n",
    "\n",
    "E. The company's IdP defines SAML assertions that properly map users or groups. In the company to IAM roles with appropriate permissions.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B, C, E\n",
    "- the question is about checklist for authentication with SAML\n",
    "    - The IAM roles created for the federated users' or federated groups' trust policy have set the SAML provider as the principal\n",
    "    - The web portal calls the AWS STS AssumeRoleWithSAML API with the ARN of the SAML provider, the ARN of the IAM role, and the SAML assertion from IdP\n",
    "    - The company's IdP defines SAML assertions that properly map users or groups. In the company to IAM roles with appropriate permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #202\n",
    "A company needs to build a disaster recovery (DR) solution for its ecommerce website. The web application is hosted on a fleet of t3.large Amazon EC2 instances and uses an Amazon RDS for MySQL DB instance. The EC2 instances are in an Auto Scaling group that extends across multiple Availability Zones.\n",
    "\n",
    "In the event of a disaster, the web application must fail over to the secondary environment with an RPO of 30 seconds and an RTO of 10 minutes.\n",
    "\n",
    "Which solution will meet these requirements MOST cost-effectively?\n",
    "\n",
    "A. Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB instance. Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instance. Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds to the DR Region. Recover the EC2 instances from the latest EC2 backup. Use an Amazon Route 53 geolocation routing policy to automatically fail over to the DR Region in the event of a disaster.\n",
    "\n",
    "B. Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB instance. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. Run the EC2 instances at the minimum capacity in the DR Region. Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster. Increase the desired capacity of the Auto Scaling group.\n",
    "\n",
    "C. Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instance. Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds to the DR Region. Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Manually restore the backed-up data on new instances. Use an Amazon Route 53 simple routing policy to automatically fail over to the DR Region in the event of a disaster.\n",
    "\n",
    "D. Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create an Amazon Aurora global database. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. Run the Auto Scaling group of EC2 instances at full capacity in the DR Region. Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster.\n",
    "\n",
    "Solution:\n",
    "- correct answer is b\n",
    "- the question is about basic design for disaster recovery\n",
    "    - Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region\n",
    "    - Create a cross-Region read replica for the DB instance\n",
    "    - Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region\n",
    "    - Run the EC2 instances at the minimum capacity in the DR Region. Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster\n",
    "    - Increase the desired capacity of the Auto Scaling group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #203\n",
    "A company is planning a one-time migration of an on-premises MySQL database to Amazon Aurora MySQL in the us-east-1 Region. The company's current internet connection has limited bandwidth. The on-premises MySQL database is 60 TB in size. The company estimates that it will take a month to transfer the data to AWS over the current internet connection. The company needs a migration solution that will migrate the database more quickly.\n",
    "\n",
    "Which solution will migrate the database in the LEAST amount of time?\n",
    "\n",
    "A. Request a 1 Gbps AWS Direct Connect connection between the on-premises data center and AWS. Use AWS Database Migration Service (AWS DMS) to migrate the on-premises MySQL database to Aurora MySQL.\n",
    "\n",
    "B. Use AWS DataSync with the current internet connection to accelerate the data transfer between the on-premises data center and AWS. Use AWS Application Migration Service to migrate the on-premises MySQL database to Aurora MySQL.\n",
    "\n",
    "C. Order an AWS Snowball Edge device. Load the data into an Amazon S3 bucket by using the S3 interface. Use AWS Database Migration Service (AWS DMS) to migrate the data from Amazon S3 to Aurora MySQL.\n",
    "\n",
    "D. Order an AWS Snowball device. Load the data into an Amazon S3 bucket by using the S3 Adapter for Snowball. Use AWS Application Migration Service to migrate the data from Amazon S3 to Aurora MySQL.\n",
    "\n",
    "Solution:\n",
    "- correct answer is c\n",
    "- the question is about how to do one-tine migrate for large mysql on-premises database\n",
    "    - using Snowball Edge device\n",
    "    - chose c because we need to use AWS Database Migration Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #204\n",
    "A company has an application in the AWS Cloud. The application runs on a fleet of 20 Amazon EC2 instances. The EC2 instances are persistent and store data on multiple attached Amazon Elastic Block Store (Amazon EBS) volumes.\n",
    "\n",
    "The company must maintain backups in a separate AWS Region. The company must be able to recover the EC2 instances and their configuration within 1 business day, with loss of no more than 1 day's worth of data. The company has limited staff and needs a backup solution that optimizes operational efficiency and cost. The company already has created an AWS CloudFormation template that can deploy the required network configuration in a secondary Region.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Create a second CloudFormation template that can recreate the EC2 instances in the secondary Region. Run daily multivolume snapshots by using AWS Systems Manager Automation runbooks. Copy the snapshots to the secondary Region. In the event of a failure launch the CloudFormation templates, restore the EBS volumes from snapshots, and transfer usage to the secondary Region.\n",
    "\n",
    "B. Use Amazon Data Lifecycle Manager (Amazon DLM) to create daily multivolume snapshots of the EBS volumes. In the event of a failure, launch the CloudFormation template and use Amazon DLM to restore the EBS volumes and transfer usage to the secondary Region.\n",
    "\n",
    "C. Use AWS Backup to create a scheduled daily backup plan for the EC2 instances. Configure the backup task to copy the backups to a vault in the secondary Region. In the event of a failure, launch the CloudFormation template, restore the instance volumes and configurations from the backup vault, and transfer usage to the secondary Region.\n",
    "\n",
    "D. Deploy EC2 instances of the same size and configuration to the secondary Region. Configure AWS DataSync daily to copy data from the primary Region to the secondary Region. In the event of a failure, launch the CloudFormation template and transfer usage to the secondary Region.\n",
    "\n",
    "Solution:\n",
    "- correct answer is c\n",
    "- the question is about design backup and recovery for ec2 in different regions (ec2 + Elastic Block Store)\n",
    "    - Use AWS Backup to create a scheduled daily backup plan for the EC2 instances. Configure the backup task to copy the backups to a vault in the secondary Region -> daily backup schedule\n",
    "    - In the event of a failure, launch the CloudFormation template, restore the instance volumes and configurations from the backup vault, and transfer usage to the secondary Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #205\n",
    "A company is designing a new website that hosts static content. The website will give users the ability to upload and download large files. According to company requirements, all data must be encrypted in transit and at rest. A solutions architect is building the solution by using Amazon S3 and Amazon CloudFront.\n",
    "\n",
    "Which combination of steps will meet the encryption requirements? (Choose three.)\n",
    "\n",
    "A. Turn on S3 server-side encryption for the S3 bucket that the web application uses.\n",
    "\n",
    "B. Add a policy attribute of \"aws:SecureTransport\": \"true\" for read and write operations in the S3 ACLs.\n",
    "\n",
    "C. Create a bucket policy that denies any unencrypted operations in the S3 bucket that the web application uses.\n",
    "\n",
    "D. Configure encryption at rest on CloudFront by using server-side encryption with AWS KMS keys (SSE-KMS).\n",
    "\n",
    "E. Configure redirection of HTTP requests to HTTPS requests in CloudFront.\n",
    "\n",
    "F. Use the RequireSSL option in the creation of presigned URLs for the S3 bucket that the web application uses.\n",
    "\n",
    "Solution:\n",
    "- correct answer is a, c, e\n",
    "- the question is about the solution which encrypt data in transit and at rest (s3 + CloudFront)\n",
    "    - Turn on S3 server-side encryption for the S3 bucket that the web application uses\n",
    "    - Create a bucket policy that denies any unencrypted operations in the S3 bucket that the web application uses\n",
    "    - Configure redirection of HTTP requests to HTTPS requests in CloudFront"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #211\n",
    "A company wants to migrate to AWS. The company is running thousands of VMs in a VMware ESXi environment. The company has no configuration management database and has little knowledge about the utilization of the VMware portfolio.\n",
    "\n",
    "A solutions architect must provide the company with an accurate inventory so that the company can plan for a cost-effective migration.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Use AWS Systems Manager Patch Manager to deploy Migration Evaluator to each VM. Review the collected data in Amazon QuickSight. Identify servers that have high utilization. Remove the servers that have high utilization from the migration list. Import the data to AWS Migration Hub.\n",
    "\n",
    "B. Export the VMware portfolio to a .csv file. Check the disk utilization for each server. Remove servers that have high utilization. Export the data to AWS Application Migration Service. Use AWS Server Migration Service (AWS SMS) to migrate the remaining servers.\n",
    "\n",
    "C. Deploy the Migration Evaluator agentless collector to the ESXi hypervisor. Review the collected data in Migration Evaluator. Identify inactive servers. Remove the inactive servers from the migration list. Import the data to AWS Migration Hub.\n",
    "\n",
    "D. Deploy the AWS Application Migration Service Agent to each VM. When the data is collected, use Amazon Redshift to import and analyze the data. Use Amazon QuickSight for data visualization.\n",
    "\n",
    "Solution:\n",
    "- correct answer is c\n",
    "- the question is about how to discovery, evaluate on-premises vm for migrate to aws\n",
    "    - use Migration Evaluator instead of Application Migration Service because of least operational overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #212\n",
    "A company runs a microservice as an AWS Lambda function. The microservice writes data to an on-premises SQL database that supports a limited number of concurrent connections. When the number of Lambda function invocations is too high, the database crashes and causes application downtime. The company has an AWS Direct Connect connection between the company's VPC and the on-premises data center. The company wants to protect the database from crashes.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Write the data to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda function to read from the queue and write to the existing database. Set a reserved concurrency limit on the Lambda function that is less than the number of connections that the database supports.\n",
    "\n",
    "B. Create a new Amazon Aurora Serverless DB cluster. Use AWS DataSync to migrate the data from the existing database to Aurora Serverless. Reconfigure the Lambda function to write to Aurora.\n",
    "\n",
    "C. Create an Amazon RDS Proxy DB instance. Attach the RDS Proxy DB instance to the Amazon RDS DB instance. Reconfigure the Lambda function to write to the RDS Proxy DB instance.\n",
    "\n",
    "D. Write the data to an Amazon Simple Notification Service (Amazon SNS) topic. Invoke the Lambda function to write to the existing database when the topic receives new messages. Configure provisioned concurrency for the Lambda function to be equal to the number of connections that the database supports.\n",
    "\n",
    "Solution:\n",
    "- correct answer is a\n",
    "- the question is about solution which prevent on-premises database from crashing\n",
    "    - queue and or proxy. However, the data base is on premise therefore we choose queue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #215\n",
    "A solutions architect is designing an AWS account structure for a company that consists of multiple teams. All the teams will work in the same AWS Region. The company needs a VPC that is connected to the on-premises network. The company expects less than 50 Mbps of total traffic to and from the on-premises network.\n",
    "\n",
    "Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)\n",
    "\n",
    "A. Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to each AWS account.\n",
    "\n",
    "B. Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to a shared services account. Share the subnets by using AWS Resource Access Manager.\n",
    "\n",
    "C. Use AWS Transit Gateway along with an AWS Site-to-Site VPN for connectivity to the on-premises network. Share the transit gateway by using AWS Resource Access Manager.\n",
    "\n",
    "D. Use AWS Site-to-Site VPN for connectivity to the on-premises network.\n",
    "\n",
    "E. Use AWS Direct Connect for connectivity to the on-premises network.\n",
    "\n",
    "Solution:\n",
    "- correct answer is b, d\n",
    "- the question is about design account structure + networking connection with less than 50 Mbps\n",
    "    - less than 50 Mbps -> Use AWS Site-to-Site VPN for connectivity to the on-premises network\n",
    "    - Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to a shared services account. Share the subnets by using AWS Resource Access Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #216\n",
    "A solutions architect at a large company needs to set up network security for outbound traffic to the internet from all AWS accounts within an organization in AWS Organizations. The organization has more than 100 AWS accounts, and the accounts route to each other by using a centralized AWS Transit Gateway. Each account has both an internet gateway and a NAT gateway for outbound traffic to the internet. The company deploys resources only into a single AWS Region.\n",
    "\n",
    "The company needs the ability to add centrally managed rule-based filtering on all outbound traffic to the internet for all AWS accounts in the organization. The peak load of outbound traffic will not exceed 25 Gbps in each Availability Zone.\n",
    "\n",
    "Which solution meets these requirements?\n",
    "\n",
    "A. Create a new VPC for outbound traffic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway. Create an Auto Scaling group of Amazon EC2 instances that run an open-source internet proxy for rule-based filtering across all Availability Zones in the Region. Modify all default routes to point to the proxy's Auto Scaling group.\n",
    "\n",
    "B. Create a new VPC for outbound traffic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway. Use an AWS Network Firewall firewall for rule-based filtering. Create Network Firewall endpoints in each Availability Zone. Modify all default routes to point to the Network Firewall endpoints.\n",
    "\n",
    "C. Create an AWS Network Firewall firewall for rule-based filtering in each AWS account. Modify all default routes to point to the Network Firewall firewalls in each account.\n",
    "\n",
    "D. In each AWS account, create an Auto Scaling group of network-optimized Amazon EC2 instances that run an open-source internet proxy for rule-based filtering. Modify all default routes to point to the proxy's Auto Scaling group.\n",
    "\n",
    "Solution:\n",
    "- correct answer is b\n",
    "- the question is about centrally managed rule-based filtering on all outbound traffic to the internet for all AWS accounts in the organization\n",
    "    - Create a new VPC for outbound traffic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway\n",
    "    - Use an AWS Network Firewall firewall for rule-based filtering. Create Network Firewall endpoints in each Availability Zone. Modify all default routes to point to the Network Firewall endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #220\n",
    "A company has IoT sensors that monitor traffic patterns throughout a large city. The company wants to read and collect data from the sensors and perform aggregations on the data.\n",
    "\n",
    "A solutions architect designs a solution in which the IoT devices are streaming to Amazon Kinesis Data Streams. Several applications are reading from the stream. However, several consumers are experiencing throttling and are periodically encountering a ReadProvisionedThroughputExceeded error.\n",
    "\n",
    "Which actions should the solutions architect take to resolve this issue? (Choose three.)\n",
    "\n",
    "A. Reshard the stream to increase the number of shards in the stream.\n",
    "\n",
    "B. Use the Kinesis Producer Library (KPL). Adjust the polling frequency.\n",
    "\n",
    "C. Use consumers with the enhanced fan-out feature.\n",
    "\n",
    "D. Reshard the stream to reduce the number of shards in the stream.\n",
    "\n",
    "E. Use an error retry and exponential backoff mechanism in the consumer logic.\n",
    "\n",
    "F. Configure the stream to use dynamic partitioning.\n",
    "\n",
    "Solution:\n",
    "- correct answer is a, c, e\n",
    "- the question is about design for IOT streaming data application and solve error ReadProvisionedThroughputExceeded\n",
    "    - Reshard the stream to increase the number of shards in the stream -> reshard\n",
    "    - Use consumers with the enhanced fan-out feature -> fan-out\n",
    "    - Use an error retry and exponential backoff mechanism in the consumer logic -> using error retry and exponential backoff mechanisum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #222\n",
    "A company wants to run a custom network analysis software package to inspect traffic as traffic leaves and enters a VPC. The company has deployed the solution by using AWS CloudFormation on three Amazon EC2 instances in an Auto Scaling group. All network routing has been established to direct traffic to the EC2 instances.\n",
    "\n",
    "Whenever the analysis software stops working, the Auto Scaling group replaces an instance. The network routes are not updated when the instance replacement occurs.\n",
    "\n",
    "Which combination of steps will resolve this issue? (Choose three.)\n",
    "\n",
    "A. Create alarms based on EC2 status check metrics that will cause the Auto Scaling group to replace the failed instance.\n",
    "\n",
    "B. Update the CloudFormation template to install the Amazon CloudWatch agent on the EC2 instances. Configure the CloudWatch agent to send process metrics for the application.\n",
    "\n",
    "C. Update the CloudFormation template to install AWS Systems Manager Agent on the EC2 instances. Configure Systems Manager Agent to send process metrics for the application.\n",
    "\n",
    "D. Create an alarm for the custom metric in Amazon CloudWatch for the failure scenarios. Configure the alarm to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic.\n",
    "\n",
    "E. Create an AWS Lambda function that responds to the Amazon Simple Notification Service (Amazon SNS) message to take the instance out of service. Update the network routes to point to the replacement instance.\n",
    "\n",
    "F. In the CloudFormation template, write a condition that updates the network routes when a replacement instance is launched.\n",
    "\n",
    "Solution:\n",
    "- correct answer is b, d, e\n",
    "- the question is about design auto update network routes for auto scaling group when scaling event happened\n",
    "    - Update the CloudFormation template to install the Amazon CloudWatch agent on the EC2 instances. Configure the CloudWatch agent to send process metrics for the application - using CloudWatch agent to collect ec2 information\n",
    "    - Create an alarm for the custom metric in Amazon CloudWatch for the failure scenarios. Configure the alarm to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic -> use alarm to know when failter or event happend, use Simple Notification to notify for trigger lambda function to handle error event\n",
    "    - Create an AWS Lambda function that responds to the Amazon Simple Notification Service (Amazon SNS) message to take the instance out of service. Update the network routes to point to the replacement instance -> use lambda function to handle event from SNS and update route tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #228\n",
    "A company wants to containerize a multi-tier web application and move the application from an on-premises data center to AWS. The application includes web. application, and database tiers. The company needs to make the application fault tolerant and scalable. Some frequently accessed data must always be available across application servers. Frontend web servers need session persistence and must scale to meet increases in traffic.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST ongoing operational overhead?\n",
    "\n",
    "A. Run the application on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Use Amazon Elastic File System (Amazon EFS) for data that is frequently accessed between the web and application tiers. Store the frontend web server session data in Amazon Simple Queue Service (Amazon SQS).\n",
    "\n",
    "B. Run the application on Amazon Elastic Container Service (Amazon ECS) on Amazon EC2. Use Amazon ElastiCache for Redis to cache frontend web server session data. Use Amazon Elastic Block Store (Amazon EBS) with Multi-Attach on EC2 instances that are distributed across multiple Availability Zones.\n",
    "\n",
    "C. Run the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Use ReplicaSets to run the web servers and applications. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system across all EKS pods to store frontend web server session data.\n",
    "\n",
    "D. Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Run the web servers and application as Kubernetes deployments in the EKS cluster. Store the frontend web server session data in an Amazon DynamoDB table. Create an Amazon Elastic File System (Amazon EFS) volume that all applications will mount at the time of deployment.\n",
    "\n",
    "Solution:\n",
    "- correct answer is d\n",
    "- the question is about what is solution architect for multi-tier application which is container based and required less operational overhead\n",
    "    - EKS or ECS\n",
    "    - session data -> DynamoDB\n",
    "    - Amazon Elastic File System (Amazon EFS) for file system for EKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #229\n",
    "A solutions architect is planning to migrate critical Microsoft SQL Server databases to AWS. Because the databases are legacy systems, the solutions architect will move the databases to a modern data architecture. The solutions architect must migrate the databases with near-zero downtime.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Use AWS Application Migration Service and the AWS Schema Conversion Tool (AWS SCT). Perform an in-place upgrade before the migration. Export the migrated data to Amazon Aurora Serverless after cutover. Repoint the applications to Amazon Aurora.\n",
    "\n",
    "B. Use AWS Database Migration Service (AWS DMS) to rehost the database. Set Amazon S3 as a target. Set up change data capture (CDC) replication. When the source and destination are fully synchronized, load the data from Amazon S3 into an Amazon RDS for Microsoft SQL Server DB instance.\n",
    "\n",
    "C. Use native database high availability tools. Connect the source system to an Amazon RDS for Microsoft SQL Server DB instance. Configure replication accordingly. When data replication is finished, transition the workload to an Amazon RDS for Microsoft SQL Server DB instance.\n",
    "\n",
    "D. Use AWS Application Migration Service. Rehost the database server on Amazon EC2. When data replication is finished, detach the database and move the database to an Amazon RDS for Microsoft SQL Server DB instance. Reattach the database and then cut over all networking.\n",
    "\n",
    "Solution:\n",
    "- correct answer is C\n",
    "- the question is about migrate Microsoft SQL Server database to aws with near-zero downtime\n",
    "    - we just need config replica direct from source database to target Amazon RDS for Microsoft SQL Server DB instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #230\n",
    "A company's solutions architect is analyzing costs of a multi-application environment. The environment is deployed across multiple Availability Zones in a single AWS Region. After a recent acquisition, the company manages two organizations in AWS Organizations. The company has created multiple service provider applications as AWS PrivateLink-powered VPC endpoint services in one organization. The company has created multiple service consumer applications in the other organization.\n",
    "\n",
    "Data transfer charges are much higher than the company expected, and the solutions architect needs to reduce the costs. The solutions architect must recommend guidelines for developers to follow when they deploy services. These guidelines must minimize data transfer charges for the whole environment.\n",
    "\n",
    "Which guidelines meet these requirements? (Choose two.)\n",
    "\n",
    "A. Use AWS Resource Access Manager to share the subnets that host the service provider applications with other accounts in the organization.\n",
    "\n",
    "B. Place the service provider applications and the service consumer applications in AWS accounts in the same organization.\n",
    "\n",
    "C. Turn off cross-zone load balancing for the Network Load Balancer in all service provider application deployments.\n",
    "\n",
    "D. Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS name.\n",
    "\n",
    "E. Create a Savings Plan that provides adequate coverage for the organization's planned inter-Availability Zone data transfer usage.\n",
    "\n",
    "Solution:\n",
    "- correct answer is b, d\n",
    "- the question is about how to recude data tranfer cost in case of multiple orgnaization \n",
    "    - Place the service provider applications and the service consumer applications in AWS accounts in the same organization\n",
    "    - Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #231\n",
    "A company has an on-premises Microsoft SQL Server database that writes a nightly 200 GB export to a local drive. The company wants to move the backups to more robust cloud storage on Amazon S3. The company has set up a 10 Gbps AWS Direct Connect connection between the on-premises data center and AWS.\n",
    "\n",
    "Which solution meets these requirements MOST cost-effectively?\n",
    "\n",
    "A. Create a new S3 bucket. Deploy an AWS Storage Gateway file gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share.\n",
    "\n",
    "B. Create an Amazon FSx for Windows File Server Single-AZ file system within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB file share on the Amazon FSx file system. Enable nightly backups.\n",
    "\n",
    "C. Create an Amazon FSx for Windows File Server Multi-AZ file system within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB file share on the Amazon FSx file system. Enable nightly backups.\n",
    "\n",
    "D. Create a new S3 bucket. Deploy an AWS Storage Gateway volume gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share on the volume gateway, and automate copies of this data to an S3 bucket.\n",
    "\n",
    "Solution:\n",
    "- correct answer is a\n",
    "- the question is about how to do backup for microsoft sql on-premises database\n",
    "    - Create a new S3 bucket\n",
    "    - Deploy an AWS Storage Gateway file gateway within the VPC that is connected to the Direct Connect connection\n",
    "    - Create a new SMB file share. Write nightly database exports to the new SMB file share\n",
    "    - using file gateway is the most cost-effective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #233\n",
    "A company is migrating its development and production workloads to a new organization in AWS Organizations. The company has created a separate member account for development and a separate member account for production. Consolidated billing is linked to the management account. In the management account, a solutions architect needs to create an IAM user that can stop or terminate resources in both member accounts.\n",
    "\n",
    "Which solution will meet this requirement?\n",
    "\n",
    "A. Create an IAM user and a cross-account role in the management account. Configure the cross-account role with least privilege access to the member accounts.\n",
    "\n",
    "B. Create an IAM user in each member account. In the management account, create a cross-account role that has least privilege access. Grant the IAM users access to the cross-account role by using a trust policy.\n",
    "\n",
    "C. Create an IAM user in the management account. In the member accounts, create an IAM group that has least privilege access. Add the IAM user from the management account to each IAM group in the member accounts.\n",
    "\n",
    "D. Create an IAM user in the management account. In the member accounts, create cross-account roles that have least privilege access. Grant the IAM user access to the roles by using a trust policy.\n",
    "\n",
    "Solution:\n",
    "- correct answer is d\n",
    "- the question is about organization and accounts which allow management account to control resource in child account\n",
    "    - Create an IAM user in the management account\n",
    "    - In the member accounts, create cross-account roles that have least privilege access. Grant the IAM user access to the roles by using a trust policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #235\n",
    "A company has built a high performance computing (HPC) cluster in AWS for a tightly coupled workload that generates a large number of shared files stored in Amazon EFS. The cluster was performing well when the number of Amazon EC2 instances in the cluster was 100. However, when the company increased the cluster size to 1.000 EC2 instances, overall performance was well below expectations.\n",
    "\n",
    "Which collection of design choices should a solutions architect make to achieve the maximum performance from the HPC cluster? (Choose three.)\n",
    "\n",
    "A. Ensure the HPC cluster is launched within a single Availability Zone.\n",
    "\n",
    "B. Launch the EC2 instances and attach elastic network interfaces in multiples of four.\n",
    "\n",
    "C. Select EC2 instance types with an Elastic Fabric Adapter (EFA) enabled.\n",
    "\n",
    "D. Ensure the cluster is launched across multiple Availability Zones.\n",
    "\n",
    "E. Replace Amazon EFS with multiple Amazon EBS volumes in a RAID array.\n",
    "\n",
    "F. Replace Amazon EFS with Amazon FSx for Lustre.\n",
    "\n",
    "Solution:\n",
    "- correct answer is a, c, f\n",
    "- the question is about how to design cluster which have up to 1,000 instance\n",
    "    - Ensure the HPC cluster is launched within a single Availability Zone. low network latency and high bandwidth, as they are located within the same data center\n",
    "    - Select EC2 instance types with an Elastic Fabric Adapter (EFA) enabled. EFA is a network interface that provides low-latency, high-bandwidth communication between EC2 instances\n",
    "    - Replace Amazon EFS with Amazon FSx for Lustre. Amazon FSx for Lustre is a high-performance file system optimized for HPC workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #236\n",
    "A company is designing an AWS Organizations structure. The company wants to standardize a process to apply tags across the entire organization. The company will require tags with specific values when a user creates a new resource. Each of the company's OUs will have unique tag values.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs.\n",
    "\n",
    "B. Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the organization's management account.\n",
    "\n",
    "C. Use an SCP to allow the creation of resources only when the resources have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs.\n",
    "\n",
    "D. Use an SCP to deny the creation of resources that do not have the required tags. Define the list of tags. Attach the SCP to the OUs.\n",
    "\n",
    "Solution:\n",
    "- correct answer is a\n",
    "- the question is how to design standardize tag system\n",
    "    - Use an SCP to deny the creation of resources that do not have the required tags\n",
    "    - Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #239\n",
    "A company is developing a gene reporting device that will collect genomic information to assist researchers with collecting large samples of data from a diverse population. The device will push 8 KB of genomic data every second to a data platform that will need to process and analyze the data and provide information back to researchers. The data platform must meet the following requirements:\n",
    "\n",
    "• Provide near-real-time analytics of the inbound genomic data\n",
    "\n",
    "• Ensure the data is flexible, parallel, and durable\n",
    "\n",
    "• Deliver results of processing to a data warehouse\n",
    "\n",
    "\n",
    "Which strategy should a solutions architect use to meet these requirements?\n",
    "\n",
    "A. Use Amazon Kinesis Data Firehose to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an Amazon RDS instance.\n",
    "\n",
    "B. Use Amazon Kinesis Data Streams to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an Amazon Redshift cluster using Amazon EMR.\n",
    "\n",
    "C. Use Amazon S3 to collect the inbound device data, analyze the data from Amazon SQS with Kinesis, and save the results to an Amazon Redshift cluster.\n",
    "\n",
    "D. Use an Amazon API Gateway to put requests into an Amazon SQS queue, analyze the data with an AWS Lambda function, and save the results to an Amazon Redshift cluster using Amazon EMR.\n",
    "\n",
    "Solution:\n",
    "- correct answer is b\n",
    "- the question is about design real time analytic and data is stored at data warehouse\n",
    "    - real time -> Kinesis\n",
    "    - data warehouse -> Amazon Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #242\n",
    "During an audit, a security team discovered that a development team was putting IAM user secret access keys in their code and then committing it to an AWS CodeCommit repository. The security team wants to automatically find and remediate instances of this security vulnerability.\n",
    "\n",
    "Which solution will ensure that the credentials are appropriately secured automatically?\n",
    "\n",
    "A. Run a script nightly using AWS Systems Manager Run Command to search for credentials on the development instances. If found, use AWS Secrets Manager to rotate the credentials\n",
    "\n",
    "B. Use a scheduled AWS Lambda function to download and scan the application code from CodeCommit. If credentials are found, generate new credentials and store them in AWS KMS.\n",
    "\n",
    "C. Configure Amazon Macie to scan for credentials in CodeCommit repositories. If credentials are found, trigger an AWS Lambda function to disable the credentials and notify the user.\n",
    "\n",
    "D. Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If credentials are found, disable them in AWS IAM and notify the user.\n",
    "\n",
    "Solution:\n",
    "- correct answer is d\n",
    "- the question is about how to do key scanning and other remediate\n",
    "    - Macie is only work on s3\n",
    "    - the solution is use lambda function to scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #243\n",
    "A company has a data lake in Amazon S3 that needs to be accessed by hundreds of applications across many AWS accounts. The company's information security policy states that the S3 bucket must not be accessed over the public internet and that each application should have the minimum permissions necessary to function.\n",
    "\n",
    "To meet these requirements, a solutions architect plans to use an S3 access point that is restricted to specific VPCs for each application.\n",
    "\n",
    "Which combination of steps should the solutions architect take to implement this solution? (Choose two.)\n",
    "\n",
    "A. Create an S3 access point for each application in the AWS account that owns the S3 bucket. Configure each access point to be accessible only from the application’s VPC. Update the bucket policy to require access from an access point.\n",
    "\n",
    "B. Create an interface endpoint for Amazon S3 in each application's VPC. Configure the endpoint policy to allow access to an S3 access point. Create a VPC gateway attachment for the S3 endpoint.\n",
    "\n",
    "C. Create a gateway endpoint for Amazon S3 in each application's VPConfigure the endpoint policy to allow access to an S3 access point. Specify the route table that is used to access the access point.\n",
    "\n",
    "D. Create an S3 access point for each application in each AWS account and attach the access points to the S3 bucket. Configure each access point to be accessible only from the application's VPC. Update the bucket policy to require access from an access point.\n",
    "\n",
    "E. Create a gateway endpoint for Amazon S3 in the data lake's VPC. Attach an endpoint policy to allow access to the S3 bucket. Specify the route table that is used to access the bucket.\n",
    "\n",
    "Solution:\n",
    "- correct answer is a, c\n",
    "- the question is about access control for s2 with s3 access point and vpc\n",
    "    - we need access point and gateway endpoint\n",
    "        - access point is config at account levelt\n",
    "        - gateway endpoint at application's VPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #247\n",
    "A large company runs workloads in VPCs that are deployed across hundreds of AWS accounts. Each VPC consists of public subnets and private subnets that span across multiple Availability Zones. NAT gateways are deployed in the public subnets and allow outbound connectivity to the internet from the private subnets.\n",
    "\n",
    "A solutions architect is working on a hub-and-spoke design. All private subnets in the spoke VPCs must route traffic to the internet through an egress VPC. The solutions architect already has deployed a NAT gateway in an egress VPC in a central AWS account.\n",
    "\n",
    "Which set of additional steps should the solutions architect take to meet these requirements?\n",
    "\n",
    "A. Create peering connections between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet.\n",
    "\n",
    "B. Create a transit gateway, and share it with the existing AWS accounts. Attach existing VPCs to the transit gateway. Configure the required routing to allow access to the internet.\n",
    "\n",
    "C. Create a transit gateway in every account. Attach the NAT gateway to the transit gateways. Configure the required routing to allow access to the internet.\n",
    "\n",
    "D. Create an AWS PrivateLink connection between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet.\n",
    "\n",
    "Solution:\n",
    "- correct answer is B\n",
    "- the question is about design centralize egress\n",
    "    - we already have egress VPC, and nat gateway\n",
    "    - we will need transit gateway to connect multiple VPC\n",
    "        - Create a transit gateway, and share it with the existing AWS accounts.\n",
    "        - Attach existing VPCs to the transit gateway\n",
    "        - Configure the required routing to allow access to the internet\n",
    "    - choose b over c because we just need one transit gateway\n",
    "    - AWS PrivateLink is a highly available, scalable technology that you can use to privately connect your VPC to services as if they were in your VPC\n",
    "    - A VPC peering connection is a networking connection between two VPCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #250\n",
    "A company has a new application that needs to run on five Amazon EC2 instances in a single AWS Region. The application requires high-throughput, low-latency network connections between all of the EC2 instances where the application will run. There is no requirement for the application to be fault tolerant.\n",
    "\n",
    "Which solution will meet these requirements?\n",
    "\n",
    "A. Launch five new EC2 instances into a cluster placement group. Ensure that the EC2 instance type supports enhanced networking.\n",
    "\n",
    "B. Launch five new EC2 instances into an Auto Scaling group in the same Availability Zone. Attach an extra elastic network interface to each EC2 instance.\n",
    "\n",
    "C. Launch five new EC2 instances into a partition placement group. Ensure that the EC2 instance type supports enhanced networking.\n",
    "\n",
    "D. Launch five new EC2 instances into a spread placement group. Attach an extra elastic network interface to each EC2 instance.\n",
    "\n",
    "Solution:\n",
    "- correct answer is a\n",
    "- the question is about which computing instance to have high-throughput, low-latency\n",
    "    - use cluster placement group over auto scalling group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #251\n",
    "A company is creating a REST API to share information with six of its partners based in the United States. The company has created an Amazon API Gateway Regional endpoint. Each of the six partners will access the API once per day to post daily sales figures.\n",
    "\n",
    "After initial deployment, the company observes 1,000 requests per second originating from 500 different IP addresses around the world. The company believes this traffic is originating from a botnet and wants to secure its API while minimizing cost.\n",
    "\n",
    "Which approach should the company take to secure its API?\n",
    "\n",
    "A. Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit more than five requests per day. Associate the web ACL with the CloudFront distribution. Configure CloudFront with an origin access identity (OAI) and associate it with the distribution. Configure API Gateway to ensure only the OAI can run the POST method.\n",
    "\n",
    "B. Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit more than five requests per day. Associate the web ACL with the CloudFront distribution. Add a custom header to the CloudFront distribution populated with an API key. Configure the API to require an API key on the POST method.\n",
    "\n",
    "C. Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a resource policy with a request limit and associate it with the API. Configure the API to require an API key on the POST method.\n",
    "\n",
    "D. Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a usage plan with a request limit and associate it with the API. Create an API key and add it to the usage plan.\n",
    "\n",
    "Solution:\n",
    "- correct answer is d\n",
    "- the question is about how to avoid bot attach given that we know who will have access to our api\n",
    "    - Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners\n",
    "    - Associate the web ACL with the API\n",
    "    - Create a usage plan with a request limit and associate it with the API\n",
    "    - Create an API key and add it to the usage plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #254\n",
    "A financial services company loaded millions of historical stock trades into an Amazon DynamoDB table. The table uses on-demand capacity mode. Once each day at midnight, a few million new records are loaded into the table. Application read activity against the table happens in bursts throughout the day. and a limited set of keys are repeatedly looked up. The company needs to reduce costs associated with DynamoDB.\n",
    "\n",
    "Which strategy should a solutions architect recommend to meet this requirement?\n",
    "\n",
    "A. Deploy an Amazon ElastiCache cluster in front of the DynamoDB table\n",
    "\n",
    "B. Deploy DynamoDB Accelerator (DAX). Configure DynamoDB auto scaling. Purchase Savings Plans in Cost Explorer.\n",
    "\n",
    "C. Use provisioned capacity mode. Purchase Savings Plans in Cost Explorer.\n",
    "\n",
    "D. Deploy DynamoDB Accelerator (DAX). Use provisioned capacity mode. Configure DynamoDB auto scaling.\n",
    "\n",
    "Solution:\n",
    "- correct answer is d\n",
    "- the question is about saving cost for DynamoDb\n",
    "    - use provisioned capacity mode\n",
    "    - Deploy DynamoDB Accelerator (DAX). Use provisioned capacity mode. Configure DynamoDB auto scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #255\n",
    "A company is creating a centralized logging service running on Amazon EC2 that will receive and analyze logs from hundreds of AWS accounts. AWS PrivateLink is being used to provide connectivity between the client services and the logging service.\n",
    "\n",
    "In each AWS account with a client, an interface endpoint has been created for the logging service and is available. The logging service running on EC2 instances with a Network Load Balancer (NLB) are deployed in different subnets. The clients are unable to submit logs using the VPC endpoint.\n",
    "\n",
    "Which combination of steps should a solutions architect take to resolve this issue? (Choose two.)\n",
    "\n",
    "A. Check that the NACL is attached to the logging service subnet to allow communications to and from the NLB subnets. Check that the NACL is attached to the NLB subnet to allow communications to and from the logging service subnets running on EC2 instances.\n",
    "\n",
    "B. Check that the NACL is attached to the logging service subnets to allow communications to and from the interface endpoint subnets. Check that the NACL is attached to the interface endpoint subnet to allow communications to and from the logging service subnets running on EC2 instances.\n",
    "\n",
    "C. Check the security group for the logging service running on the EC2 instances to ensure it allows ingress from the NLB subnets.\n",
    "\n",
    "D. Check the security group for the logging service running on EC2 instances to ensure it allows ingress from the clients.\n",
    "\n",
    "E. Check the security group for the NLB to ensure it allows ingress from the interface endpoint subnets.\n",
    "\n",
    "Solution:\n",
    "- correct answer is a, c\n",
    "- the question is about networking solution given\n",
    "    - logging service running on EC2 instances with a Network Load Balancer (NLB) are deployed in different subnets\n",
    "    - The clients are unable to submit logs using the VPC endpoint\n",
    "    - Check that the NACL is attached to the logging service subnet to allow communications to and from the NLB subnets. Check that the NACL is attached to the NLB subnet to allow communications to and from the logging service subnets running on EC2 instances\n",
    "    - Check the security group for the logging service running on the EC2 instances to ensure it allows ingress from the NLB subnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #256\n",
    "A company has millions of objects in an Amazon S3 bucket. The objects are in the S3 Standard storage class. All the S3 objects are accessed frequently. The number of users and applications that access the objects is increasing rapidly. The objects are encrypted with server-side encryption with AWS KMS keys (SSE-KMS).\n",
    "\n",
    "A solutions architect reviews the company’s monthly AWS invoice and notices that AWS KMS costs are increasing because of the high number of requests from Amazon S3. The solutions architect needs to optimize costs with minimal changes to the application.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Create a new S3 bucket that has server-side encryption with customer-provided keys (SSE-C) as the encryption type. Copy the existing objects to the new S3 bucket. Specify SSE-C.\n",
    "\n",
    "B. Create a new S3 bucket that has server-side encryption with Amazon S3 managed keys (SSE-S3) as the encryption type. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. Specify SSE-S3.\n",
    "\n",
    "C. Use AWS CloudHSM to store the encryption keys. Create a new S3 bucket. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. Encrypt the objects by using the keys from CloudHSM.\n",
    "\n",
    "D. Use the S3 Intelligent-Tiering storage class for the S3 bucket. Create an S3 Intelligent-Tiering archive configuration to transition objects that are not accessed for 90 days to S3 Glacier Deep Archive.\n",
    "\n",
    "Solution:\n",
    "- correct answer is b\n",
    "- the question is how to minimize the cost of AWS KMS using to encrypt s3 object\n",
    "    - change from customer managed key to s3 managed key to reduced cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #257\n",
    "A media storage application uploads user photos to Amazon S3 for processing by AWS Lambda functions. Application state is stored in Amazon DynamoDB tables. Users are reporting that some uploaded photos are not being processed properly. The application developers trace the logs and find that Lambda is experiencing photo processing issues when thousands of users upload photos simultaneously. The issues are the result of Lambda concurrency limits and the performance of DynamoDB when data is saved.\n",
    "\n",
    "Which combination of actions should a solutions architect take to increase the performance and reliability of the application? (Choose two.)\n",
    "\n",
    "A. Evaluate and adjust the RCUs for the DynamoDB tables.\n",
    "\n",
    "B. Evaluate and adjust the WCUs for the DynamoDB tables.\n",
    "\n",
    "C. Add an Amazon ElastiCache layer to increase the performance of Lambda functions.\n",
    "\n",
    "D. Add an Amazon Simple Queue Service (Amazon SQS) queue and reprocessing logic between Amazon S3 and the Lambda functions.\n",
    "\n",
    "E. Use S3 Transfer Acceleration to provide lower latency to users.\n",
    "\n",
    "Solution:\n",
    "- correct answer is b, d\n",
    "- the question is about how to handle high concurrent request when using lambda function and dynamodb\n",
    "    - RCUs (read capacity unit), WCUs (write capacity unit)\n",
    "    - use queue combine with lambda function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #258\n",
    "A company runs an application in an on-premises data center. The application gives users the ability to upload media files. The files persist in a file server. The web application has many users. The application server is overutilized, which causes data uploads to fail occasionally. The company frequently adds new storage to the file server. The company wants to resolve these challenges by migrating the application to AWS.\n",
    "\n",
    "Users from across the United States and Canada access the application. Only authenticated users should have the ability to access the application to upload files. The company will consider a solution that refactors the application, and the company needs to accelerate application development.\n",
    "\n",
    "Which solution will meet these requirements with the LEAST operational overhead?\n",
    "\n",
    "A. Use AWS Application Migration Service to migrate the application server to Amazon EC2 instances. Create an Auto Scaling group for the EC2 instances. Use an Application Load Balancer to distribute the requests. Modify the application to use Amazon S3 to persist the files. Use Amazon Cognito to authenticate users.\n",
    "\n",
    "B. Use AWS Application Migration Service to migrate the application server to Amazon EC2 instances. Create an Auto Scaling group for the EC2 instances. Use an Application Load Balancer to distribute the requests. Set up AWS IAM Identity Center (AWS Single Sign-On) to give users the ability to sign in to the application. Modify the application to use Amazon S3 to persist the files.\n",
    "\n",
    "C. Create a static website for uploads of media files. Store the static assets in Amazon S3. Use AWS AppSync to create an API. Use AWS Lambda resolvers to upload the media files to Amazon S3. Use Amazon Cognito to authenticate users.\n",
    "\n",
    "D. Use AWS Amplify to create a static website for uploads of media files. Use Amplify Hosting to serve the website through Amazon CloudFront. Use Amazon S3 to store the uploaded media files. Use Amazon Cognito to authenticate users.\n",
    "\n",
    "Solution:\n",
    "- correct answer is d\n",
    "- the question is a web storage application architect with least operational over head\n",
    "    - Use AWS Amplify to create a static website for uploads of media files\n",
    "    - Use Amplify Hosting to serve the website through Amazon CloudFront\n",
    "    - Use Amazon S3 to store the uploaded media files\n",
    "    - Use Amazon Cognito to authenticate users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #260\n",
    "A company runs an unauthenticated static website (www.example.com) that includes a registration form for users. The website uses Amazon S3 for hosting and uses Amazon CloudFront as the content delivery network with AWS WAF configured. When the registration form is submitted, the website calls an Amazon API Gateway API endpoint that invokes an AWS Lambda function to process the payload and forward the payload to an external API call.\n",
    "\n",
    "During testing, a solutions architect encounters a cross-origin resource sharing (CORS) error. The solutions architect confirms that the CloudFront distribution origin has the Access-Control-Allow-Origin header set to www.example.com.\n",
    "\n",
    "What should the solutions architect do to resolve the error?\n",
    "\n",
    "A. Change the CORS configuration on the S3 bucket. Add rules for CORS to the AllowedOrigin element for www.example.com.\n",
    "\n",
    "B. Enable the CORS setting in AWS WAF. Create a web ACL rule in which the Access-Control-Allow-Origin header is set to www.example.com.\n",
    "\n",
    "C. Enable the CORS setting on the API Gateway API endpoint. Ensure that the API endpoint is configured to return all responses that have the Access-Control-Allow-Origin header set to www.example.com.\n",
    "\n",
    "D. Enable the CORS setting on the Lambda function. Ensure that the return code of the function has the Access-Control-Allow-Origin header set to www.example.com.\n",
    "\n",
    "Solution:\n",
    "- correct answer is c\n",
    "- the question is about how to solve cross-origin resource sharing when lambda function call external api\n",
    "    - Cross-Origin Resource Sharing (CORS) is a security measure that allows or denies scripts on webpages from making requests to a different domain than the one the script came from\n",
    "    - Enable the CORS setting on the API Gateway API endpoint\n",
    "    - Ensure that the API endpoint is configured to return all responses that have the Access-Control-Allow-Origin header set to www.example.com -> allow example.com to make request to external resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #261\n",
    "A company has many separate AWS accounts and uses no central billing or management. Each AWS account hosts services for different departments in the company. The company has a Microsoft Azure Active Directory that is deployed.\n",
    "\n",
    "A solutions architect needs to centralize billing and management of the company’s AWS accounts. The company wants to start using identity federation instead of manual user management. The company also wants to use temporary credentials instead of long-lived access keys.\n",
    "\n",
    "Which combination of steps will meet these requirements? (Choose three.)\n",
    "\n",
    "A. Create a new AWS account to serve as a management account. Deploy an organization in AWS Organizations. Invite each existing AWS account to join the organization. Ensure that each account accepts the invitation.\n",
    "\n",
    "B. Configure each AWS account's email address to be aws+@example.com so that account management email messages and invoices are sent to the same place.\n",
    "\n",
    "C. Deploy AWS IAM Identity Center (AWS Single Sign-On) in the management account. Connect IAM Identity Center to the Azure Active Directory. Configure IAM Identity Center for automatic synchronization of users and groups.\n",
    "\n",
    "D. Deploy an AWS Managed Microsoft AD directory in the management account. Share the directory with all other accounts in the organization by using AWS Resource Access Manager (AWS RAM).\n",
    "\n",
    "E. Create AWS IAM Identity Center (AWS Single Sign-On) permission sets. Attach the permission sets to the appropriate IAM Identity Center groups and AWS accounts.\n",
    "\n",
    "F. Configure AWS Identity and Access Management (IAM) in each AWS account to use AWS Managed Microsoft AD for authentication and authorization.\n",
    "\n",
    "Solution:\n",
    "- correct answer is a, c, e\n",
    "- the question is about how to design authenticate system with current directory\n",
    "    - Create a new AWS account to serve as a management account. Deploy an organization in AWS Organizations. Invite each existing AWS account to join the organization. Ensure that each account accepts the invitation\n",
    "    - Deploy AWS IAM Identity Center (AWS Single Sign-On) in the management account. Connect IAM Identity Center to the Azure Active Directory. Configure IAM Identity Center for automatic synchronization of users and groups\n",
    "    - Create AWS IAM Identity Center (AWS Single Sign-On) permission sets. Attach the permission sets to the appropriate IAM Identity Center groups and AWS accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #263\n",
    "A solutions architect needs to review the design of an Amazon EMR cluster that is using the EMR File System (EMRFS). The cluster performs tasks that are critical to business needs. The cluster is running Amazon EC2 On-Demand Instances at all times for all task, primary, and core nodes. The EMR tasks run each morning, starting at 1:00 AM. and take 6 hours to finish running. The amount of time to complete the processing is not a priority because the data is not referenced until late in the day.\n",
    "\n",
    "The solutions architect must review the architecture and suggest a solution to minimize the compute costs.\n",
    "\n",
    "Which solution should the solutions architect recommend to meet these requirements?\n",
    "\n",
    "A. Launch all task, primary, and core nodes on Spot Instances in an instance fleet. Terminate the cluster, including all instances, when the processing is completed.\n",
    "\n",
    "B. Launch the primary and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fleet. Terminate the cluster, including all instances, when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage.\n",
    "\n",
    "C. Continue to launch all nodes on On-Demand Instances. Terminate the cluster, including all instances, when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage.\n",
    "\n",
    "D. Launch the primary and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fleet. Terminate only the task node instances when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage.\n",
    "\n",
    "Solution:\n",
    "- correct answer is b\n",
    "- the question is how to reduce computing cost for EMR cluster\n",
    "    - Launch the primary and core nodes on On-Demand Instances\n",
    "    - Launch the task nodes on Spot Instances in an instance fleet\n",
    "    - Terminate the cluster, including all instances, when the processing is completed\n",
    "    - Purchase Compute Savings Plans to cover the On-Demand Instance usage"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
